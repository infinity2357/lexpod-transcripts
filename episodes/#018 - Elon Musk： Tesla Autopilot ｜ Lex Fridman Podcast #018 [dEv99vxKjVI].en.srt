1
00:00:00,120 --> 00:00:03,080
- The following is a
conversation with Elon Musk.

2
00:00:03,080 --> 00:00:06,290
He's the CEO of Tesla, SpaceX, Neuralink,

3
00:00:06,290 --> 00:00:09,340
and a co-founder of
several other companies.

4
00:00:09,340 --> 00:00:10,970
This conversation is part of

5
00:00:10,970 --> 00:00:13,300
the Artificial Intelligence Podcast.

6
00:00:13,300 --> 00:00:15,740
This series includes leading researchers

7
00:00:15,740 --> 00:00:17,320
in academia and industry,

8
00:00:17,320 --> 00:00:20,380
including CEOs and CTOs of automotive,

9
00:00:20,380 --> 00:00:24,200
robotics, AI and technology companies.

10
00:00:24,200 --> 00:00:26,460
This conversation
happened after the release

11
00:00:26,460 --> 00:00:28,680
of the paper from our group at MIT

12
00:00:28,680 --> 00:00:30,570
on driver functional vigilance

13
00:00:30,570 --> 00:00:32,637
during use of Tesla's Autopilot.

14
00:00:32,637 --> 00:00:34,630
The Tesla team reached out to me

15
00:00:34,630 --> 00:00:37,560
offering a podcast
conversation with Mr. Musk.

16
00:00:37,560 --> 00:00:40,620
I accepted with full control
of questions I could ask

17
00:00:40,620 --> 00:00:43,650
and the choice of what
is released publicly.

18
00:00:43,650 --> 00:00:46,920
I ended up editing out
nothing of substance.

19
00:00:46,920 --> 00:00:49,750
I've never spoken with Elon
before this conversation,

20
00:00:49,750 --> 00:00:51,820
publicly or privately.

21
00:00:51,820 --> 00:00:54,450
Neither he nor his
companies have any influence

22
00:00:54,450 --> 00:00:57,850
on my opinion, nor on
the rigor and integrity

23
00:00:57,850 --> 00:00:59,770
of the scientific method that I practice

24
00:00:59,770 --> 00:01:01,850
in my position at MIT.

25
00:01:01,850 --> 00:01:04,650
Tesla has never financially
supported my research

26
00:01:04,650 --> 00:01:07,320
and I've never owned a Tesla vehicle,

27
00:01:07,320 --> 00:01:10,190
and I've never owned Tesla stock.

28
00:01:10,190 --> 00:01:12,820
This podcast is not a scientific paper,

29
00:01:12,820 --> 00:01:14,380
it is a conversation.

30
00:01:14,380 --> 00:01:16,750
I respect Elon as I do all other leaders

31
00:01:16,750 --> 00:01:18,680
and engineers I've spoken with.

32
00:01:18,680 --> 00:01:21,460
We agree on some things
and disagree on others.

33
00:01:21,460 --> 00:01:23,490
My goal, as always with
these conversations,

34
00:01:23,490 --> 00:01:26,950
is to understand the way
the guest sees the world.

35
00:01:26,950 --> 00:01:29,940
One particular point of
disagreement in this conversation

36
00:01:29,940 --> 00:01:33,250
was the extent to which
camera-based driver monitoring

37
00:01:33,250 --> 00:01:36,110
will improve outcomes and for how long

38
00:01:36,110 --> 00:01:39,113
it will remain relevant
for AI-assisted driving.

39
00:01:39,960 --> 00:01:42,520
As someone who works
on and is fascinated by

40
00:01:42,520 --> 00:01:45,200
human-centered artificial intelligence,

41
00:01:45,200 --> 00:01:48,730
I believe that, if implemented
and integrated effectively,

42
00:01:48,730 --> 00:01:51,860
camera-based driver monitoring
is likely to be of benefit

43
00:01:51,860 --> 00:01:55,660
in both the short term and the long term.

44
00:01:55,660 --> 00:01:59,250
In contrast, Elon and Tesla's focus

45
00:01:59,250 --> 00:02:01,210
is on the improvement of Autopilot

46
00:02:01,210 --> 00:02:04,470
such that its statistical safety benefits

47
00:02:04,470 --> 00:02:09,050
override any concern for
human behavior and psychology.

48
00:02:09,050 --> 00:02:12,030
Elon and I may not agree on everything,

49
00:02:12,030 --> 00:02:14,850
but I deeply respect the
engineering and innovation

50
00:02:14,850 --> 00:02:16,890
behind the efforts that he leads.

51
00:02:16,890 --> 00:02:19,860
My goal here is to catalyze a rigorous,

52
00:02:19,860 --> 00:02:21,800
nuanced and objective discussion

53
00:02:21,800 --> 00:02:26,270
in industry and academia
on AI-assisted driving,

54
00:02:26,270 --> 00:02:30,900
one that ultimately makes
for a safer and better world.

55
00:02:30,900 --> 00:02:34,623
And now, here's my
conversation with Elon Musk.

56
00:02:35,630 --> 00:02:37,560
What was the vision, the dream,

57
00:02:37,560 --> 00:02:40,030
of Autopilot in the beginning?

58
00:02:40,030 --> 00:02:43,720
The big picture system level
when it was first conceived

59
00:02:43,720 --> 00:02:46,110
and started being installed in 2014,

60
00:02:46,110 --> 00:02:47,570
the hardware in the cars?

61
00:02:47,570 --> 00:02:49,780
What was the vision, the dream?

62
00:02:49,780 --> 00:02:51,400
- I wouldn't characterize
it as a vision or dream,

63
00:02:51,400 --> 00:02:56,400
it's simply that there are
obviously two massive revolutions

64
00:02:56,562 --> 00:03:00,130
in the automobile industry.

65
00:03:00,130 --> 00:03:03,023
One is the transition to electrification,

66
00:03:04,460 --> 00:03:06,393
and then the other is autonomy.

67
00:03:07,720 --> 00:03:12,720
And it became obvious to
me that, in the future,

68
00:03:13,290 --> 00:03:16,280
any car that does not have autonomy

69
00:03:16,280 --> 00:03:19,170
would be about as useful as a horse.

70
00:03:19,170 --> 00:03:20,820
Which is not to say that there's no use,

71
00:03:20,820 --> 00:03:23,750
it's just rare, and
somewhat idiosyncratic,

72
00:03:23,750 --> 00:03:25,540
if somebody has a horse at this point.

73
00:03:25,540 --> 00:03:26,620
It's just obvious that cars

74
00:03:26,620 --> 00:03:28,090
will drive themselves completely,

75
00:03:28,090 --> 00:03:29,680
it's just a question of time.

76
00:03:29,680 --> 00:03:34,680
And if we did not participate
in the autonomy revolution,

77
00:03:37,040 --> 00:03:40,930
then our cars would not
be useful to people,

78
00:03:40,930 --> 00:03:43,760
relative to cars that are autonomous.

79
00:03:43,760 --> 00:03:47,250
I mean, an autonomous
car is arguably worth

80
00:03:48,260 --> 00:03:52,943
five to 10 times more than a
car which is not autonomous.

81
00:03:53,800 --> 00:03:55,471
- In the long term.

82
00:03:55,471 --> 00:03:57,730
- Depends what you mean by long term but,

83
00:03:57,730 --> 00:03:59,580
let's say at least for
the next five years,

84
00:03:59,580 --> 00:04:00,543
perhaps 10 years.

85
00:04:01,500 --> 00:04:04,140
- So there are a lot of very
interesting design choices

86
00:04:04,140 --> 00:04:05,790
with Autopilot early on.

87
00:04:05,790 --> 00:04:10,010
First is showing on
the instrument cluster,

88
00:04:10,010 --> 00:04:12,750
or in the Model 3 and
the center stack display,

89
00:04:12,750 --> 00:04:14,943
what the combined sensor suite sees.

90
00:04:15,780 --> 00:04:17,970
What was the thinking behind that choice?

91
00:04:17,970 --> 00:04:20,550
Was there a debate, what was the process?

92
00:04:20,550 --> 00:04:22,510
- The whole point of the display

93
00:04:22,510 --> 00:04:25,520
is to provide a health check on

94
00:04:26,901 --> 00:04:28,160
the vehicle's perception of reality.

95
00:04:28,160 --> 00:04:30,067
So the vehicle's taking in information

96
00:04:30,067 --> 00:04:32,280
from a bunch of sensors,
primarily cameras,

97
00:04:32,280 --> 00:04:37,280
but also radar and
ultrasonics, GPS and so forth.

98
00:04:37,423 --> 00:04:42,423
And then, that information
is then rendered into

99
00:04:42,560 --> 00:04:46,030
vector space with a bunch of objects,

100
00:04:46,030 --> 00:04:49,170
with properties like lane lines

101
00:04:49,170 --> 00:04:51,230
and traffic lights and other cars.

102
00:04:51,230 --> 00:04:54,890
And then, in vector
space, that is re-rendered

103
00:04:54,890 --> 00:04:57,960
onto a display so you can confirm whether

104
00:04:57,960 --> 00:04:59,810
the car knows what's going on or not,

105
00:05:00,820 --> 00:05:02,730
by looking out the window.

106
00:05:02,730 --> 00:05:05,470
- Right, I think that's an
extremely powerful thing

107
00:05:05,470 --> 00:05:07,960
for people to get an understanding,

108
00:05:07,960 --> 00:05:09,240
sort of become one with the system

109
00:05:09,240 --> 00:05:11,750
and understanding what
the system is capable of.

110
00:05:11,750 --> 00:05:14,850
Now, have you considered showing more?

111
00:05:14,850 --> 00:05:16,700
So if we look at the computer vision,

112
00:05:18,020 --> 00:05:19,800
like road segmentation, lane detection,

113
00:05:19,800 --> 00:05:23,070
vehicle detection, object
detection, underlying the system,

114
00:05:23,070 --> 00:05:25,750
there is at the edges, some uncertainty.

115
00:05:25,750 --> 00:05:29,440
Have you considered revealing the parts

116
00:05:29,440 --> 00:05:33,670
that the uncertainty in
the system, the sort of--

117
00:05:33,670 --> 00:05:35,600
- Probabilities associated with say,

118
00:05:35,600 --> 00:05:36,850
image recognition or something like that?

119
00:05:36,850 --> 00:05:40,130
- Yeah, so right now, it shows
the vehicles in the vicinity,

120
00:05:40,130 --> 00:05:43,480
a very clean crisp image,
and people do confirm

121
00:05:43,480 --> 00:05:44,660
that there's a car in front of me

122
00:05:44,660 --> 00:05:46,700
and the system sees there's
a car in front of me,

123
00:05:46,700 --> 00:05:49,070
but to help people build an intuition

124
00:05:49,070 --> 00:05:50,810
of what computer vision is,

125
00:05:50,810 --> 00:05:53,120
by showing some of the uncertainty.

126
00:05:53,120 --> 00:05:58,120
- Well, in my car I always look
at this with the debug view.

127
00:05:58,240 --> 00:06:00,110
And there's two debug views.

128
00:06:00,110 --> 00:06:04,970
One is augmented vision,
which I'm sure you've seen,

129
00:06:04,970 --> 00:06:08,570
where it's basically we
draw boxes and labels

130
00:06:08,570 --> 00:06:10,483
around objects that are recognized.

131
00:06:11,470 --> 00:06:15,330
And then there's we what
call the visualizer,

132
00:06:15,330 --> 00:06:18,020
which is basically vector
space representation,

133
00:06:18,020 --> 00:06:21,053
summing up the input from all sensors.

134
00:06:22,470 --> 00:06:24,723
That does not show any pictures,

135
00:06:28,276 --> 00:06:29,570
which basically shows the car's view

136
00:06:29,570 --> 00:06:32,383
of the world in vector space.

137
00:06:33,520 --> 00:06:35,532
But I think this is very difficult

138
00:06:35,532 --> 00:06:37,170
for normal people to understand,

139
00:06:37,170 --> 00:06:39,550
they're would not know what
thing they're looking at.

140
00:06:39,550 --> 00:06:41,390
- So it's almost an HMI challenge through

141
00:06:41,390 --> 00:06:44,830
the current things that are
being displayed is optimized

142
00:06:44,830 --> 00:06:47,170
for the general public understanding

143
00:06:47,170 --> 00:06:48,810
of what the system's capable of.

144
00:06:48,810 --> 00:06:51,750
- If you have no idea how
computer vision works or anything,

145
00:06:51,750 --> 00:06:53,080
you can still look at the screen

146
00:06:53,080 --> 00:06:55,840
and see if the car knows what's going on.

147
00:06:55,840 --> 00:06:58,500
And then if you're a development engineer,

148
00:06:58,500 --> 00:07:02,430
or if you have the
development build like I do,

149
00:07:02,430 --> 00:07:06,070
then you can see all
the debug information.

150
00:07:06,070 --> 00:07:10,353
But this would just be like
total gibberish to most people.

151
00:07:11,320 --> 00:07:14,290
- What's your view on how
to best distribute effort?

152
00:07:14,290 --> 00:07:16,890
So there's three, I would
say, technical aspects

153
00:07:16,890 --> 00:07:19,100
of Autopilot that are really important.

154
00:07:19,100 --> 00:07:20,530
So it's the underlying algorithms,

155
00:07:20,530 --> 00:07:22,330
like the neural network architecture,

156
00:07:22,330 --> 00:07:24,530
there's the data that it's trained on,

157
00:07:24,530 --> 00:07:27,710
and then there's the hardware
development and maybe others.

158
00:07:27,710 --> 00:07:32,130
So, look, algorithm, data, hardware.

159
00:07:32,130 --> 00:07:35,290
You only have so much money,
only have so much time.

160
00:07:35,290 --> 00:07:37,369
What do you think is
the most important thing

161
00:07:37,369 --> 00:07:40,070
to allocate resources to?

162
00:07:40,070 --> 00:07:42,320
Or do you see it as
pretty evenly distributed

163
00:07:43,180 --> 00:07:44,580
between those three?

164
00:07:44,580 --> 00:07:46,700
- We automatically get
vast amounts of data

165
00:07:46,700 --> 00:07:48,720
because all of our cars have

166
00:07:51,670 --> 00:07:53,640
eight external facing cameras,

167
00:07:53,640 --> 00:07:58,163
and radar, and usually
12 ultrasonic sensors,

168
00:07:59,120 --> 00:08:02,563
GPS obviously, and IMU.

169
00:08:09,908 --> 00:08:12,320
And we've got about
400,000 cars on the road

170
00:08:12,320 --> 00:08:13,920
that have that level of data.

171
00:08:13,920 --> 00:08:15,880
Actually, I think you keep quite
close track of it actually.

172
00:08:15,880 --> 00:08:16,713
- Yes.

173
00:08:16,713 --> 00:08:20,820
- Yeah, so we're approaching
half a million cars on the road

174
00:08:20,820 --> 00:08:22,420
that have the full sensor suite.

175
00:08:24,948 --> 00:08:27,410
I'm not sure how many
other cars on the road

176
00:08:27,410 --> 00:08:29,430
have this sensor suite,

177
00:08:29,430 --> 00:08:32,340
but I'd be surprised if
it's more than 5,000,

178
00:08:32,340 --> 00:08:35,193
which means that we have
99% of all the data.

179
00:08:36,150 --> 00:08:38,420
- So there's this huge inflow of data.

180
00:08:38,420 --> 00:08:40,710
- Absolutely, a massive inflow of data.

181
00:08:40,710 --> 00:08:44,440
And then it's taken us about three years,

182
00:08:44,440 --> 00:08:45,740
but now we've finally developed

183
00:08:45,740 --> 00:08:47,690
our full self-driving computer,

184
00:08:47,690 --> 00:08:52,690
which can process an
order of magnitude as much

185
00:08:53,850 --> 00:08:56,370
as the NVIDIA system that we
currently have in the cars,

186
00:08:56,370 --> 00:09:00,290
and to use it, you unplug
the NVIDIA computer

187
00:09:00,290 --> 00:09:02,643
and plug the Tesla
computer in and that's it.

188
00:09:05,810 --> 00:09:07,950
In fact, we still are exploring

189
00:09:07,950 --> 00:09:10,190
the boundaries of its capabilities.

190
00:09:10,190 --> 00:09:11,990
We're able to run the
cameras at full frame-rate,

191
00:09:11,990 --> 00:09:15,470
full resolution, not even crop the images,

192
00:09:15,470 --> 00:09:20,020
and it's still got headroom
even on one of the systems.

193
00:09:20,020 --> 00:09:23,510
The full self-driving computer
is really two computers,

194
00:09:23,510 --> 00:09:26,140
two systems on a chip,
that are fully redundant.

195
00:09:26,140 --> 00:09:27,460
So you could put a boat through

196
00:09:27,460 --> 00:09:30,230
basically any part of that
system and it still works.

197
00:09:30,230 --> 00:09:33,537
- The redundancy, are they
perfect copies of each other or--

198
00:09:33,537 --> 00:09:34,460
- Yeah.

199
00:09:34,460 --> 00:09:36,010
- Oh, so it's purely for redundancy

200
00:09:36,010 --> 00:09:38,470
as opposed to an arguing
machine kind of architecture

201
00:09:38,470 --> 00:09:40,080
where they're both making decisions,

202
00:09:40,080 --> 00:09:42,207
this is purely for redundancy.

203
00:09:42,207 --> 00:09:43,870
- Think of it more like it's

204
00:09:43,870 --> 00:09:45,570
a twin-engine commercial aircraft.

205
00:09:47,550 --> 00:09:51,823
The system will operate best
if both systems are operating,

206
00:09:52,920 --> 00:09:55,703
but it's capable of
operating safely on one.

207
00:09:56,650 --> 00:10:00,300
So, as it is right now, we can just run,

208
00:10:00,300 --> 00:10:04,460
we haven't even hit
the edge of performance

209
00:10:04,460 --> 00:10:09,340
so there's no need to actually distribute

210
00:10:09,340 --> 00:10:13,860
functionality across both SOCs.

211
00:10:13,860 --> 00:10:17,010
We can actually just run a
full duplicate on each one.

212
00:10:17,010 --> 00:10:19,270
- So you haven't really explored

213
00:10:19,270 --> 00:10:20,660
or hit the limit of the system.

214
00:10:20,660 --> 00:10:22,540
- [Elon] No not yet, the limit, no.

215
00:10:22,540 --> 00:10:24,760
- So the magic of deep learning

216
00:10:24,760 --> 00:10:27,290
is that it gets better with data.

217
00:10:27,290 --> 00:10:29,640
You said there's a huge inflow of data,

218
00:10:29,640 --> 00:10:32,180
but the thing about driving,
- Yeah.

219
00:10:32,180 --> 00:10:36,003
- the really valuable data to
learn from is the edge cases.

220
00:10:39,060 --> 00:10:44,060
I've heard you talk somewhere
about Autopilot disengagements

221
00:10:44,180 --> 00:10:46,990
being an important moment of time to use.

222
00:10:46,990 --> 00:10:48,320
Is there other edge cases

223
00:10:48,320 --> 00:10:52,640
or perhaps can you speak
to those edge cases,

224
00:10:52,640 --> 00:10:54,700
what aspects of them might be valuable,

225
00:10:54,700 --> 00:10:56,170
or if you have other ideas,

226
00:10:56,170 --> 00:10:57,400
how to discover more and more

227
00:10:57,400 --> 00:10:59,253
and more edge cases in driving?

228
00:11:00,310 --> 00:11:02,410
- Well there's a lot of
things that are learnt.

229
00:11:02,410 --> 00:11:04,940
There are certainly edge cases where,

230
00:11:04,940 --> 00:11:08,120
say somebody's on Autopilot
and they take over,

231
00:11:08,120 --> 00:11:12,500
and then that's a trigger
that goes out to our system

232
00:11:12,500 --> 00:11:15,200
and says, okay, did they
take over for convenience,

233
00:11:15,200 --> 00:11:18,160
or did they take over
because the Autopilot

234
00:11:18,160 --> 00:11:19,420
wasn't working properly?

235
00:11:19,420 --> 00:11:21,900
There's also, let's say
we're trying to figure out,

236
00:11:21,900 --> 00:11:26,393
what is the optimal spline for
traversing an intersection.

237
00:11:27,920 --> 00:11:31,400
Then the ones where there
are no interventions

238
00:11:32,550 --> 00:11:33,680
are the right ones.

239
00:11:33,680 --> 00:11:36,410
So you then you say, okay,
when it looks like this,

240
00:11:36,410 --> 00:11:37,453
do the following.

241
00:11:38,320 --> 00:11:40,980
And then you get the optimal spline for

242
00:11:42,100 --> 00:11:44,823
navigating a complex intersection.

243
00:11:47,390 --> 00:11:49,190
- So there's kind of the common case,

244
00:11:49,190 --> 00:11:52,320
So you're trying to capture
a huge amount of samples

245
00:11:52,320 --> 00:11:54,990
of a particular intersection
when things went right,

246
00:11:54,990 --> 00:11:57,260
and then there's the edge case

247
00:11:57,260 --> 00:12:00,170
where, as you said, not for convenience,

248
00:12:00,170 --> 00:12:03,047
but something didn't go exactly right.

249
00:12:03,047 --> 00:12:06,030
- So if somebody started
manual control from Autopilot.

250
00:12:06,030 --> 00:12:07,680
And really, the way to look at this

251
00:12:07,680 --> 00:12:09,940
is view all input as error.

252
00:12:09,940 --> 00:12:12,680
If the user had to do
input, there's something,

253
00:12:12,680 --> 00:12:13,960
all input is error.

254
00:12:13,960 --> 00:12:16,400
- That's a powerful line
to think of it that way

255
00:12:16,400 --> 00:12:17,800
'cause it may very well be error,

256
00:12:17,800 --> 00:12:19,990
but if you wanna exit the highway,

257
00:12:19,990 --> 00:12:23,130
or if it's a navigation decision

258
00:12:23,130 --> 00:12:25,430
that Autopilot's not
currently designed to do,

259
00:12:25,430 --> 00:12:27,850
then the driver takes
over, how do you know

260
00:12:27,850 --> 00:12:29,060
the difference?
- Yeah, that's gonna change

261
00:12:29,060 --> 00:12:32,130
with Navigate on Autopilot,
which we've just released,

262
00:12:32,130 --> 00:12:33,763
and without stalk confirm.

263
00:12:37,603 --> 00:12:40,066
Assuming control in order
to do a lane change,

264
00:12:40,066 --> 00:12:44,780
or exit a freeway, or doing
a highway interchange,

265
00:12:44,780 --> 00:12:46,070
the vast majority of that will go away

266
00:12:46,070 --> 00:12:48,930
with the release that just went out.

267
00:12:48,930 --> 00:12:52,960
- Yeah, so that, I don't
think people quite understand

268
00:12:52,960 --> 00:12:54,550
how big of a step that is.

269
00:12:54,550 --> 00:12:55,500
- Yeah, they don't.

270
00:12:56,400 --> 00:12:58,270
If you drive the car then you do.

271
00:12:58,270 --> 00:12:59,620
- So you still have to keep your hands

272
00:12:59,620 --> 00:13:00,790
on the steering wheel currently

273
00:13:00,790 --> 00:13:02,740
when it does the automatic lane change.

274
00:13:05,990 --> 00:13:08,720
There's these big leaps through
he development of Autopilot,

275
00:13:08,720 --> 00:13:11,320
through its history and,

276
00:13:11,320 --> 00:13:13,600
what stands out to you as the big leaps?

277
00:13:13,600 --> 00:13:16,190
I would say this one,
Navigate on Autopilot

278
00:13:16,190 --> 00:13:21,130
without having to confirm is a huge leap.

279
00:13:21,130 --> 00:13:22,855
- It is a huge leap.
- What are the--

280
00:13:22,855 --> 00:13:24,910
It also automatically overtakes slow cars.

281
00:13:24,910 --> 00:13:29,910
So it's both navigation and
seeking the fastest lane.

282
00:13:31,070 --> 00:13:36,070
So it'll overtake slow
cars and exit the freeway

283
00:13:36,980 --> 00:13:38,733
and take highway interchanges,

284
00:13:40,661 --> 00:13:44,900
and then we have traffic
light recognition,

285
00:13:47,200 --> 00:13:50,240
which introduced initially as a warning.

286
00:13:50,240 --> 00:13:52,330
I mean, on the development
version that I'm driving,

287
00:13:52,330 --> 00:13:56,940
the car fully stops and
goes at traffic lights.

288
00:13:56,940 --> 00:13:58,530
- So those are the steps, right?

289
00:13:58,530 --> 00:14:00,010
You've just mentioned some things

290
00:14:00,010 --> 00:14:03,320
that are an inkling of a
step towards full autonomy.

291
00:14:03,320 --> 00:14:07,330
What would you say are
the biggest technological

292
00:14:07,330 --> 00:14:10,010
roadblocks to full self-driving?

293
00:14:10,010 --> 00:14:13,573
- Actually, the full self-driving
computer that we just,

294
00:14:14,430 --> 00:14:17,570
the Tesla, what we call, FSD computer

295
00:14:17,570 --> 00:14:19,663
that's now in production,

296
00:14:21,910 --> 00:14:25,580
so if you order any Model S or X,

297
00:14:25,580 --> 00:14:29,570
or any Model 3 that has the
full self-driving package,

298
00:14:29,570 --> 00:14:31,053
you'll get the FSD computer.

299
00:14:32,390 --> 00:14:37,290
That's important to have
enough base computation.

300
00:14:37,290 --> 00:14:40,173
Then refining the neural net
and the control software.

301
00:14:41,800 --> 00:14:45,200
All of that can just be provided
as an over-the-air update.

302
00:14:45,200 --> 00:14:47,110
The thing that's really profound,

303
00:14:47,110 --> 00:14:51,530
and what I'll be emphasizing
at the investor day

304
00:14:51,530 --> 00:14:53,320
that we're having focused on autonomy,

305
00:14:53,320 --> 00:14:56,130
is that the car is
currently being produced,

306
00:14:56,130 --> 00:14:58,210
with the hard word
currently being produced,

307
00:14:58,210 --> 00:15:01,040
is capable of full self-driving.

308
00:15:01,040 --> 00:15:03,657
- But capable is an
interesting word because--

309
00:15:04,610 --> 00:15:05,516
- [Elon] The hardware is.

310
00:15:05,516 --> 00:15:06,349
- Yeah, the hardware.

311
00:15:06,349 --> 00:15:07,620
- And as we refine the software,

312
00:15:09,430 --> 00:15:11,760
the capabilities will
increase dramatically,

313
00:15:11,760 --> 00:15:14,010
and then the reliability
will increase dramatically,

314
00:15:14,010 --> 00:15:16,190
and then it will receive
regulatory approval.

315
00:15:16,190 --> 00:15:17,680
So essentially, buying a car today

316
00:15:17,680 --> 00:15:19,230
is an investment in the future.

317
00:15:23,330 --> 00:15:26,330
I think the most profound thing is that

318
00:15:26,330 --> 00:15:27,830
if you buy a Tesla today,

319
00:15:27,830 --> 00:15:30,510
I believe you're buying
an appreciating asset,

320
00:15:30,510 --> 00:15:33,160
not a depreciating asset.

321
00:15:33,160 --> 00:15:35,360
- So that's a really
important statement there

322
00:15:35,360 --> 00:15:37,830
because if hardware is capable enough,

323
00:15:37,830 --> 00:15:38,780
that's the hard thing

324
00:15:38,780 --> 00:15:41,320
to upgrade usually.
- Yes, exactly.

325
00:15:41,320 --> 00:15:43,820
- Then the rest is a software problem--

326
00:15:43,820 --> 00:15:46,993
- Yes, software has no
marginal cost really.

327
00:15:47,950 --> 00:15:51,510
- But, what's your intuition
on the software side?

328
00:15:51,510 --> 00:15:54,690
How hard are the remaining steps

329
00:15:55,860 --> 00:16:00,860
to get it to where the experience,

330
00:16:02,650 --> 00:16:05,800
not just the safety,
but the full experience

331
00:16:05,800 --> 00:16:09,421
is something that people would enjoy?

332
00:16:09,421 --> 00:16:12,860
- I think people it enjoy
it very much so on highways.

333
00:16:12,860 --> 00:16:16,820
It's a total game changer
for quality of life,

334
00:16:16,820 --> 00:16:21,380
for using Tesla Autopilot on the highways.

335
00:16:21,380 --> 00:16:23,060
So it's really just
extending that functionality

336
00:16:23,060 --> 00:16:28,060
to city streets, adding in
the traffic light recognition,

337
00:16:29,260 --> 00:16:31,490
navigating complex intersections,

338
00:16:31,490 --> 00:16:36,490
and then being able to navigate
complicated parking lots

339
00:16:37,290 --> 00:16:41,330
so the car can exit a parking
space and come and find you,

340
00:16:41,330 --> 00:16:44,693
even if it's in a complete
maze of a parking lot.

341
00:16:46,450 --> 00:16:49,970
And, then it can just drop you off

342
00:16:49,970 --> 00:16:53,000
and find a parking spot, by itself.

343
00:16:53,000 --> 00:16:56,225
- Yeah, in terms of enjoyabilty,
and something that people

344
00:16:56,225 --> 00:16:58,890
would actually find a lotta use from,

345
00:16:58,890 --> 00:17:03,600
the parking lot, it's rich of annoyance

346
00:17:03,600 --> 00:17:04,760
when you have to do it manually,

347
00:17:04,760 --> 00:17:06,710
so there's a lot of benefit to be gained

348
00:17:06,710 --> 00:17:08,670
from automation there.

349
00:17:08,670 --> 00:17:10,410
So, let me start injecting the human

350
00:17:10,410 --> 00:17:12,820
into this discussion a little bit.

351
00:17:12,820 --> 00:17:15,670
So let's talk about full autonomy,

352
00:17:15,670 --> 00:17:17,500
if you look at the current
level four vehicles

353
00:17:17,500 --> 00:17:19,823
being tested on row like Waymo and so on,

354
00:17:20,730 --> 00:17:23,400
they're only technically autonomous,

355
00:17:23,400 --> 00:17:25,480
they're really level two systems

356
00:17:26,770 --> 00:17:28,910
with just a different design philosophy,

357
00:17:28,910 --> 00:17:30,550
because there's always a safety driver

358
00:17:30,550 --> 00:17:31,710
in almost all cases, and

359
00:17:31,710 --> 00:17:33,380
they're monitoring the system.
- Right.

360
00:17:33,380 --> 00:17:38,380
- Do you see Tesla's full
self-driving as still,

361
00:17:39,070 --> 00:17:43,210
for a time to come, requiring supervision

362
00:17:43,210 --> 00:17:44,840
of the human being.

363
00:17:44,840 --> 00:17:47,510
So its capabilities are
powerful enough to drive

364
00:17:47,510 --> 00:17:49,080
but nevertheless requires a human

365
00:17:49,080 --> 00:17:50,970
to still be supervising, just like

366
00:17:50,970 --> 00:17:55,970
a safety driver is in other
fully autonomous vehicles?

367
00:17:57,410 --> 00:18:01,590
- I think it will require
detecting hands on wheel

368
00:18:01,590 --> 00:18:06,590
for at least six months or
something like that from here.

369
00:18:09,540 --> 00:18:13,883
Really it's a question of,
from a regulatory standpoint,

370
00:18:16,120 --> 00:18:19,810
how much safer than a person
does Autopilot need to be,

371
00:18:19,810 --> 00:18:23,183
for it to be okay to not monitor the car.

372
00:18:24,950 --> 00:18:27,150
And this is a debate that one can have,

373
00:18:27,150 --> 00:18:32,150
and then, but you need
a large amount of data,

374
00:18:32,950 --> 00:18:35,280
so that you can prove,
with high confidence,

375
00:18:35,280 --> 00:18:37,620
statistically speaking, that the car

376
00:18:37,620 --> 00:18:40,390
is dramatically safer than a person.

377
00:18:40,390 --> 00:18:42,830
And that adding in the person monitoring

378
00:18:42,830 --> 00:18:45,960
does not materially affect the safety.

379
00:18:45,960 --> 00:18:50,170
So it might need to be 200
or 300% safer than a person.

380
00:18:50,170 --> 00:18:51,200
- And how do you prove that?

381
00:18:51,200 --> 00:18:52,440
- Incidents per mile.

382
00:18:52,440 --> 00:18:53,550
- Incidents per mile.
- Yeah.

383
00:18:53,550 --> 00:18:56,950
- So crashes and fatalities--

384
00:18:56,950 --> 00:18:58,710
- Yeah, fatalities would be a factor,

385
00:18:58,710 --> 00:19:00,490
but there are just not enough fatalities

386
00:19:00,490 --> 00:19:04,060
to be statistically significant, at scale.

387
00:19:04,060 --> 00:19:07,400
But there are enough crashes,

388
00:19:07,400 --> 00:19:10,050
there are far more crashes
then there are fatalities.

389
00:19:11,020 --> 00:19:14,953
So you can assess what is
the probability of a crash.

390
00:19:16,330 --> 00:19:19,640
Then there's another step
which is probability of injury.

391
00:19:19,640 --> 00:19:21,730
And probability of permanent injury,

392
00:19:21,730 --> 00:19:23,900
the probability of death.

393
00:19:23,900 --> 00:19:27,720
And all of those need to be
much better than a person,

394
00:19:27,720 --> 00:19:32,720
by at least, perhaps, 200%.

395
00:19:33,080 --> 00:19:36,080
- And you think there's
the ability to have

396
00:19:36,080 --> 00:19:38,720
a healthy discourse with
the regulatory bodies

397
00:19:38,720 --> 00:19:40,100
on this topic?

398
00:19:40,100 --> 00:19:44,270
- I mean, there's no
question that regulators paid

399
00:19:44,270 --> 00:19:46,920
a disproportionate amount of attention

400
00:19:46,920 --> 00:19:48,760
to that which generates press,

401
00:19:48,760 --> 00:19:50,940
this is just an objective fact.

402
00:19:50,940 --> 00:19:53,410
And it also generates a lot of press.

403
00:19:53,410 --> 00:19:58,410
So, in the United States there's, I think,

404
00:19:58,560 --> 00:20:01,950
almost 40,000 automotive deaths per year.

405
00:20:01,950 --> 00:20:04,510
But if there are four in Tesla,

406
00:20:04,510 --> 00:20:07,040
they will probably receive
a thousand times more press

407
00:20:07,040 --> 00:20:08,860
than anyone else.

408
00:20:08,860 --> 00:20:11,480
- So the psychology of that
is actually fascinating,

409
00:20:11,480 --> 00:20:12,590
I don't think we'll have enough time

410
00:20:12,590 --> 00:20:15,360
to talk about that, but I
have to talk to you about

411
00:20:15,360 --> 00:20:17,070
the human side of things.

412
00:20:17,070 --> 00:20:19,803
So, myself and our team
at MIT recently released

413
00:20:19,803 --> 00:20:22,940
a paper on functional vigilance of drivers

414
00:20:22,940 --> 00:20:24,640
while using Autopilot.

415
00:20:24,640 --> 00:20:27,540
This is work we've been
doing since Autopilot

416
00:20:27,540 --> 00:20:31,070
was first released publicly,
over three years ago,

417
00:20:31,070 --> 00:20:34,640
collecting video of driver
faces and driver body.

418
00:20:34,640 --> 00:20:38,490
So I saw that you tweeted
a quote from the abstract,

419
00:20:38,490 --> 00:20:43,490
so I can at least guess
that you've glanced at it.

420
00:20:43,521 --> 00:20:44,570
- Yeah, I read it.

421
00:20:44,570 --> 00:20:46,460
- Can I talk you through what we found?

422
00:20:46,460 --> 00:20:47,293
- Sure.

423
00:20:47,293 --> 00:20:52,293
- Okay, it appears that in
the data that we've collected,

424
00:20:52,480 --> 00:20:55,920
that drivers are maintaining
functional vigilance such that,

425
00:20:55,920 --> 00:20:58,600
we're looking at 18,000
disengagements from Autopilot,

426
00:20:58,600 --> 00:21:03,400
18,900, and annotating were they able

427
00:21:03,400 --> 00:21:05,780
to take over control in a timely manner.

428
00:21:05,780 --> 00:21:08,770
So they were there,
present, looking at the road

429
00:21:08,770 --> 00:21:11,460
to take over control, okay.

430
00:21:11,460 --> 00:21:15,670
So this goes against
what many would predict

431
00:21:15,670 --> 00:21:19,540
from the body of literature
on vigilance with automation.

432
00:21:19,540 --> 00:21:22,040
Now the question is, do you think

433
00:21:22,040 --> 00:21:24,900
these results hold across
the broader population.

434
00:21:24,900 --> 00:21:27,163
So, ours is just a small subset.

435
00:21:28,610 --> 00:21:32,930
One of the criticism is that,
there's a small minority

436
00:21:32,930 --> 00:21:36,120
of drivers that may be highly responsible,

437
00:21:36,120 --> 00:21:38,870
where their vigilance
decrement would increase

438
00:21:38,870 --> 00:21:40,390
with Autopilot use.

439
00:21:40,390 --> 00:21:42,610
- I think this is all
really gonna be swept,

440
00:21:42,610 --> 00:21:46,650
I mean, the system's improving so much,

441
00:21:46,650 --> 00:21:50,383
so fast, that this is gonna
be a moot point very soon.

442
00:21:52,140 --> 00:21:57,140
Where vigilance is, if
something's many times safer

443
00:21:57,950 --> 00:22:01,630
than a person, then adding a person does,

444
00:22:01,630 --> 00:22:04,583
the effect on safety is limited.

445
00:22:05,750 --> 00:22:08,823
And, in fact, it could be negative.

446
00:22:10,690 --> 00:22:15,000
- That's really interesting,
so the fact that a human may,

447
00:22:15,000 --> 00:22:18,500
some percent of the population may exhibit

448
00:22:18,500 --> 00:22:20,710
a vigilance decrement, will not affect

449
00:22:20,710 --> 00:22:22,400
overall statistics, numbers on safety?

450
00:22:22,400 --> 00:22:24,853
- No, in fact, I think it will become,

451
00:22:25,900 --> 00:22:29,290
very, very quickly, maybe even
towards the end of this year,

452
00:22:29,290 --> 00:22:32,100
but I would say, I'd be
shocked if it's not next year

453
00:22:32,100 --> 00:22:36,370
at the latest, that
having a human intervene

454
00:22:36,370 --> 00:22:37,923
will decrease safety.

455
00:22:39,750 --> 00:22:42,950
Decrease, like imagine
if you're in an elevator.

456
00:22:42,950 --> 00:22:45,730
Now it used to be that there
were elevator operators.

457
00:22:45,730 --> 00:22:48,400
And you couldn't go on
an elevator by yourself

458
00:22:48,400 --> 00:22:50,993
and work the lever to move between floors.

459
00:22:52,500 --> 00:22:57,010
And now nobody wants an elevator operator,

460
00:22:57,010 --> 00:23:00,520
because the automated elevator
that stops at the floors

461
00:23:00,520 --> 00:23:02,743
is much safer than the elevator operator.

462
00:23:04,060 --> 00:23:05,450
And in fact it would be quite dangerous

463
00:23:05,450 --> 00:23:06,870
to have someone with a lever

464
00:23:06,870 --> 00:23:09,830
that can move the elevator between floors.

465
00:23:09,830 --> 00:23:12,830
- So, that's a really powerful statement,

466
00:23:12,830 --> 00:23:14,710
and a really interesting one,

467
00:23:14,710 --> 00:23:16,960
but I also have to ask
from a user experience

468
00:23:16,960 --> 00:23:18,770
and from a safety perspective,

469
00:23:18,770 --> 00:23:21,310
one of the passions for me algorithmically

470
00:23:21,310 --> 00:23:26,020
is camera-based detection
of just sensing the human,

471
00:23:26,020 --> 00:23:27,830
but detecting what the
driver's looking at,

472
00:23:27,830 --> 00:23:30,850
cognitive load, body pose,
on the computer vision side

473
00:23:30,850 --> 00:23:32,300
that's a fascinating problem.

474
00:23:33,160 --> 00:23:34,910
And there's many in industry who believe

475
00:23:34,910 --> 00:23:37,570
you have to have camera-based
driver monitoring.

476
00:23:37,570 --> 00:23:39,850
Do you think there could be benefit gained

477
00:23:39,850 --> 00:23:41,760
from driver monitoring?

478
00:23:41,760 --> 00:23:46,660
- If you have a system that's
at or below a human level

479
00:23:46,660 --> 00:23:49,210
of reliability, then driver
monitoring makes sense.

480
00:23:50,260 --> 00:23:52,150
But if your system is dramatically better,

481
00:23:52,150 --> 00:23:55,900
more reliable than a human,
then driver monitoring

482
00:23:57,624 --> 00:23:59,500
does not help much.

483
00:23:59,500 --> 00:24:01,593
And, like I said,

484
00:24:05,791 --> 00:24:07,170
if you're in an elevator,
do you really want

485
00:24:07,170 --> 00:24:09,840
someone with a big
lever, some random person

486
00:24:09,840 --> 00:24:11,740
operating the elevator between floors?

487
00:24:13,020 --> 00:24:14,715
I wouldn't trust that.

488
00:24:14,715 --> 00:24:16,073
I would rather have the buttons.

489
00:24:17,500 --> 00:24:20,910
- Okay, you're optimistic
about the pace of improvement

490
00:24:20,910 --> 00:24:22,900
of the system, from what you've seen

491
00:24:22,900 --> 00:24:25,500
with the full self-driving car computer.

492
00:24:25,500 --> 00:24:27,550
- The rate of improvement is exponential.

493
00:24:28,570 --> 00:24:30,920
- So, one of the other very interesting

494
00:24:30,920 --> 00:24:33,830
design choices early on
that connects to this,

495
00:24:33,830 --> 00:24:38,300
is the operational design
domain of Autopilot.

496
00:24:38,300 --> 00:24:41,743
So, where Autopilot is
able to be turned on.

497
00:24:43,200 --> 00:24:47,190
So contrast another vehicle
system that we were studying

498
00:24:47,190 --> 00:24:49,540
is the Cadillac Super
Cruise system that's,

499
00:24:49,540 --> 00:24:51,520
in terms of ODD, very constrained

500
00:24:51,520 --> 00:24:53,600
to particular kinds of highways,

501
00:24:53,600 --> 00:24:56,500
well mapped, tested,
but it's much narrower

502
00:24:56,500 --> 00:24:58,713
than the ODD of Tesla vehicles.

503
00:25:00,840 --> 00:25:02,830
- It's like ADD (both laugh).

504
00:25:02,830 --> 00:25:06,493
- Yeah, that's good, that's a good line.

505
00:25:08,080 --> 00:25:12,010
What was the design decision in

506
00:25:12,010 --> 00:25:13,760
that different philosophy of thinking,

507
00:25:13,760 --> 00:25:15,600
where there's pros and cons.

508
00:25:15,600 --> 00:25:18,650
What we see with a wide ODD

509
00:25:18,650 --> 00:25:22,320
is Tesla drivers are able to explore more

510
00:25:22,320 --> 00:25:23,680
the limitations of the system,

511
00:25:23,680 --> 00:25:26,230
at least early on, and they understand,

512
00:25:26,230 --> 00:25:28,270
together with the
instrument cluster display,

513
00:25:28,270 --> 00:25:30,400
they start to understand
what are the capabilities,

514
00:25:30,400 --> 00:25:31,970
so that's a benefit.

515
00:25:31,970 --> 00:25:34,900
The con is you're letting drivers

516
00:25:34,900 --> 00:25:37,227
use it basically anywhere--

517
00:25:38,710 --> 00:25:40,930
- Anywhere that it can
detect lanes with confidence.

518
00:25:40,930 --> 00:25:43,093
- Lanes, was there a philosophy,

519
00:25:44,640 --> 00:25:46,580
design decisions that were challenging,

520
00:25:46,580 --> 00:25:48,170
that were being made there?

521
00:25:48,170 --> 00:25:53,170
Or from the very beginning
was that done on purpose,

522
00:25:53,600 --> 00:25:54,433
with intent?

523
00:25:56,110 --> 00:25:57,900
- Frankly it's pretty crazy letting people

524
00:25:57,900 --> 00:26:01,563
drive a two-ton death machine manually.

525
00:26:02,910 --> 00:26:06,380
That's crazy, like, in the
future will people be like,

526
00:26:06,380 --> 00:26:09,100
I can't believe anyone
was just allowed to drive

527
00:26:09,100 --> 00:26:13,040
one of these two-ton death machines,

528
00:26:13,040 --> 00:26:14,510
and they just drive wherever they wanted.

529
00:26:14,510 --> 00:26:16,343
Just like elevators, you could just move

530
00:26:16,343 --> 00:26:18,240
that elevator with that
lever wherever you wanted,

531
00:26:18,240 --> 00:26:20,583
can stop it halfway
between floors if you want.

532
00:26:22,520 --> 00:26:25,223
It's pretty crazy, so,

533
00:26:27,310 --> 00:26:30,210
it's gonna seem like a
mad thing in the future

534
00:26:30,210 --> 00:26:31,883
that people were driving cars.

535
00:26:32,970 --> 00:26:34,620
- So I have a bunch of questions about

536
00:26:34,620 --> 00:26:37,387
the human psychology,
about behavior and so on--

537
00:26:38,720 --> 00:26:41,050
- That's moot, it's totally moot.

538
00:26:41,050 --> 00:26:45,523
- Because you have faith in the AI system,

539
00:26:46,370 --> 00:26:50,520
not faith but, both on the hardware side

540
00:26:50,520 --> 00:26:52,960
and the deep learning approach
of learning from data,

541
00:26:52,960 --> 00:26:55,690
will make it just far safer than humans.

542
00:26:55,690 --> 00:26:57,270
- Yeah, exactly.

543
00:26:57,270 --> 00:26:59,470
- Recently there were a few hackers,

544
00:26:59,470 --> 00:27:02,170
who tricked Autopilot to
act in unexpected ways

545
00:27:02,170 --> 00:27:03,970
for the adversarial examples.

546
00:27:03,970 --> 00:27:06,540
So we all know that neural network systems

547
00:27:06,540 --> 00:27:08,500
are very sensitive to minor disturbances,

548
00:27:08,500 --> 00:27:11,270
these adversarial examples, on input.

549
00:27:11,270 --> 00:27:12,300
Do you think it's possible

550
00:27:12,300 --> 00:27:14,140
to defend against something like this,

551
00:27:14,140 --> 00:27:19,120
for the industry?
- Sure (both laugh), yeah.

552
00:27:19,120 --> 00:27:22,693
- Can you elaborate on the
confidence behind that answer?

553
00:27:25,479 --> 00:27:26,670
- A neural net is just basically a bunch

554
00:27:26,670 --> 00:27:28,397
of matrix math.

555
00:27:28,397 --> 00:27:30,743
But you have to be a very sophisticated,

556
00:27:31,750 --> 00:27:33,370
somebody who really
understands neural nets

557
00:27:33,370 --> 00:27:37,400
and basically reverse-engineer
how the matrix

558
00:27:37,400 --> 00:27:40,540
is being built, and then
create a little thing

559
00:27:40,540 --> 00:27:44,160
that's just exactly causes the matrix math

560
00:27:44,160 --> 00:27:45,460
to be slightly off.

561
00:27:45,460 --> 00:27:48,873
But it's very easy to
block that by having,

562
00:27:49,730 --> 00:27:51,850
what would basically negative recognition,

563
00:27:51,850 --> 00:27:53,950
it's like if the system sees something

564
00:27:53,950 --> 00:27:57,753
that looks like a matrix hack, exclude it.

565
00:27:59,940 --> 00:28:01,643
It's such a easy thing to do.

566
00:28:02,730 --> 00:28:06,260
- So learn both on the valid
data and the invalid data,

567
00:28:06,260 --> 00:28:08,260
so basically learn on
the adversarial examples

568
00:28:08,260 --> 00:28:09,790
to be able to exclude them.

569
00:28:09,790 --> 00:28:12,360
- Yeah, you like basically wanna both know

570
00:28:12,360 --> 00:28:16,160
what is a car and what
is definitely not a car.

571
00:28:16,160 --> 00:28:17,880
And you train for, this is a car,

572
00:28:17,880 --> 00:28:19,130
and this is definitely not a car.

573
00:28:19,130 --> 00:28:20,680
Those are two different things.

574
00:28:21,710 --> 00:28:23,870
People have no idea of neural nets really,

575
00:28:23,870 --> 00:28:25,550
They probably think neural nets involves,

576
00:28:25,550 --> 00:28:27,780
a fishing net or something (Lex laughs).

577
00:28:29,220 --> 00:28:34,220
- So, as you know, taking
a step beyond just Tesla

578
00:28:34,840 --> 00:28:37,810
and Autopilot, current
deep learning approaches

579
00:28:37,810 --> 00:28:40,510
still seem, in some ways,

580
00:28:40,510 --> 00:28:44,690
to be far from general
intelligence systems.

581
00:28:44,690 --> 00:28:46,770
Do you think the current approaches

582
00:28:46,770 --> 00:28:49,780
will take us to general intelligence,

583
00:28:49,780 --> 00:28:53,873
or do totally new ideas
need to be invented?

584
00:28:55,720 --> 00:28:57,260
- I think we're missing a few key ideas

585
00:28:57,260 --> 00:29:01,913
for artificial general intelligence.

586
00:29:05,180 --> 00:29:07,393
But it's gonna be upon us very quickly,

587
00:29:08,880 --> 00:29:12,480
and then we'll need to
figure out what shall we do,

588
00:29:12,480 --> 00:29:13,973
if we even have that choice.

589
00:29:16,340 --> 00:29:18,360
It's amazing how people
can't differentiate

590
00:29:18,360 --> 00:29:22,760
between, say, the narrow
AI that allows a car

591
00:29:22,760 --> 00:29:27,760
to figure out what a lane
line is, and navigate streets,

592
00:29:27,920 --> 00:29:30,540
versus general intelligence.

593
00:29:30,540 --> 00:29:33,170
Like these are just very different things.

594
00:29:33,170 --> 00:29:35,840
Like your toaster and your
computer are both machines,

595
00:29:35,840 --> 00:29:38,680
but one's much more
sophisticated than another.

596
00:29:38,680 --> 00:29:41,470
- You're confident with
Tesla you can create

597
00:29:41,470 --> 00:29:43,670
the world's best toaster--

598
00:29:43,670 --> 00:29:45,260
- The world's best toaster, yes.

599
00:29:45,260 --> 00:29:46,930
The world's best self-driving...

600
00:29:50,240 --> 00:29:54,513
yes, to me right now this
seems game, set and match.

601
00:29:55,409 --> 00:29:57,230
I mean, I don't want us to be complacent

602
00:29:57,230 --> 00:29:59,010
or over-confident, but that's what it,

603
00:29:59,010 --> 00:30:02,690
that is just literally
how it appears right now,

604
00:30:02,690 --> 00:30:06,360
I could be wrong, but it
appears to be the case

605
00:30:06,360 --> 00:30:09,683
that Tesla is vastly ahead of everyone.

606
00:30:10,950 --> 00:30:12,470
- Do you think we will ever create

607
00:30:12,470 --> 00:30:16,390
an AI system that we can
love, and loves us back

608
00:30:16,390 --> 00:30:18,790
in a deep meaningful way,
like in the movie Her?

609
00:30:20,290 --> 00:30:23,500
- I think AI will
capable of convincing you

610
00:30:23,500 --> 00:30:25,880
to fall in love with it very well.

611
00:30:25,880 --> 00:30:27,780
- And that's different than us humans?

612
00:30:29,340 --> 00:30:30,340
- You know, we start getting into

613
00:30:30,340 --> 00:30:33,090
a metaphysical question of, do emotions

614
00:30:33,090 --> 00:30:34,700
and thoughts exist in a different realm

615
00:30:34,700 --> 00:30:35,650
than the physical?

616
00:30:35,650 --> 00:30:38,070
And maybe they do, maybe
they don't, I don't know.

617
00:30:38,070 --> 00:30:41,903
But from a physics standpoint,
I tend to think of things,

618
00:30:42,790 --> 00:30:47,490
you know, like physics was
my main sort of training,

619
00:30:47,490 --> 00:30:51,450
and from a physics
standpoint, essentially,

620
00:30:51,450 --> 00:30:54,260
if it loves you in a
way that you can't tell

621
00:30:54,260 --> 00:30:56,253
whether it's real or not, it is real.

622
00:30:57,660 --> 00:30:59,140
- That's a physics view of love.

623
00:30:59,140 --> 00:31:04,140
- Yeah (laughs), if you
cannot prove that it does not,

624
00:31:06,000 --> 00:31:08,120
if there's no test that you can apply

625
00:31:08,960 --> 00:31:10,133
that would make it,

626
00:31:13,820 --> 00:31:15,940
allow you to tell the difference,

627
00:31:15,940 --> 00:31:17,170
then there is no difference.

628
00:31:17,170 --> 00:31:20,913
- Right, and it's similar to
seeing our world a simulation,

629
00:31:20,913 --> 00:31:23,500
they may not be a test to
tell the difference between

630
00:31:23,500 --> 00:31:24,978
what the real world
- Yes.

631
00:31:24,978 --> 00:31:26,320
- and the simulation, and therefore,

632
00:31:26,320 --> 00:31:27,890
from a physics perspective,

633
00:31:27,890 --> 00:31:29,380
it might as well be the same thing.

634
00:31:29,380 --> 00:31:32,190
- Yes, and there may
be ways to test whether

635
00:31:32,190 --> 00:31:34,780
it's a simulation, there might be,

636
00:31:34,780 --> 00:31:36,060
I'm not saying there aren't.

637
00:31:36,060 --> 00:31:37,390
But you could certainly imagine that

638
00:31:37,390 --> 00:31:39,380
a simulation could correct,

639
00:31:39,380 --> 00:31:41,530
that once an entity in
the simulation found

640
00:31:41,530 --> 00:31:43,120
a way to detect the simulation,

641
00:31:43,120 --> 00:31:47,430
it could either pause the simulation,

642
00:31:47,430 --> 00:31:49,930
start a new simulation, or
do one of many other things

643
00:31:49,930 --> 00:31:51,683
that then corrects for that error.

644
00:31:53,170 --> 00:31:57,540
- So when, maybe you,
or somebody else creates

645
00:31:57,540 --> 00:32:02,540
an AGI system, and you get
to ask her one question,

646
00:32:03,020 --> 00:32:04,420
what would that question be?

647
00:32:17,050 --> 00:32:18,713
- What's outside the simulation?

648
00:32:21,730 --> 00:32:23,173
- Elon, thank you so
much for talking today,

649
00:32:23,173 --> 00:32:24,290
it's a pleasure.

650
00:32:24,290 --> 00:32:25,440
- All right, thank you.


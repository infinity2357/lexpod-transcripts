1
00:00:00,470 --> 00:00:03,210
- The following is a
conversation with Eric Schmidt.

2
00:00:03,210 --> 00:00:05,200
He was the CEO of Google for 10 years

3
00:00:05,200 --> 00:00:06,820
and a chairman for six more,

4
00:00:06,820 --> 00:00:10,130
guiding the company through
an incredible period of growth

5
00:00:10,130 --> 00:00:13,000
and a series of
world-changing innovations.

6
00:00:13,000 --> 00:00:15,350
He is one of the most impactful leaders

7
00:00:15,350 --> 00:00:17,430
in the era of the internet

8
00:00:17,430 --> 00:00:20,950
and the powerful voice for
the promise of technology

9
00:00:20,950 --> 00:00:22,360
in our society.

10
00:00:22,360 --> 00:00:24,830
It was truly an honor to speak with him

11
00:00:24,830 --> 00:00:28,660
as part of the MIT course on
artificial general intelligence

12
00:00:28,660 --> 00:00:31,940
and the Artificial Intelligence podcast.

13
00:00:31,940 --> 00:00:36,213
And now, here's my
conversation with Eric Schmidt.

14
00:00:37,050 --> 00:00:38,060
What was the first moment

15
00:00:38,060 --> 00:00:40,033
when you fell in love with technology?

16
00:00:40,940 --> 00:00:44,420
- I grew up in 1960's as a boy

17
00:00:44,420 --> 00:00:46,860
where every boy wanted to be an astronaut

18
00:00:46,860 --> 00:00:48,940
and part of the space program.

19
00:00:48,940 --> 00:00:51,370
So like everyone else of my age,

20
00:00:51,370 --> 00:00:54,380
we would go out to the cow
pasture behind my house,

21
00:00:54,380 --> 00:00:56,300
which was literally a cow pasture,

22
00:00:56,300 --> 00:00:58,570
and we would shoot model rockets off,

23
00:00:58,570 --> 00:01:00,280
and that I think is the beginning.

24
00:01:00,280 --> 00:01:03,560
And of course generationally today,

25
00:01:03,560 --> 00:01:05,810
it would be video games and
all of the amazing things

26
00:01:05,810 --> 00:01:08,233
that you can do online with computers.

27
00:01:09,130 --> 00:01:12,040
- [Lex] There's a
transformative inspiring aspect

28
00:01:12,040 --> 00:01:15,050
of science and math that maybe rockets

29
00:01:15,936 --> 00:01:17,460
would instill in individuals.

30
00:01:17,460 --> 00:01:20,180
You mentioned yesterday
that eighth grade math

31
00:01:20,180 --> 00:01:22,220
is where the journey through
mathematical universe

32
00:01:22,220 --> 00:01:23,830
diverges for many people.

33
00:01:23,830 --> 00:01:26,950
It's this fork in the roadway.

34
00:01:26,950 --> 00:01:30,193
There's a professor of math
at Berkeley, Edward Franco.

35
00:01:31,250 --> 00:01:32,540
I'm not sure if you're familiar with him.

36
00:01:32,540 --> 00:01:33,373
- I am.

37
00:01:33,373 --> 00:01:35,480
- [Lex] He has written this amazing book

38
00:01:35,480 --> 00:01:37,750
I recommend to everybody
called Love and Math.

39
00:01:37,750 --> 00:01:39,617
Two of my favorite words.

40
00:01:39,617 --> 00:01:41,780
(laughs)

41
00:01:41,780 --> 00:01:46,740
He says that if painting
was taught like math,

42
00:01:46,740 --> 00:01:49,690
then students would be
asked to paint a fence.

43
00:01:49,690 --> 00:01:52,490
It's just his analogy of
essentially how math is taught.

44
00:01:53,435 --> 00:01:54,650
So you never get a chance to discover

45
00:01:54,650 --> 00:01:57,200
the beauty of the art of painting

46
00:01:57,200 --> 00:01:59,310
or the beauty of the art of math.

47
00:01:59,310 --> 00:02:03,893
So how, when, and where did
you discover that beauty?

48
00:02:05,270 --> 00:02:08,080
- I think what happens
with people like myself

49
00:02:08,080 --> 00:02:11,410
is that you're math-enabled pretty early,

50
00:02:11,410 --> 00:02:13,300
and all of the sudden you discover

51
00:02:13,300 --> 00:02:16,600
that you can use that to
discover new insights.

52
00:02:16,600 --> 00:02:19,120
The great scientists
will all tell a story.

53
00:02:19,120 --> 00:02:22,060
The men and women who are fantastic today,

54
00:02:22,060 --> 00:02:24,640
it's somewhere when they were
in high school or in college

55
00:02:24,640 --> 00:02:25,680
they discovered that they could

56
00:02:25,680 --> 00:02:27,840
discover something themselves.

57
00:02:27,840 --> 00:02:29,910
And that sense of building something,

58
00:02:29,910 --> 00:02:32,320
of having an impact that you own

59
00:02:32,320 --> 00:02:35,510
drives knowledge acquisition and learning.

60
00:02:35,510 --> 00:02:37,080
In my case, it was programming

61
00:02:37,080 --> 00:02:39,850
and the notion that I could build things

62
00:02:39,850 --> 00:02:41,130
that had not existed,

63
00:02:41,130 --> 00:02:44,440
that I had built that had my name of it.

64
00:02:44,440 --> 00:02:46,210
And this was before open-source,

65
00:02:46,210 --> 00:02:49,130
but you could think of it as
open-source contributions.

66
00:02:49,130 --> 00:02:51,820
So today if I were a 16
or a 17-year-old boy,

67
00:02:51,820 --> 00:02:54,730
I'm sure that I would aspire
as a computer scientist

68
00:02:54,730 --> 00:02:58,100
to make a contribution
like the open-source heroes

69
00:02:58,100 --> 00:02:58,950
of the world today.

70
00:02:58,950 --> 00:03:00,410
That would be what would be driving me,

71
00:03:00,410 --> 00:03:02,150
and I would be trying and learning,

72
00:03:02,150 --> 00:03:06,670
and making mistakes and so
forth in the ways that it works.

73
00:03:06,670 --> 00:03:10,000
The repository that GitHub represents

74
00:03:10,000 --> 00:03:12,240
and that open-source libraries represent

75
00:03:12,240 --> 00:03:14,960
is an enormous bank of knowledge

76
00:03:14,960 --> 00:03:17,150
of all of the people who are doing that.

77
00:03:17,150 --> 00:03:19,600
And one of the lessons
that I learned at Google

78
00:03:19,600 --> 00:03:21,510
was that the world is a very big place,

79
00:03:21,510 --> 00:03:23,600
and there's an awful lot of smart people.

80
00:03:23,600 --> 00:03:26,360
And an awful lot of
them are underutilized.

81
00:03:26,360 --> 00:03:28,990
So here's an opportunity, for example,

82
00:03:28,990 --> 00:03:31,730
building parts or programs,
building new ideas,

83
00:03:31,730 --> 00:03:33,873
to contribute to the greater of society.

84
00:03:36,590 --> 00:03:38,490
- [Lex] So in that moment in the 70's,

85
00:03:38,490 --> 00:03:40,700
the inspiring moment
where there was nothing

86
00:03:40,700 --> 00:03:42,860
and then you cerated
something through programming,

87
00:03:42,860 --> 00:03:44,690
that magical moment.

88
00:03:44,690 --> 00:03:49,220
So in 1975, I think, you
created a program called Lex,

89
00:03:49,220 --> 00:03:51,520
which I especially like
because my name is Lex.

90
00:03:51,520 --> 00:03:54,650
So thank you, thank you
for creating a brand

91
00:03:54,650 --> 00:03:58,300
that established a reputation
that's long-lasting, reliable,

92
00:03:58,300 --> 00:04:01,200
and has a big impact on the
world and is still used today.

93
00:04:01,200 --> 00:04:02,840
So thank you for that.

94
00:04:02,840 --> 00:04:07,490
But more seriously, in that time,

95
00:04:07,490 --> 00:04:09,470
in the 70's as an engineer

96
00:04:09,470 --> 00:04:11,220
personal computers were being born.

97
00:04:12,590 --> 00:04:14,170
Did you think you would be able to predict

98
00:04:14,170 --> 00:04:18,930
the 80's, 90's and the noughts
of where computers would go?

99
00:04:18,930 --> 00:04:22,183
- I'm sure I could not and
would not have gotten it right.

100
00:04:23,250 --> 00:04:25,470
I was the beneficiary of the great work

101
00:04:25,470 --> 00:04:29,090
of many many people who
saw it clearer than I did.

102
00:04:29,090 --> 00:04:32,570
With Lex, I worked with a
fellow named Michael Lesk

103
00:04:32,570 --> 00:04:34,030
who was my supervisor,

104
00:04:34,030 --> 00:04:36,330
and he essentially helped me architect

105
00:04:36,330 --> 00:04:39,220
and deliver a system
that's still in use today.

106
00:04:39,220 --> 00:04:42,260
After that, I worked at Xerox
Palo Alto Research Center

107
00:04:42,260 --> 00:04:43,710
where the Alto was invented,

108
00:04:43,710 --> 00:04:46,100
and the Alto is the predecessor

109
00:04:46,100 --> 00:04:50,220
of the modern personal computer,
or Macintosh and so forth.

110
00:04:50,220 --> 00:04:52,350
And the Altos were very rare,

111
00:04:52,350 --> 00:04:55,290
and I had to drive an hour
from Berkeley to go use them,

112
00:04:55,290 --> 00:04:57,410
but I made a point of skipping classes

113
00:04:57,410 --> 00:04:59,600
and doing whatever it took

114
00:04:59,600 --> 00:05:02,550
to have access to this
extraordinary achievement.

115
00:05:02,550 --> 00:05:04,930
I knew that they were consequential.

116
00:05:04,930 --> 00:05:08,290
What I did not understand was scaling.

117
00:05:08,290 --> 00:05:09,900
I did not understand what would happen

118
00:05:09,900 --> 00:05:12,740
when you had 100 million
as opposed to 100.

119
00:05:12,740 --> 00:05:16,310
And so since then, and I have
learned the benefit of scale,

120
00:05:16,310 --> 00:05:17,500
I always look for things

121
00:05:17,500 --> 00:05:19,450
which are going to scale to platforms,

122
00:05:19,450 --> 00:05:23,100
so mobile phones, Android,
all of those things.

123
00:05:23,100 --> 00:05:25,406
The world is a numerous,

124
00:05:25,406 --> 00:05:27,430
there are many many people in the world.

125
00:05:27,430 --> 00:05:28,550
People really have needs.

126
00:05:28,550 --> 00:05:29,990
They really will use these platforms,

127
00:05:29,990 --> 00:05:32,610
and you can build big
businesses on top of them.

128
00:05:32,610 --> 00:05:33,443
- [Lex] So it's interesting,

129
00:05:33,443 --> 00:05:34,920
so when you see a piece of technology,

130
00:05:34,920 --> 00:05:37,350
now you think what will
this technology look like

131
00:05:37,350 --> 00:05:39,090
when it's in the hands
of a billion people.

132
00:05:39,090 --> 00:05:40,150
- That's right.

133
00:05:40,150 --> 00:05:45,000
So an example would be that the
market is so competitive now

134
00:05:45,000 --> 00:05:46,990
that if you can't figure out a way

135
00:05:46,990 --> 00:05:50,850
for something to have a million
users or a billion users,

136
00:05:50,850 --> 00:05:53,150
it probably is not going to be successful

137
00:05:53,150 --> 00:05:56,830
because something else will
become the general platform

138
00:05:56,830 --> 00:06:01,090
and your idea will become a lost idea

139
00:06:01,090 --> 00:06:04,310
or a specialized service
with relatively few users.

140
00:06:04,310 --> 00:06:05,940
So it's a path to generality.

141
00:06:05,940 --> 00:06:07,700
It's a path to general platform use.

142
00:06:07,700 --> 00:06:10,090
It's a path to broad applicability.

143
00:06:10,090 --> 00:06:12,680
Now there are plenty of good
businesses that are tiny,

144
00:06:12,680 --> 00:06:14,910
so luxury goods for example,

145
00:06:14,910 --> 00:06:18,540
but if you want to have
an impact at scale,

146
00:06:18,540 --> 00:06:21,390
you have to look for things
which are of common value,

147
00:06:21,390 --> 00:06:23,320
common pricing, common distribution,

148
00:06:23,320 --> 00:06:24,760
and solve common problems.

149
00:06:24,760 --> 00:06:26,180
They're problems that everyone has.

150
00:06:26,180 --> 00:06:28,140
And by the way, people
have lots of problems.

151
00:06:28,140 --> 00:06:31,200
Information, medicine, health,
education, and so forth,

152
00:06:31,200 --> 00:06:33,040
work on those problems.

153
00:06:33,040 --> 00:06:34,280
- [Lex] Like you said,

154
00:06:34,280 --> 00:06:36,820
you're a big fan of the middle class--

155
00:06:36,820 --> 00:06:37,870
- 'Cause there's so many of them.

156
00:06:37,870 --> 00:06:38,790
- [Lex] There's so many of them.

157
00:06:38,790 --> 00:06:40,200
- By definition.

158
00:06:40,200 --> 00:06:43,817
- [Lex] So any product, any
thing that has a huge impact

159
00:06:43,817 --> 00:06:47,530
and improves their lives is
a great business decision,

160
00:06:47,530 --> 00:06:48,770
and it's just good for society.

161
00:06:48,770 --> 00:06:50,990
- And there's nothing
wrong with starting off

162
00:06:50,990 --> 00:06:53,720
in the high-end as long as you have a plan

163
00:06:53,720 --> 00:06:55,470
to get to the middle class.

164
00:06:55,470 --> 00:06:56,820
There's nothing wrong with starting

165
00:06:56,820 --> 00:06:58,400
with a specialized market in order

166
00:06:58,400 --> 00:07:01,050
to learn and to build and to fund things.

167
00:07:01,050 --> 00:07:02,590
So you start luxury market

168
00:07:02,590 --> 00:07:04,520
to build a general purpose market.

169
00:07:04,520 --> 00:07:07,550
But if you define yourself
as only a narrow market,

170
00:07:07,550 --> 00:07:10,960
someone else can come along
with a general purpose market

171
00:07:10,960 --> 00:07:12,380
that can push you to the corner,

172
00:07:12,380 --> 00:07:14,300
can restrict the scale of operation,

173
00:07:14,300 --> 00:07:17,860
can force you to be a lesser
impact than you might be.

174
00:07:17,860 --> 00:07:19,940
So it's very important to think in terms

175
00:07:19,940 --> 00:07:22,380
of broad businesses and broad impact,

176
00:07:22,380 --> 00:07:25,063
even if you start in a
little corner somewhere.

177
00:07:26,310 --> 00:07:28,260
- [Lex] So as you look to the 70's

178
00:07:28,260 --> 00:07:33,160
but also in the decades to
come and you saw computers,

179
00:07:33,160 --> 00:07:34,900
did you see them as tools,

180
00:07:34,900 --> 00:07:39,900
or was there a little
element of another entity?

181
00:07:40,330 --> 00:07:43,470
I remember a quote saying AI began

182
00:07:43,470 --> 00:07:46,170
with our dream to create the gods.

183
00:07:46,170 --> 00:07:48,670
Is there a feeling when
you wrote that program

184
00:07:48,670 --> 00:07:51,330
that you were creating another entity,

185
00:07:51,330 --> 00:07:52,860
giving life to something?

186
00:07:52,860 --> 00:07:54,680
- I wish I could say otherwise,

187
00:07:54,680 --> 00:07:58,770
but I simply found the
technology platforms so exciting.

188
00:07:58,770 --> 00:08:00,520
That's what I was focused on.

189
00:08:00,520 --> 00:08:03,440
I think the majority of the
people that I've worked with,

190
00:08:03,440 --> 00:08:06,750
and there are a few exceptions,
Steve Jobs being an example,

191
00:08:06,750 --> 00:08:10,020
really saw this a great
technological play.

192
00:08:10,020 --> 00:08:13,760
I think relatively few of the
technical people understood

193
00:08:13,760 --> 00:08:15,440
the scale of its impact.

194
00:08:15,440 --> 00:08:19,740
So I used MCP which is
a predecessor to TCP/IP.

195
00:08:19,740 --> 00:08:21,240
It just made sense to connect things.

196
00:08:21,240 --> 00:08:23,840
We didn't think of it
in terms of the internet

197
00:08:23,840 --> 00:08:26,160
and then companies and then Facebook

198
00:08:26,160 --> 00:08:29,210
and then Twitter and then
politics and so forth.

199
00:08:29,210 --> 00:08:30,760
We never did that build.

200
00:08:30,760 --> 00:08:32,870
We didn't have that vision.

201
00:08:32,870 --> 00:08:33,973
And I think most people,

202
00:08:33,973 --> 00:08:38,010
it's a rare person who can
see compounding at scale.

203
00:08:38,010 --> 00:08:39,060
Most people can see,

204
00:08:39,060 --> 00:08:41,450
if you ask people to predict the future,

205
00:08:41,450 --> 00:08:42,283
they'll give you an answer

206
00:08:42,283 --> 00:08:44,490
of six to nine months or 12 months

207
00:08:44,490 --> 00:08:47,480
because that's about as
far as people can imagine.

208
00:08:47,480 --> 00:08:48,690
But there's an old saying,

209
00:08:48,690 --> 00:08:49,790
which actually was attributed

210
00:08:49,790 --> 00:08:52,070
to a professor at MIT a long time ago,

211
00:08:52,070 --> 00:08:56,400
that we overestimate what
can be done in one year.

212
00:08:56,400 --> 00:09:00,130
We underestimate was
can be done in a decade.

213
00:09:00,130 --> 00:09:02,480
And there's a great deal of evidence

214
00:09:02,480 --> 00:09:03,860
that these core platforms

215
00:09:03,860 --> 00:09:06,683
of hardware and software take a decade.

216
00:09:07,740 --> 00:09:09,450
So think about self-driving cars.

217
00:09:09,450 --> 00:09:12,100
Self-driving cars were
thought about in the 90's.

218
00:09:12,100 --> 00:09:13,340
There were projects around them.

219
00:09:13,340 --> 00:09:17,130
The first DARPA Grand
Challenge was roughly 2004.

220
00:09:17,130 --> 00:09:19,500
So that's roughly 15 years ago.

221
00:09:19,500 --> 00:09:22,070
And today we have
self-driving cars operating

222
00:09:22,070 --> 00:09:25,280
at a city in Arizona, so 15 years.

223
00:09:25,280 --> 00:09:26,640
And we still have a ways to go

224
00:09:26,640 --> 00:09:28,640
before they're more generally available.

225
00:09:31,610 --> 00:09:33,800
- [Lex] So you've spoken
about the importance,

226
00:09:33,800 --> 00:09:37,110
you just talked about
predicting into the future.

227
00:09:37,110 --> 00:09:38,240
You've spoken about the importance

228
00:09:38,240 --> 00:09:40,740
of thinking five years ahead

229
00:09:40,740 --> 00:09:42,687
and having a plan for those five years.

230
00:09:42,687 --> 00:09:44,740
- The way to say it is that

231
00:09:44,740 --> 00:09:47,530
almost everybody has a one-year plan.

232
00:09:47,530 --> 00:09:50,940
Almost no one has a proper five-year plan.

233
00:09:50,940 --> 00:09:52,800
And the key thing to have
on the five-year plan

234
00:09:52,800 --> 00:09:55,260
is having a model for
what's going to happen

235
00:09:55,260 --> 00:09:56,870
under the underlying platforms.

236
00:09:56,870 --> 00:09:58,173
So here's an example.

237
00:10:00,010 --> 00:10:01,140
Moore's law as we know it,

238
00:10:01,140 --> 00:10:03,230
the thing that powered improvement

239
00:10:03,230 --> 00:10:05,800
in CPUs has largely halted

240
00:10:05,800 --> 00:10:07,600
in its traditional shrinking mechanisms

241
00:10:07,600 --> 00:10:10,340
because the costs have just gotten so high

242
00:10:10,340 --> 00:10:12,160
and it's getting harder and harder.

243
00:10:12,160 --> 00:10:14,600
But there's plenty of
algorithmic improvements

244
00:10:14,600 --> 00:10:16,580
and specialized hardware improvements.

245
00:10:16,580 --> 00:10:19,640
So you need to understand the
nature of those improvements

246
00:10:19,640 --> 00:10:21,940
and where they'll go
in order to understand

247
00:10:21,940 --> 00:10:24,330
how it will change the platform.

248
00:10:24,330 --> 00:10:26,100
In the area of network conductivity,

249
00:10:26,100 --> 00:10:29,450
what are the gains that are
to be possible in wireless?

250
00:10:29,450 --> 00:10:33,410
It looks like there's
an enormous expansion

251
00:10:33,410 --> 00:10:36,950
of wireless conductivity
at many different bands

252
00:10:36,950 --> 00:10:38,690
and that we will primarily,

253
00:10:38,690 --> 00:10:39,900
historical I've always thought

254
00:10:39,900 --> 00:10:42,130
that we were primarily
going to be using fiber,

255
00:10:42,130 --> 00:10:43,520
but now it looks like
we're going to be using

256
00:10:43,520 --> 00:10:47,410
fiber plus very powerful high bandwidth

257
00:10:47,410 --> 00:10:51,530
sort of short distance conductivity
to bridge the last mile.

258
00:10:51,530 --> 00:10:53,060
That's an amazing achievement.

259
00:10:53,060 --> 00:10:54,440
If you know that,

260
00:10:54,440 --> 00:10:56,910
then you're going to build
your systems differently.

261
00:10:56,910 --> 00:10:59,640
By the way, those networks have
different latency properties

262
00:10:59,640 --> 00:11:01,620
because they're more symmetric.

263
00:11:01,620 --> 00:11:03,913
The algorithms feel
faster for that reason.

264
00:11:05,000 --> 00:11:06,310
- [Lex] And so when you think about,

265
00:11:06,310 --> 00:11:09,870
whether it's fiber or just
technologies in general,

266
00:11:09,870 --> 00:11:14,190
so there's this Barbara
Wootton poem or quote

267
00:11:14,190 --> 00:11:15,860
that I really like.

268
00:11:15,860 --> 00:11:18,240
It's from the champions of the impossible,

269
00:11:18,240 --> 00:11:20,340
rather than the slaves of the possible,

270
00:11:20,340 --> 00:11:23,240
that evolution draws its creative force.

271
00:11:23,240 --> 00:11:25,990
So in predicting the next five years,

272
00:11:25,990 --> 00:11:29,210
I'd like to talk about the
impossible and the possible.

273
00:11:29,210 --> 00:11:32,280
- Well, and again, one of the
great things about humanity

274
00:11:32,280 --> 00:11:33,913
is that we produce dreamers.

275
00:11:34,800 --> 00:11:37,770
We literally have people who
have a vision and a dream.

276
00:11:37,770 --> 00:11:40,590
They are, if you will,
disagreeable in the sense

277
00:11:40,590 --> 00:11:42,750
that they disagree with the,

278
00:11:42,750 --> 00:11:45,819
they disagree with what
the sort of zeitgeist is.

279
00:11:45,819 --> 00:11:48,030
They say there is another way.

280
00:11:48,030 --> 00:11:49,070
They have a belief.

281
00:11:49,070 --> 00:11:50,280
They have a vision.

282
00:11:50,280 --> 00:11:51,820
If you look at science,

283
00:11:51,820 --> 00:11:55,293
science is always marked by such people

284
00:11:55,293 --> 00:11:58,360
who went against some conventional wisdom,

285
00:11:58,360 --> 00:12:00,210
collected the knowledge at the time,

286
00:12:00,210 --> 00:12:03,630
and assembled it in a way that
produced a powerful platform.

287
00:12:03,630 --> 00:12:08,320
- [Lex] And you've been
amazingly honest about,

288
00:12:08,320 --> 00:12:09,880
in an inspiring way,

289
00:12:09,880 --> 00:12:12,190
about things you've been
wrong about predicting,

290
00:12:12,190 --> 00:12:14,690
and you've obviously been
right about a lot of things.

291
00:12:14,690 --> 00:12:19,060
But in this kind of tension,

292
00:12:19,060 --> 00:12:22,010
how do you balance as a company predicting

293
00:12:22,010 --> 00:12:26,590
the next five years
planning for the impossible,

294
00:12:26,590 --> 00:12:28,910
listening to those crazy dreamers,

295
00:12:28,910 --> 00:12:33,910
letting them run away and
make the impossible real,

296
00:12:34,330 --> 00:12:35,260
make it happen,

297
00:12:35,260 --> 00:12:38,930
and you know that's how
programmers often think,

298
00:12:38,930 --> 00:12:41,750
and slowing things down and saying

299
00:12:41,750 --> 00:12:43,310
well this is the rational,

300
00:12:43,310 --> 00:12:44,790
this is the possible,

301
00:12:44,790 --> 00:12:49,130
the pragmatic, the dreamer
versus the pragmatist that is.

302
00:12:49,130 --> 00:12:51,390
- So it's helpful to have a model

303
00:12:51,390 --> 00:12:56,020
which encourages a
predictable revenue stream

304
00:12:56,020 --> 00:12:58,670
as well as the ability to do new things.

305
00:12:58,670 --> 00:12:59,620
So in Google's case,

306
00:12:59,620 --> 00:13:02,350
we're big enough and well
enough managed and so forth

307
00:13:02,350 --> 00:13:05,200
that we have a pretty good sense
of what our revenue will be

308
00:13:05,200 --> 00:13:07,900
for the next year or two,
at least for a while.

309
00:13:07,900 --> 00:13:11,520
And so we have enough cash generation

310
00:13:11,520 --> 00:13:13,340
that we can make bets.

311
00:13:13,340 --> 00:13:16,790
And indeed, Google has become Alphabet,

312
00:13:16,790 --> 00:13:19,480
so the corporation is
organized around these bets.

313
00:13:19,480 --> 00:13:21,530
And these bets are in areas

314
00:13:21,530 --> 00:13:24,080
of fundamental importance to the world,

315
00:13:24,080 --> 00:13:26,730
whether it's artificial intelligence,

316
00:13:26,730 --> 00:13:29,720
medical technology, self-driving cars,

317
00:13:29,720 --> 00:13:33,200
conductivity through
balloons, on and on and on.

318
00:13:33,200 --> 00:13:35,970
And there's more coming and more coming.

319
00:13:35,970 --> 00:13:38,010
So one way you could express this

320
00:13:38,010 --> 00:13:41,510
is that the current business
is successful enough

321
00:13:41,510 --> 00:13:44,370
that we have the luxury of making bets.

322
00:13:44,370 --> 00:13:45,930
And another one that you could say

323
00:13:45,930 --> 00:13:49,130
is that we have the wisdom
of being able to see

324
00:13:49,130 --> 00:13:51,560
that a corporate structure
needs to be created

325
00:13:51,560 --> 00:13:55,280
to enhance the likelihood of
the success of those bets.

326
00:13:55,280 --> 00:13:58,860
So we essentially turned
ourselves into a conglomerate

327
00:13:58,860 --> 00:14:02,110
of bets and then this
underlying corporation, Google,

328
00:14:02,110 --> 00:14:04,280
which is itself innovative.

329
00:14:04,280 --> 00:14:05,910
So in order to pull this off,

330
00:14:05,910 --> 00:14:08,060
you have to have a
bunch of belief systems,

331
00:14:08,060 --> 00:14:09,580
and one of them is that you have to have

332
00:14:09,580 --> 00:14:11,460
bottoms up and tops down.

333
00:14:11,460 --> 00:14:13,570
The bottoms up we call 20% time,

334
00:14:13,570 --> 00:14:15,770
and the idea is that people
can spend 20% of the time

335
00:14:15,770 --> 00:14:16,960
on whatever they want.

336
00:14:16,960 --> 00:14:19,780
And the top down is that
our founders in particular

337
00:14:19,780 --> 00:14:21,820
have a keen eye on technology,

338
00:14:21,820 --> 00:14:23,950
and they're reviewing things constantly.

339
00:14:23,950 --> 00:14:26,630
So an example would be
they'll hear about an idea

340
00:14:26,630 --> 00:14:28,780
or I'll hear about something
and it sounds interesting.

341
00:14:28,780 --> 00:14:30,440
Let's go visit them,

342
00:14:30,440 --> 00:14:33,120
and then let's begin
to assemble the pieces

343
00:14:33,120 --> 00:14:34,850
to see if that's possible.

344
00:14:34,850 --> 00:14:36,060
And if you do this long enough,

345
00:14:36,060 --> 00:14:39,860
you get pretty good at
predicting what's likely to work.

346
00:14:39,860 --> 00:14:42,090
- [Lex] So that's a beautiful
balance that's struck.

347
00:14:42,090 --> 00:14:45,098
Is this something that
applies at all scale?

348
00:14:45,098 --> 00:14:46,713
- Seems to be.

349
00:14:48,970 --> 00:14:51,873
Sergey, again 15 years ago,

350
00:14:53,120 --> 00:14:56,910
came up with a concept
called 10% of the budget

351
00:14:56,910 --> 00:14:59,060
should be on things that are unrelated.

352
00:14:59,060 --> 00:15:01,000
It was called 70/20/10.

353
00:15:01,000 --> 00:15:03,600
70% of our time on core business,

354
00:15:03,600 --> 00:15:06,840
20% on adjacent business,
and 10% on other.

355
00:15:06,840 --> 00:15:08,790
And he proved mathematically,

356
00:15:08,790 --> 00:15:10,640
of course he's a brilliant mathematician,

357
00:15:10,640 --> 00:15:14,680
that you needed that 10% to
make the sum of the growth work.

358
00:15:14,680 --> 00:15:16,263
And it turns out that he was right.

359
00:15:18,660 --> 00:15:19,970
- [Lex] So getting into the world

360
00:15:19,970 --> 00:15:20,990
of artificial intelligence,

361
00:15:20,990 --> 00:15:25,460
you've talked quite
extensively and effectively

362
00:15:25,460 --> 00:15:28,860
to the impact in the near term,

363
00:15:28,860 --> 00:15:31,493
the positive impact of
artificial intelligence,

364
00:15:33,020 --> 00:15:34,230
especially machine learning

365
00:15:34,230 --> 00:15:38,670
in medical applications and education

366
00:15:38,670 --> 00:15:41,670
and just making information
more accessible.

367
00:15:41,670 --> 00:15:45,940
In the AI community,
there is a kind of debate.

368
00:15:45,940 --> 00:15:47,740
There's this shroud of uncertainty

369
00:15:47,740 --> 00:15:50,540
as we face this new world
of artificial intelligence.

370
00:15:50,540 --> 00:15:55,540
And there is some people like
Elon Musk you've disagreed on,

371
00:15:56,170 --> 00:15:57,720
at least in the degree of emphasis

372
00:15:57,720 --> 00:16:00,770
he places on the existential threat of AI.

373
00:16:00,770 --> 00:16:03,440
So I've spoken with Stuart
Russell, Max Tegmark,

374
00:16:03,440 --> 00:16:05,400
who share Elon Musk's view,

375
00:16:05,400 --> 00:16:09,230
and Yoshua Bengio,
Steven Pinker who do not.

376
00:16:09,230 --> 00:16:11,910
And so there's a lot of very smart people

377
00:16:11,910 --> 00:16:13,940
who are thinking about this stuff,

378
00:16:13,940 --> 00:16:17,160
disagreeing, which is
really healthy, of course.

379
00:16:17,160 --> 00:16:19,140
So what do you think is the healthiest way

380
00:16:19,140 --> 00:16:22,060
for the AI community to,

381
00:16:22,060 --> 00:16:25,560
and really for the general
public to think about AI

382
00:16:25,560 --> 00:16:29,160
and the concern of the technology

383
00:16:29,160 --> 00:16:32,950
being mismanaged in some kind of way.

384
00:16:32,950 --> 00:16:35,100
- So the source of education
for the general public

385
00:16:35,100 --> 00:16:40,100
has been robot killer movies
and Terminator, etcetera.

386
00:16:40,880 --> 00:16:44,530
And the one thing I can
assure you we're not building

387
00:16:44,530 --> 00:16:46,670
are those kinds of solutions.

388
00:16:46,670 --> 00:16:48,440
Furthermore, if they were to show up,

389
00:16:48,440 --> 00:16:51,170
someone would notice and unplug them.

390
00:16:51,170 --> 00:16:53,170
So as exciting as those movies are,

391
00:16:53,170 --> 00:16:54,720
and they're great movies,

392
00:16:54,720 --> 00:16:57,540
were the killer robots to start,

393
00:16:57,540 --> 00:16:59,623
we would find a way to stop them,

394
00:17:00,540 --> 00:17:02,893
so I'm not concerned about that.

395
00:17:04,100 --> 00:17:06,010
And much of this has to do

396
00:17:06,010 --> 00:17:08,580
with the timeframe of conversation.

397
00:17:08,580 --> 00:17:13,360
So you can imagine a
situation 100 years from now

398
00:17:13,360 --> 00:17:15,960
when the human brain is fully understood

399
00:17:15,960 --> 00:17:17,370
in the next generation

400
00:17:17,370 --> 00:17:19,600
and next generation of
brilliant MIT scientists

401
00:17:19,600 --> 00:17:20,980
have figured all this out,

402
00:17:20,980 --> 00:17:25,180
we're gonna have a large
number of ethics questions

403
00:17:25,180 --> 00:17:27,430
around science and thinking and robots

404
00:17:27,430 --> 00:17:29,740
and computers and so forth and so on.

405
00:17:29,740 --> 00:17:32,300
So it depends on the
question of the timeframe.

406
00:17:32,300 --> 00:17:34,820
In the next five to 10 years,

407
00:17:34,820 --> 00:17:37,260
we're not facing those questions.

408
00:17:37,260 --> 00:17:39,130
What we're facing in the
next five to 10 years

409
00:17:39,130 --> 00:17:42,170
is how do we spread this
disruptive technology

410
00:17:42,170 --> 00:17:46,530
as broadly as possible to gain
the maximum benefit of it?

411
00:17:46,530 --> 00:17:48,150
The primary benefit should be

412
00:17:48,150 --> 00:17:50,890
in healthcare and in education.

413
00:17:50,890 --> 00:17:52,360
Healthcare because it's obvious.

414
00:17:52,360 --> 00:17:55,810
We're all the same even though
we somehow believe we're not.

415
00:17:55,810 --> 00:17:57,400
As a medical matter,

416
00:17:57,400 --> 00:17:59,210
the fact that we have
big data about our health

417
00:17:59,210 --> 00:18:00,390
will save lives,

418
00:18:00,390 --> 00:18:04,480
allow us to deal with skin
cancer and other cancers,

419
00:18:04,480 --> 00:18:05,510
ophthalmological problems.

420
00:18:05,510 --> 00:18:08,440
There's people working
on psychological diseases

421
00:18:08,440 --> 00:18:10,270
and so forth using these techniques.

422
00:18:10,270 --> 00:18:11,700
I can go on and on.

423
00:18:11,700 --> 00:18:15,840
The promise of AI in
medicine is extraordinary.

424
00:18:15,840 --> 00:18:18,620
There are many many companies
and start-ups and funds

425
00:18:18,620 --> 00:18:22,160
and solutions and we will all
live much better for that.

426
00:18:22,160 --> 00:18:25,600
The same argument in education.

427
00:18:25,600 --> 00:18:27,700
Can you imagine that for each generation

428
00:18:27,700 --> 00:18:31,860
of child and even adult
you have a tutor educator.

429
00:18:31,860 --> 00:18:34,040
It's AI based that's not a human

430
00:18:34,040 --> 00:18:37,190
but is properly trained
that helps you get smarter,

431
00:18:37,190 --> 00:18:39,320
helps you address your
language difficulties

432
00:18:39,320 --> 00:18:41,400
or your math difficulties
or what have you.

433
00:18:41,400 --> 00:18:43,340
Why don't we focus on those two?

434
00:18:43,340 --> 00:18:46,650
The gain societally of
making humans smarter

435
00:18:46,650 --> 00:18:48,970
and healthier are enormous.

436
00:18:48,970 --> 00:18:51,490
And those translate for
decades and decades,

437
00:18:51,490 --> 00:18:53,090
and we'll all benefit from them.

438
00:18:53,930 --> 00:18:56,350
There are people who are
working on AI safety,

439
00:18:56,350 --> 00:18:58,100
which is the issue that you're describing,

440
00:18:58,100 --> 00:19:00,720
and there are conversations
in the community

441
00:19:00,720 --> 00:19:02,550
that should there be such problems

442
00:19:02,550 --> 00:19:04,440
what should the rules be like?

443
00:19:04,440 --> 00:19:07,590
Google, for example, has
announced its policies

444
00:19:07,590 --> 00:19:10,090
with respect to AI safety,
which I certainly support,

445
00:19:10,090 --> 00:19:12,290
and I think most everybody would support.

446
00:19:12,290 --> 00:19:14,190
And they make sense.

447
00:19:14,190 --> 00:19:16,360
So it helps guide the research.

448
00:19:16,360 --> 00:19:19,580
But the killer robots are
not arriving this year,

449
00:19:19,580 --> 00:19:21,230
and they're not even being built.

450
00:19:22,570 --> 00:19:25,380
- [Lex] And on that line of thinking,

451
00:19:25,380 --> 00:19:26,733
you said the timescale.

452
00:19:27,810 --> 00:19:32,233
In this topic or other topics
have you found a useful,

453
00:19:33,410 --> 00:19:35,630
on the business side or
the intellectual side,

454
00:19:35,630 --> 00:19:37,540
to think beyond five to 10 years,

455
00:19:37,540 --> 00:19:39,400
to think 50 years out?

456
00:19:39,400 --> 00:19:42,030
Has it ever been useful or productive--

457
00:19:42,030 --> 00:19:45,180
- In our industry there
are essentially no examples

458
00:19:45,180 --> 00:19:47,480
of 50 year predictions
that have been correct.

459
00:19:48,870 --> 00:19:50,460
Let's review AI.

460
00:19:50,460 --> 00:19:53,100
AI, which was partially
invented here at MIT

461
00:19:53,100 --> 00:19:57,820
and a couple of other
universities in 1956, 1957, 1958,

462
00:19:57,820 --> 00:20:01,360
the original claims were a decade or two.

463
00:20:01,360 --> 00:20:05,210
And when I was a PhD
student, I studied AI,

464
00:20:05,210 --> 00:20:07,700
and it entered during my looking at it

465
00:20:07,700 --> 00:20:10,400
a period which is known as AI winter

466
00:20:10,400 --> 00:20:12,810
which went on for about 30 years,

467
00:20:12,810 --> 00:20:15,380
which is a whole generation
of science, scientists,

468
00:20:15,380 --> 00:20:16,690
and a whole group of people

469
00:20:16,690 --> 00:20:18,450
who didn't make a lot of progress

470
00:20:18,450 --> 00:20:19,947
because the algorithms had not improved

471
00:20:19,947 --> 00:20:22,100
and the computers had not improved.

472
00:20:22,100 --> 00:20:23,860
It took some brilliant mathematicians

473
00:20:23,860 --> 00:20:25,410
starting with a fellow names Geoff Hinton

474
00:20:25,410 --> 00:20:27,220
at Toronto and Montreal

475
00:20:28,410 --> 00:20:30,810
who basically invented
this deep learning model

476
00:20:30,810 --> 00:20:32,686
which empowers us today.

477
00:20:32,686 --> 00:20:36,110
The seminal work there was 20 years ago,

478
00:20:36,110 --> 00:20:39,990
and in the last 10 years
it's become popularized.

479
00:20:39,990 --> 00:20:43,890
So think about the timeframes
for that level of discovery.

480
00:20:43,890 --> 00:20:45,900
It's very hard to predict.

481
00:20:45,900 --> 00:20:47,730
Many people think that
we'll be flying around

482
00:20:47,730 --> 00:20:50,190
in the equivalent of flying cars.

483
00:20:50,190 --> 00:20:51,210
Who knows?

484
00:20:51,210 --> 00:20:54,470
My own view, if I want
to go out on a limb,

485
00:20:54,470 --> 00:20:56,860
is to say we know a couple of things

486
00:20:56,860 --> 00:20:58,000
about 50 years from now.

487
00:20:58,000 --> 00:21:00,470
We know that they'll be more people alive.

488
00:21:00,470 --> 00:21:02,180
We know that we'll have to have platforms

489
00:21:02,180 --> 00:21:03,560
that are more sustainable

490
00:21:03,560 --> 00:21:07,370
because the earth is limited
in the ways we all know,

491
00:21:07,370 --> 00:21:10,510
and that the kind of platforms
that are gonna get built

492
00:21:10,510 --> 00:21:13,020
will be consistent with the
principles that I've described.

493
00:21:13,020 --> 00:21:15,730
They will be much more
empowering of individuals.

494
00:21:15,730 --> 00:21:17,750
They'll be much more
sensitive to the ecology

495
00:21:17,750 --> 00:21:19,040
'cause they have to be.

496
00:21:19,040 --> 00:21:20,550
They just have to be.

497
00:21:20,550 --> 00:21:21,860
I also think that humans

498
00:21:21,860 --> 00:21:23,790
are going to be a great deal smarter,

499
00:21:23,790 --> 00:21:25,090
and I think they're gonna be a lot smarter

500
00:21:25,090 --> 00:21:27,770
because of the tools that
I've discussed with you,

501
00:21:27,770 --> 00:21:29,210
and of course people will live longer.

502
00:21:29,210 --> 00:21:32,180
Life extension is continuing at a pace,

503
00:21:32,180 --> 00:21:36,070
a baby born today has a reasonable
chance of living to 100,

504
00:21:36,070 --> 00:21:37,120
which is pretty exciting.

505
00:21:37,120 --> 00:21:38,610
It's well past the 21st century,

506
00:21:38,610 --> 00:21:40,630
so we better take care of them.

507
00:21:40,630 --> 00:21:42,620
- [Lex] And you've mentioned
an interesting statistic

508
00:21:42,620 --> 00:21:44,825
on some very large percentage,

509
00:21:44,825 --> 00:21:47,363
60%, 70% of people may live in cities.

510
00:21:48,200 --> 00:21:50,510
- Today more than half
the world lives in cities,

511
00:21:50,510 --> 00:21:53,780
and one of the great stories of humanity

512
00:21:53,780 --> 00:21:57,490
in the last 20 years has been
the rural to urban migration.

513
00:21:57,490 --> 00:21:59,230
This has occurred in the United States.

514
00:21:59,230 --> 00:22:01,150
It's occurred in Europe.

515
00:22:01,150 --> 00:22:04,700
It's occurring in Asia, and
it's occurring in Africa.

516
00:22:04,700 --> 00:22:07,820
When people move to cities,
the cities get more crowded,

517
00:22:07,820 --> 00:22:10,400
but believe it or not
their health gets better.

518
00:22:10,400 --> 00:22:12,310
Their productivity gets better.

519
00:22:12,310 --> 00:22:15,490
Their IQ and educational
capabilities improve.

520
00:22:15,490 --> 00:22:18,337
So it's good news that
people are moving to cities,

521
00:22:18,337 --> 00:22:20,853
but we have to make them livable and safe.

522
00:22:22,787 --> 00:22:27,290
- [Lex] So first of all, you
are but you've also worked with

523
00:22:27,290 --> 00:22:30,010
some of the greatest leaders
in the history of tech.

524
00:22:30,010 --> 00:22:31,400
What insights do you draw

525
00:22:31,400 --> 00:22:35,770
from the difference in
leadership styles of yourself,

526
00:22:35,770 --> 00:22:39,200
Steve Jobs, Elon Musk, Larry Page,

527
00:22:39,200 --> 00:22:42,800
now the new CEO, Sundar Pichai and others,

528
00:22:42,800 --> 00:22:47,800
from the I would say calm
sages to the mad geniuses.

529
00:22:49,530 --> 00:22:52,190
- One of the things that I
learned as a young executive

530
00:22:52,190 --> 00:22:54,863
is that there's no single
formula for leadership.

531
00:22:56,150 --> 00:23:00,010
They try to teach one, but
that's not how it really works.

532
00:23:00,010 --> 00:23:02,700
There are people who just
understand what they need to do

533
00:23:02,700 --> 00:23:04,300
and they need to do it quickly.

534
00:23:04,300 --> 00:23:06,820
Those people are often entrepreneurs.

535
00:23:06,820 --> 00:23:09,070
They just know, and they move fast.

536
00:23:09,070 --> 00:23:09,903
There are other people

537
00:23:09,903 --> 00:23:11,630
who are systems thinkers and planners.

538
00:23:11,630 --> 00:23:14,700
That's more who I am,
somewhat more conservative,

539
00:23:14,700 --> 00:23:16,800
more thorough in execution,

540
00:23:16,800 --> 00:23:18,700
a little bit more risk-adverse.

541
00:23:18,700 --> 00:23:22,046
There's also people who
are sort of slightly insane

542
00:23:22,046 --> 00:23:26,120
in the sense that they are
emphatic and charismatic

543
00:23:26,120 --> 00:23:28,970
and they feel it and they
drive it and so forth.

544
00:23:28,970 --> 00:23:31,420
There's no single formula to success.

545
00:23:31,420 --> 00:23:32,950
There is one thing that unifies

546
00:23:32,950 --> 00:23:34,610
all of the people that you named,

547
00:23:34,610 --> 00:23:36,960
which is very high intelligence.

548
00:23:36,960 --> 00:23:40,260
At the end of the day, the
thing that characterizes

549
00:23:40,260 --> 00:23:43,710
all of them is that they saw
the world quicker, faster.

550
00:23:43,710 --> 00:23:45,770
They processed information faster.

551
00:23:45,770 --> 00:23:46,603
They didn't necessarily

552
00:23:46,603 --> 00:23:48,270
make the right decisions all the time,

553
00:23:48,270 --> 00:23:50,040
but they were on top of it.

554
00:23:50,040 --> 00:23:51,330
And the other thing that's interesting

555
00:23:51,330 --> 00:23:52,230
about all of those people

556
00:23:52,230 --> 00:23:54,240
is that they all started young.

557
00:23:54,240 --> 00:23:57,020
So think about Steve Jobs starting Apple

558
00:23:57,020 --> 00:23:58,450
roughly at 18 or 19.

559
00:23:58,450 --> 00:24:01,690
Think about Bill Gates
staring at roughly 20, 21.

560
00:24:01,690 --> 00:24:03,750
Think about by the time they were 30,

561
00:24:03,750 --> 00:24:06,980
Mark Zuckerburg a good
example at 19 or 20,

562
00:24:06,980 --> 00:24:10,690
by the time they were
30, they had 10 years,

563
00:24:10,690 --> 00:24:13,770
at 30 years old they had
10 years of experience

564
00:24:13,770 --> 00:24:16,180
of dealing with people and products

565
00:24:16,180 --> 00:24:19,820
and shipments and the press
and business and so forth.

566
00:24:19,820 --> 00:24:22,810
It's incredible how
much experience they had

567
00:24:22,810 --> 00:24:25,270
compared to the rest of us
who are busy getting our PhDs.

568
00:24:25,270 --> 00:24:26,140
- [Lex] Yes, exactly.

569
00:24:26,140 --> 00:24:28,520
- So we should celebrate these people

570
00:24:28,520 --> 00:24:31,390
because they've just
had more life experience

571
00:24:32,260 --> 00:24:34,450
and that helps them form the judgment.

572
00:24:34,450 --> 00:24:36,173
At the end of the day,

573
00:24:37,390 --> 00:24:40,080
when you're at the top
of these organizations,

574
00:24:40,080 --> 00:24:42,493
all of the easy questions
have been dealt with.

575
00:24:43,570 --> 00:24:45,710
How should we design the buildings?

576
00:24:45,710 --> 00:24:48,260
Where should we put the
colors on our products?

577
00:24:48,260 --> 00:24:50,123
What should the box look like?

578
00:24:52,350 --> 00:24:54,520
That's why it's so interesting
to be in these rooms.

579
00:24:54,520 --> 00:24:56,500
The problems that they face

580
00:24:56,500 --> 00:24:58,390
in terms of the way they operate,

581
00:24:58,390 --> 00:25:00,850
the way they deal with their
employees, their customers,

582
00:25:00,850 --> 00:25:03,990
their innovation are
profoundly challenging.

583
00:25:03,990 --> 00:25:08,990
Each of the companies is
demonstrably different culturally.

584
00:25:09,430 --> 00:25:11,780
They are not, in fact, cut of the same.

585
00:25:11,780 --> 00:25:14,270
They behave differently based on input.

586
00:25:14,270 --> 00:25:15,910
Their internal cultures are different.

587
00:25:15,910 --> 00:25:17,530
Their compensation schemes are different.

588
00:25:17,530 --> 00:25:19,410
Their values are different.

589
00:25:19,410 --> 00:25:22,003
So there's proof that diversity works.

590
00:25:24,780 --> 00:25:28,590
- [Lex] So when faced
with a tough decision

591
00:25:29,850 --> 00:25:31,930
in need of advice,

592
00:25:31,930 --> 00:25:34,230
it's been said that the
best thing one can do

593
00:25:34,230 --> 00:25:36,830
is to find the best person in the world

594
00:25:36,830 --> 00:25:39,080
who can give that advice

595
00:25:39,080 --> 00:25:43,673
and find a way to be in a room
with them one-on-one and ask.

596
00:25:44,780 --> 00:25:46,040
So here we are.

597
00:25:46,040 --> 00:25:48,070
And let me ask in a long-winded way.

598
00:25:48,070 --> 00:25:49,183
I wrote this down.

599
00:25:50,800 --> 00:25:53,430
In 1998, there were many
good search engines:

600
00:25:53,430 --> 00:25:56,790
Lycos, Excite, AltaVista, InfoSeek,

601
00:25:56,790 --> 00:26:00,363
Ask Jeeves maybe, Yahoo even.

602
00:26:01,900 --> 00:26:04,700
So Google stepped in and
disrupted everything.

603
00:26:04,700 --> 00:26:06,600
They disrupted the nature of search,

604
00:26:06,600 --> 00:26:08,890
the nature of our access to information,

605
00:26:08,890 --> 00:26:10,693
the way we discover new knowledge.

606
00:26:11,900 --> 00:26:16,030
So now it's 2018, actually 20 years later.

607
00:26:16,030 --> 00:26:18,760
There are many good
personal AI assistants,

608
00:26:18,760 --> 00:26:21,033
including, of course,
the best from Google.

609
00:26:22,290 --> 00:26:25,540
So you've spoken in medical and education

610
00:26:25,540 --> 00:26:28,650
the impact of such an AI
assistant could bring.

611
00:26:28,650 --> 00:26:30,370
So we arrive at this question.

612
00:26:30,370 --> 00:26:32,200
So it's a personal one for me,

613
00:26:32,200 --> 00:26:36,210
but I hope my situation
represents that of many other

614
00:26:37,170 --> 00:26:41,110
as we said dreamers and
the crazy engineers.

615
00:26:41,110 --> 00:26:44,010
So my whole live I've dreamed

616
00:26:44,010 --> 00:26:46,380
of creating such an AI assistant.

617
00:26:46,380 --> 00:26:48,970
Every step I've taken has
been towards that goal.

618
00:26:48,970 --> 00:26:50,440
Now I'm a research scientist

619
00:26:50,440 --> 00:26:52,850
in human-centered AI here at MIT.

620
00:26:52,850 --> 00:26:56,890
So the next step for me as
I sit here facing my passion

621
00:26:58,180 --> 00:27:01,053
is to do what Larry and Sergey did in '98,

622
00:27:02,511 --> 00:27:04,740
the simple start-up.

623
00:27:04,740 --> 00:27:06,860
And so here's my simple question.

624
00:27:06,860 --> 00:27:10,660
Given the low odds of success,
the timing and luck required,

625
00:27:10,660 --> 00:27:11,850
the countless other factors

626
00:27:11,850 --> 00:27:13,720
that can't be controlled or predicted,

627
00:27:13,720 --> 00:27:16,480
which is all the things
that Larry and Sergey faced,

628
00:27:16,480 --> 00:27:17,980
is there some calculation,

629
00:27:17,980 --> 00:27:21,620
some strategy to follow in the step?

630
00:27:21,620 --> 00:27:23,720
Or do you simply follow the passion

631
00:27:23,720 --> 00:27:25,570
just because there's no other choice?

632
00:27:26,600 --> 00:27:29,700
- I think the people
who are in universities

633
00:27:29,700 --> 00:27:31,900
are always trying to study

634
00:27:31,900 --> 00:27:34,130
the extraordinarily chaotic nature

635
00:27:34,130 --> 00:27:37,280
of innovation and entrepreneurship.

636
00:27:37,280 --> 00:27:41,220
My answer is that they didn't
have that conversation.

637
00:27:41,220 --> 00:27:42,860
They just did it.

638
00:27:42,860 --> 00:27:47,260
They sensed a moment when
in the case of Google,

639
00:27:47,260 --> 00:27:49,750
there was all of this data
that needed to be organized,

640
00:27:49,750 --> 00:27:51,340
and they had a better algorithm.

641
00:27:51,340 --> 00:27:53,830
They had invented a better way.

642
00:27:53,830 --> 00:27:56,350
So today, with human-centered AI,

643
00:27:56,350 --> 00:27:58,100
which is your area of research,

644
00:27:58,100 --> 00:28:00,900
there must be new approaches.

645
00:28:00,900 --> 00:28:02,490
It's such a big field.

646
00:28:02,490 --> 00:28:05,410
There must be new approaches different

647
00:28:05,410 --> 00:28:07,270
from what we and others are doing.

648
00:28:07,270 --> 00:28:09,600
There must be start-ups to fund.

649
00:28:09,600 --> 00:28:11,950
There must be research projects to try.

650
00:28:11,950 --> 00:28:15,060
There must be graduate students
to work on new approaches.

651
00:28:15,060 --> 00:28:18,210
Here at MIT, there are people
who are looking at learning

652
00:28:18,210 --> 00:28:20,620
from the standpoint of
looking at child learning.

653
00:28:20,620 --> 00:28:22,110
How do children learn starting

654
00:28:22,110 --> 00:28:23,540
at age one and two--
- Josh Tenenbaum and others.

655
00:28:23,540 --> 00:28:25,380
- And the work is fantastic.

656
00:28:25,380 --> 00:28:27,240
Those approached are different

657
00:28:27,240 --> 00:28:29,820
from the approach that
most people are taking.

658
00:28:29,820 --> 00:28:31,990
Perhaps that's a bet that you should make,

659
00:28:31,990 --> 00:28:33,850
or perhaps there's another one.

660
00:28:33,850 --> 00:28:35,890
But at the end of the day,

661
00:28:35,890 --> 00:28:40,160
the successful entrepreneurs
are not as crazy as they sound.

662
00:28:40,160 --> 00:28:43,110
They see an opportunity
based on what's happened.

663
00:28:43,110 --> 00:28:45,360
Let's use Uber as an example.

664
00:28:45,360 --> 00:28:46,850
As Travis tells the story,

665
00:28:46,850 --> 00:28:49,000
he and his co-founder
were sitting in Paris,

666
00:28:49,000 --> 00:28:52,060
and they had this idea 'cause
they couldn't get a cab.

667
00:28:52,060 --> 00:28:56,710
And they said we have smartphones,
and the rest is history.

668
00:28:56,710 --> 00:28:58,780
So what's the equivalent

669
00:28:58,780 --> 00:29:03,150
of that Travis Eiffel
Tower where is a cab moment

670
00:29:03,150 --> 00:29:05,980
that you could as an
entrepreneur take advantage of,

671
00:29:05,980 --> 00:29:08,570
whether it's in human-centered
AI or something else?

672
00:29:08,570 --> 00:29:10,153
That's the next great start-up.

673
00:29:11,290 --> 00:29:13,670
- [Lex] And the psychology of that moment.

674
00:29:13,670 --> 00:29:16,423
So when Sergey and Larry talk about,

675
00:29:16,423 --> 00:29:18,930
in listening to a few interviews,

676
00:29:18,930 --> 00:29:20,230
it's very nonchalant.

677
00:29:20,230 --> 00:29:23,883
Well here's a very fascinating web data,

678
00:29:24,770 --> 00:29:27,800
and here's an algorithm we have.

679
00:29:27,800 --> 00:29:29,500
We just kind of want to
play around with that data,

680
00:29:29,500 --> 00:29:31,090
and it seems like that's a really nice way

681
00:29:31,090 --> 00:29:32,363
to organize this data.

682
00:29:33,605 --> 00:29:35,640
- Well I should say
what happened, remember,

683
00:29:35,640 --> 00:29:38,050
is that they were graduate
students at Stanford,

684
00:29:38,050 --> 00:29:39,360
and they thought this was interesting.

685
00:29:39,360 --> 00:29:40,600
So they build a search engine

686
00:29:40,600 --> 00:29:42,150
and they kept it in their room.

687
00:29:43,080 --> 00:29:46,370
And they had to get power
from the room next door

688
00:29:46,370 --> 00:29:48,070
'cause they were using too
much power in their room,

689
00:29:48,070 --> 00:29:49,980
so they ran an extension cord over

690
00:29:51,520 --> 00:29:53,550
and then they went and they found a house

691
00:29:53,550 --> 00:29:55,390
and they had Google world headquarters

692
00:29:55,390 --> 00:29:57,640
of five people to start the company.

693
00:29:57,640 --> 00:30:00,280
And they raised $100,000
from Andy Bechtolsheim,

694
00:30:00,280 --> 00:30:02,270
who is the Sun founder to do this

695
00:30:02,270 --> 00:30:04,520
and Dave Cheriton and a few others.

696
00:30:04,520 --> 00:30:08,260
The point is their
beginnings were very simple,

697
00:30:08,260 --> 00:30:10,513
but they were based on a powerful insight.

698
00:30:11,750 --> 00:30:14,920
That is a replicable
model for any start-up.

699
00:30:14,920 --> 00:30:16,530
It has to be a powerful insight,

700
00:30:16,530 --> 00:30:17,660
the beginnings are simple,

701
00:30:17,660 --> 00:30:19,860
and there has to be an innovation.

702
00:30:19,860 --> 00:30:22,840
In Larry and Sergey's
case, it was PageRank,

703
00:30:22,840 --> 00:30:24,010
which was a brilliant idea,

704
00:30:24,010 --> 00:30:26,730
one of the most sited
papers in the world today.

705
00:30:26,730 --> 00:30:27,863
What's the next one?

706
00:30:29,790 --> 00:30:33,520
- [Lex] So you're one of, if I may say,

707
00:30:33,520 --> 00:30:35,073
richest people in the world,

708
00:30:36,220 --> 00:30:38,720
and yet it seems that money
is simply a side effect

709
00:30:38,720 --> 00:30:41,993
of your passions and not an inherent goal.

710
00:30:43,010 --> 00:30:48,010
But you're a fascinating person to ask.

711
00:30:48,250 --> 00:30:51,590
So much of our society
at the individual level

712
00:30:51,590 --> 00:30:55,070
and at the company level and as nations

713
00:30:55,070 --> 00:30:57,433
is driven by the desire for wealth.

714
00:30:58,530 --> 00:31:01,130
What do you think about this drive,

715
00:31:01,130 --> 00:31:03,180
and what have you learned about,

716
00:31:03,180 --> 00:31:05,060
if I may romanticize the notion,

717
00:31:05,060 --> 00:31:08,160
the meaning of life
having achieved success

718
00:31:08,160 --> 00:31:10,280
on so many dimensions?

719
00:31:10,280 --> 00:31:13,610
- There have been many
studies of human happiness,

720
00:31:13,610 --> 00:31:16,360
and above some threshold,

721
00:31:16,360 --> 00:31:19,540
which is typically relatively
low for this conversation,

722
00:31:19,540 --> 00:31:23,590
there's no difference in
happiness about money.

723
00:31:23,590 --> 00:31:27,090
The happiness is correlated
with meaning and purpose,

724
00:31:27,090 --> 00:31:30,060
a sense of family, a sense of impact.

725
00:31:30,060 --> 00:31:31,980
So if you organize your life,

726
00:31:31,980 --> 00:31:33,680
assuming you have enough to get around

727
00:31:33,680 --> 00:31:35,910
and have a nice home and so forth,

728
00:31:35,910 --> 00:31:38,350
you'll be far happier if you figure out

729
00:31:38,350 --> 00:31:41,690
what you care about and work on that.

730
00:31:41,690 --> 00:31:43,927
It's often being in service to others.

731
00:31:43,927 --> 00:31:46,900
There's a great deal of evidence
that people are happiest

732
00:31:46,900 --> 00:31:49,590
when they're serving
others and not themselves.

733
00:31:49,590 --> 00:31:51,500
This goes directly against

734
00:31:51,500 --> 00:31:55,360
the sort of press-induced excitement

735
00:31:55,360 --> 00:31:59,280
about powerful and wealthy
leaders of the world,

736
00:31:59,280 --> 00:32:01,760
and indeed these are consequential people.

737
00:32:01,760 --> 00:32:03,900
But if you are in a situation

738
00:32:03,900 --> 00:32:06,150
where you've been very
fortunate as I have,

739
00:32:06,150 --> 00:32:09,080
you also have to take
that as a responsibility

740
00:32:09,080 --> 00:32:12,220
and you have to basically
work both to educate others

741
00:32:12,220 --> 00:32:13,630
and give them that opportunity

742
00:32:13,630 --> 00:32:16,760
but also use that wealth
to advance human society.

743
00:32:16,760 --> 00:32:18,370
In my case, I'm particularly interested

744
00:32:18,370 --> 00:32:20,620
in using the tools of
artificial intelligence

745
00:32:20,620 --> 00:32:22,910
and machine learning
to make society better.

746
00:32:22,910 --> 00:32:24,090
I've mentioned education.

747
00:32:24,090 --> 00:32:26,930
I've mentioned inequality in middle class

748
00:32:26,930 --> 00:32:30,140
and things like this, all of
which are a passion of mine.

749
00:32:30,140 --> 00:32:31,880
It doesn't matter what you do.

750
00:32:31,880 --> 00:32:33,750
It matters that you believe in it,

751
00:32:33,750 --> 00:32:35,430
that it's important to you,

752
00:32:35,430 --> 00:32:38,140
and your life can be far more satisfying

753
00:32:38,140 --> 00:32:40,570
if you spend your life doing that.

754
00:32:40,570 --> 00:32:43,520
- [Lex] I think there's
no better place to end

755
00:32:43,520 --> 00:32:45,260
than a discussion of the meaning of life.

756
00:32:45,260 --> 00:32:47,493
- Eric, thank you so much.
- Thank you very much, Lex.


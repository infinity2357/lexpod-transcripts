1
00:00:00,060 --> 00:00:02,600

you've studied the human mind cognition

2
00:00:02,600 --> 00:00:02,610
you've studied the human mind cognition
 

3
00:00:02,610 --> 00:00:05,210
you've studied the human mind cognition
language vision evolution psychology

4
00:00:05,210 --> 00:00:05,220
language vision evolution psychology
 

5
00:00:05,220 --> 00:00:08,540
language vision evolution psychology
from child to adult from the level of

6
00:00:08,540 --> 00:00:08,550
from child to adult from the level of
 

7
00:00:08,550 --> 00:00:10,310
from child to adult from the level of
individual to the level of our entire

8
00:00:10,310 --> 00:00:10,320
individual to the level of our entire
 

9
00:00:10,320 --> 00:00:13,039
individual to the level of our entire
civilization so I feel like I can start

10
00:00:13,039 --> 00:00:13,049
civilization so I feel like I can start
 

11
00:00:13,049 --> 00:00:15,580
civilization so I feel like I can start
with a simple multiple-choice question

12
00:00:15,580 --> 00:00:15,590
with a simple multiple-choice question
 

13
00:00:15,590 --> 00:00:20,390
with a simple multiple-choice question
what is the meaning of life is it a to

14
00:00:20,390 --> 00:00:20,400
what is the meaning of life is it a to
 

15
00:00:20,400 --> 00:00:23,060
what is the meaning of life is it a to
attain knowledge as Plato said B to

16
00:00:23,060 --> 00:00:23,070
attain knowledge as Plato said B to
 

17
00:00:23,070 --> 00:00:26,060
attain knowledge as Plato said B to
attain power as Nietzsche said C to

18
00:00:26,060 --> 00:00:26,070
attain power as Nietzsche said C to
 

19
00:00:26,070 --> 00:00:29,150
attain power as Nietzsche said C to
escape death as Ernest Becker said d to

20
00:00:29,150 --> 00:00:29,160
escape death as Ernest Becker said d to
 

21
00:00:29,160 --> 00:00:32,060
escape death as Ernest Becker said d to
propagate our genes as Darwin and others

22
00:00:32,060 --> 00:00:32,070
propagate our genes as Darwin and others
 

23
00:00:32,070 --> 00:00:35,360
propagate our genes as Darwin and others
have said e there is no meaning as the

24
00:00:35,360 --> 00:00:35,370
have said e there is no meaning as the
 

25
00:00:35,370 --> 00:00:38,840
have said e there is no meaning as the
nihilists have said F knowing the

26
00:00:38,840 --> 00:00:38,850
nihilists have said F knowing the
 

27
00:00:38,850 --> 00:00:40,639
nihilists have said F knowing the
meaning of life is beyond our cognitive

28
00:00:40,639 --> 00:00:40,649
meaning of life is beyond our cognitive
 

29
00:00:40,649 --> 00:00:43,310
meaning of life is beyond our cognitive
capabilities as Steven Pinker said based

30
00:00:43,310 --> 00:00:43,320
capabilities as Steven Pinker said based
 

31
00:00:43,320 --> 00:00:44,810
capabilities as Steven Pinker said based
on my interpretation twenty years ago

32
00:00:44,810 --> 00:00:44,820
on my interpretation twenty years ago
 

33
00:00:44,820 --> 00:00:47,450
on my interpretation twenty years ago
and G none of the above

34
00:00:47,450 --> 00:00:47,460
and G none of the above
 

35
00:00:47,460 --> 00:00:50,420
and G none of the above
I'd say aid comes closest but I would

36
00:00:50,420 --> 00:00:50,430
I'd say aid comes closest but I would
 

37
00:00:50,430 --> 00:00:52,970
I'd say aid comes closest but I would
amend that to attaining not only

38
00:00:52,970 --> 00:00:52,980
amend that to attaining not only
 

39
00:00:52,980 --> 00:00:55,459
amend that to attaining not only
knowledge but fulfillment more generally

40
00:00:55,459 --> 00:00:55,469
knowledge but fulfillment more generally
 

41
00:00:55,469 --> 00:01:01,130
knowledge but fulfillment more generally
that is life health stimulation access

42
00:01:01,130 --> 00:01:01,140
that is life health stimulation access
 

43
00:01:01,140 --> 00:01:05,570
that is life health stimulation access
to the living cultural and social world

44
00:01:05,570 --> 00:01:05,580
to the living cultural and social world
 

45
00:01:05,580 --> 00:01:08,090
to the living cultural and social world
now this is our meaning of life it's not

46
00:01:08,090 --> 00:01:08,100
now this is our meaning of life it's not
 

47
00:01:08,100 --> 00:01:10,219
now this is our meaning of life it's not
the meaning of life if you were to ask

48
00:01:10,219 --> 00:01:10,229
the meaning of life if you were to ask
 

49
00:01:10,229 --> 00:01:14,600
the meaning of life if you were to ask
our genes their meaning is to propagate

50
00:01:14,600 --> 00:01:14,610
our genes their meaning is to propagate
 

51
00:01:14,610 --> 00:01:16,850
our genes their meaning is to propagate
copies of themselves but that is

52
00:01:16,850 --> 00:01:16,860
copies of themselves but that is
 

53
00:01:16,860 --> 00:01:18,649
copies of themselves but that is
distinct from the meaning that the brain

54
00:01:18,649 --> 00:01:18,659
distinct from the meaning that the brain
 

55
00:01:18,659 --> 00:01:23,210
distinct from the meaning that the brain
that they lead to sets for itself so to

56
00:01:23,210 --> 00:01:23,220
that they lead to sets for itself so to
 

57
00:01:23,220 --> 00:01:26,929
that they lead to sets for itself so to
you knowledge is a small subset or a

58
00:01:26,929 --> 00:01:26,939
you knowledge is a small subset or a
 

59
00:01:26,939 --> 00:01:28,999
you knowledge is a small subset or a
large subset it's a large subset but

60
00:01:28,999 --> 00:01:29,009
large subset it's a large subset but
 

61
00:01:29,009 --> 00:01:31,370
large subset it's a large subset but
it's not the entirety of human striding

62
00:01:31,370 --> 00:01:31,380
it's not the entirety of human striding
 

63
00:01:31,380 --> 00:01:35,780
it's not the entirety of human striding
because we also want to interact with

64
00:01:35,780 --> 00:01:35,790
because we also want to interact with
 

65
00:01:35,790 --> 00:01:37,910
because we also want to interact with
people we want to experience beauty we

66
00:01:37,910 --> 00:01:37,920
people we want to experience beauty we
 

67
00:01:37,920 --> 00:01:39,770
people we want to experience beauty we
want to experience the the richness of

68
00:01:39,770 --> 00:01:39,780
want to experience the the richness of
 

69
00:01:39,780 --> 00:01:43,160
want to experience the the richness of
the natural world but understanding the

70
00:01:43,160 --> 00:01:43,170
the natural world but understanding the
 

71
00:01:43,170 --> 00:01:46,999
the natural world but understanding the
what makes the universe tick is his way

72
00:01:46,999 --> 00:01:47,009
what makes the universe tick is his way
 

73
00:01:47,009 --> 00:01:49,330
what makes the universe tick is his way
up there for some of us more than others

74
00:01:49,330 --> 00:01:49,340
up there for some of us more than others
 

75
00:01:49,340 --> 00:01:53,240
up there for some of us more than others
certainly for me that's that's one of

76
00:01:53,240 --> 00:01:53,250
certainly for me that's that's one of
 

77
00:01:53,250 --> 00:01:56,600
certainly for me that's that's one of
the top five so is that a fundamental

78
00:01:56,600 --> 00:01:56,610
the top five so is that a fundamental
 

79
00:01:56,610 --> 00:01:58,819
the top five so is that a fundamental
aspect are you just describing your own

80
00:01:58,819 --> 00:01:58,829
aspect are you just describing your own
 

81
00:01:58,829 --> 00:02:00,620
aspect are you just describing your own
preference or is this a fundamental

82
00:02:00,620 --> 00:02:00,630
preference or is this a fundamental
 

83
00:02:00,630 --> 00:02:02,209
preference or is this a fundamental
aspect of human nature is to seek

84
00:02:02,209 --> 00:02:02,219
aspect of human nature is to seek
 

85
00:02:02,219 --> 00:02:05,510
aspect of human nature is to seek
knowledge just in your latest book you

86
00:02:05,510 --> 00:02:05,520
knowledge just in your latest book you
 

87
00:02:05,520 --> 00:02:07,730
knowledge just in your latest book you
talk about the the the power the

88
00:02:07,730 --> 00:02:07,740
talk about the the the power the
 

89
00:02:07,740 --> 00:02:10,339
talk about the the the power the
usefulness of rationality and reason so

90
00:02:10,339 --> 00:02:10,349
usefulness of rationality and reason so
 

91
00:02:10,349 --> 00:02:12,480
usefulness of rationality and reason so
on is that a fundamental nature

92
00:02:12,480 --> 00:02:12,490
on is that a fundamental nature
 

93
00:02:12,490 --> 00:02:15,360
on is that a fundamental nature
human beings or is it something we

94
00:02:15,360 --> 00:02:15,370
human beings or is it something we
 

95
00:02:15,370 --> 00:02:17,880
human beings or is it something we
should just strive for it's both it is

96
00:02:17,880 --> 00:02:17,890
should just strive for it's both it is
 

97
00:02:17,890 --> 00:02:20,430
should just strive for it's both it is
we're capable of striving for it because

98
00:02:20,430 --> 00:02:20,440
we're capable of striving for it because
 

99
00:02:20,440 --> 00:02:23,100
we're capable of striving for it because
it is one of the things that make us

100
00:02:23,100 --> 00:02:23,110
it is one of the things that make us
 

101
00:02:23,110 --> 00:02:27,930
it is one of the things that make us
what we are Homo sapiens wise men we are

102
00:02:27,930 --> 00:02:27,940
what we are Homo sapiens wise men we are
 

103
00:02:27,940 --> 00:02:31,290
what we are Homo sapiens wise men we are
unusual among animals in the degree to

104
00:02:31,290 --> 00:02:31,300
unusual among animals in the degree to
 

105
00:02:31,300 --> 00:02:33,870
unusual among animals in the degree to
which we acquire knowledge and use it to

106
00:02:33,870 --> 00:02:33,880
which we acquire knowledge and use it to
 

107
00:02:33,880 --> 00:02:37,110
which we acquire knowledge and use it to
survive we we make tools we strike

108
00:02:37,110 --> 00:02:37,120
survive we we make tools we strike
 

109
00:02:37,120 --> 00:02:41,550
survive we we make tools we strike
agreements via language we extract

110
00:02:41,550 --> 00:02:41,560
agreements via language we extract
 

111
00:02:41,560 --> 00:02:44,310
agreements via language we extract
poisons we predict the behavior of

112
00:02:44,310 --> 00:02:44,320
poisons we predict the behavior of
 

113
00:02:44,320 --> 00:02:47,430
poisons we predict the behavior of
animals we try to get at the workings of

114
00:02:47,430 --> 00:02:47,440
animals we try to get at the workings of
 

115
00:02:47,440 --> 00:02:49,110
animals we try to get at the workings of
plants and when I say we I don't just

116
00:02:49,110 --> 00:02:49,120
plants and when I say we I don't just
 

117
00:02:49,120 --> 00:02:51,660
plants and when I say we I don't just
mean we in the modern West but we as a

118
00:02:51,660 --> 00:02:51,670
mean we in the modern West but we as a
 

119
00:02:51,670 --> 00:02:53,460
mean we in the modern West but we as a
species everywhere which is how we've

120
00:02:53,460 --> 00:02:53,470
species everywhere which is how we've
 

121
00:02:53,470 --> 00:02:56,340
species everywhere which is how we've
managed to occupy every niche on the

122
00:02:56,340 --> 00:02:56,350
managed to occupy every niche on the
 

123
00:02:56,350 --> 00:02:57,930
managed to occupy every niche on the
planet and how we've managed to drive

124
00:02:57,930 --> 00:02:57,940
planet and how we've managed to drive
 

125
00:02:57,940 --> 00:02:59,790
planet and how we've managed to drive
other animals to extinction and the

126
00:02:59,790 --> 00:02:59,800
other animals to extinction and the
 

127
00:02:59,800 --> 00:03:03,390
other animals to extinction and the
refinement of Reason in pursuit of human

128
00:03:03,390 --> 00:03:03,400
refinement of Reason in pursuit of human
 

129
00:03:03,400 --> 00:03:07,830
refinement of Reason in pursuit of human
well-being of health happiness social

130
00:03:07,830 --> 00:03:07,840
well-being of health happiness social
 

131
00:03:07,840 --> 00:03:11,640
well-being of health happiness social
richness cultural richness is our our

132
00:03:11,640 --> 00:03:11,650
richness cultural richness is our our
 

133
00:03:11,650 --> 00:03:13,910
richness cultural richness is our our
main challenge in the present that is

134
00:03:13,910 --> 00:03:13,920
main challenge in the present that is
 

135
00:03:13,920 --> 00:03:16,170
main challenge in the present that is
using our intellect using our knowledge

136
00:03:16,170 --> 00:03:16,180
using our intellect using our knowledge
 

137
00:03:16,180 --> 00:03:18,570
using our intellect using our knowledge
to figure out how the world works how we

138
00:03:18,570 --> 00:03:18,580
to figure out how the world works how we
 

139
00:03:18,580 --> 00:03:21,390
to figure out how the world works how we
work in order to make discoveries and

140
00:03:21,390 --> 00:03:21,400
work in order to make discoveries and
 

141
00:03:21,400 --> 00:03:23,670
work in order to make discoveries and
strike agreements that make us all

142
00:03:23,670 --> 00:03:23,680
strike agreements that make us all
 

143
00:03:23,680 --> 00:03:26,340
strike agreements that make us all
better off in the long run right and you

144
00:03:26,340 --> 00:03:26,350
better off in the long run right and you
 

145
00:03:26,350 --> 00:03:30,120
better off in the long run right and you
do that almost undeniably and in a

146
00:03:30,120 --> 00:03:30,130
do that almost undeniably and in a
 

147
00:03:30,130 --> 00:03:32,250
do that almost undeniably and in a
data-driven way in a recent book but I'd

148
00:03:32,250 --> 00:03:32,260
data-driven way in a recent book but I'd
 

149
00:03:32,260 --> 00:03:33,600
data-driven way in a recent book but I'd
like to focus on the artificial

150
00:03:33,600 --> 00:03:33,610
like to focus on the artificial
 

151
00:03:33,610 --> 00:03:36,390
like to focus on the artificial
intelligence aspect of things and not

152
00:03:36,390 --> 00:03:36,400
intelligence aspect of things and not
 

153
00:03:36,400 --> 00:03:37,980
intelligence aspect of things and not
just artificial intelligence but natural

154
00:03:37,980 --> 00:03:37,990
just artificial intelligence but natural
 

155
00:03:37,990 --> 00:03:40,920
just artificial intelligence but natural
intelligence too so twenty years ago in

156
00:03:40,920 --> 00:03:40,930
intelligence too so twenty years ago in
 

157
00:03:40,930 --> 00:03:42,570
intelligence too so twenty years ago in
the book you've written on how the mind

158
00:03:42,570 --> 00:03:42,580
the book you've written on how the mind
 

159
00:03:42,580 --> 00:03:43,110
the book you've written on how the mind
works

160
00:03:43,110 --> 00:03:43,120
works
 

161
00:03:43,120 --> 00:03:46,170
works
you conjecture again my right to

162
00:03:46,170 --> 00:03:46,180
you conjecture again my right to
 

163
00:03:46,180 --> 00:03:49,290
you conjecture again my right to
interpret things you could you can

164
00:03:49,290 --> 00:03:49,300
interpret things you could you can
 

165
00:03:49,300 --> 00:03:50,580
interpret things you could you can
correct me if I'm wrong but you

166
00:03:50,580 --> 00:03:50,590
correct me if I'm wrong but you
 

167
00:03:50,590 --> 00:03:52,590
correct me if I'm wrong but you
conjecture that human thought in the

168
00:03:52,590 --> 00:03:52,600
conjecture that human thought in the
 

169
00:03:52,600 --> 00:03:54,900
conjecture that human thought in the
brain may be a result of and now we're a

170
00:03:54,900 --> 00:03:54,910
brain may be a result of and now we're a
 

171
00:03:54,910 --> 00:03:56,280
brain may be a result of and now we're a
massive network of highly interconnected

172
00:03:56,280 --> 00:03:56,290
massive network of highly interconnected
 

173
00:03:56,290 --> 00:03:59,610
massive network of highly interconnected
neurons so from this interconnectivity

174
00:03:59,610 --> 00:03:59,620
neurons so from this interconnectivity
 

175
00:03:59,620 --> 00:04:02,580
neurons so from this interconnectivity
emerges thought compared to artificial

176
00:04:02,580 --> 00:04:02,590
emerges thought compared to artificial
 

177
00:04:02,590 --> 00:04:05,010
emerges thought compared to artificial
neural networks we use for machine

178
00:04:05,010 --> 00:04:05,020
neural networks we use for machine
 

179
00:04:05,020 --> 00:04:07,170
neural networks we use for machine
learning today is there something

180
00:04:07,170 --> 00:04:07,180
learning today is there something
 

181
00:04:07,180 --> 00:04:10,050
learning today is there something
fundamentally more complex mysterious

182
00:04:10,050 --> 00:04:10,060
fundamentally more complex mysterious
 

183
00:04:10,060 --> 00:04:13,050
fundamentally more complex mysterious
even magical about the biological neural

184
00:04:13,050 --> 00:04:13,060
even magical about the biological neural
 

185
00:04:13,060 --> 00:04:15,150
even magical about the biological neural
networks versus the ones we've been

186
00:04:15,150 --> 00:04:15,160
networks versus the ones we've been
 

187
00:04:15,160 --> 00:04:19,260
networks versus the ones we've been
starting to use over the past 60 years

188
00:04:19,260 --> 00:04:19,270
starting to use over the past 60 years
 

189
00:04:19,270 --> 00:04:21,469
starting to use over the past 60 years
and it becomes a success in the past 10

190
00:04:21,469 --> 00:04:21,479
and it becomes a success in the past 10
 

191
00:04:21,479 --> 00:04:23,720
and it becomes a success in the past 10
there is something

192
00:04:23,720 --> 00:04:23,730
there is something
 

193
00:04:23,730 --> 00:04:26,480
there is something
a little bit mysterious about the human

194
00:04:26,480 --> 00:04:26,490
a little bit mysterious about the human
 

195
00:04:26,490 --> 00:04:29,150
a little bit mysterious about the human
neural networks which is that each one

196
00:04:29,150 --> 00:04:29,160
neural networks which is that each one
 

197
00:04:29,160 --> 00:04:31,700
neural networks which is that each one
of us who is a neural network knows that

198
00:04:31,700 --> 00:04:31,710
of us who is a neural network knows that
 

199
00:04:31,710 --> 00:04:34,580
of us who is a neural network knows that
we ourselves are conscious conscious not

200
00:04:34,580 --> 00:04:34,590
we ourselves are conscious conscious not
 

201
00:04:34,590 --> 00:04:36,080
we ourselves are conscious conscious not
of a sense of registering our

202
00:04:36,080 --> 00:04:36,090
of a sense of registering our
 

203
00:04:36,090 --> 00:04:37,700
of a sense of registering our
surroundings or even registering our

204
00:04:37,700 --> 00:04:37,710
surroundings or even registering our
 

205
00:04:37,710 --> 00:04:40,640
surroundings or even registering our
internal state but in having subjective

206
00:04:40,640 --> 00:04:40,650
internal state but in having subjective
 

207
00:04:40,650 --> 00:04:42,650
internal state but in having subjective
first-person present-tense experience

208
00:04:42,650 --> 00:04:42,660
first-person present-tense experience
 

209
00:04:42,660 --> 00:04:45,380
first-person present-tense experience
that is when I see red it's not just

210
00:04:45,380 --> 00:04:45,390
that is when I see red it's not just
 

211
00:04:45,390 --> 00:04:48,200
that is when I see red it's not just
different from green but it just there's

212
00:04:48,200 --> 00:04:48,210
different from green but it just there's
 

213
00:04:48,210 --> 00:04:50,690
different from green but it just there's
there's a redness to it I feel whether

214
00:04:50,690 --> 00:04:50,700
there's a redness to it I feel whether
 

215
00:04:50,700 --> 00:04:52,580
there's a redness to it I feel whether
an artificial system would experience

216
00:04:52,580 --> 00:04:52,590
an artificial system would experience
 

217
00:04:52,590 --> 00:04:54,350
an artificial system would experience
that or not I don't know and I don't

218
00:04:54,350 --> 00:04:54,360
that or not I don't know and I don't
 

219
00:04:54,360 --> 00:04:56,210
that or not I don't know and I don't
think I can know that's why it's

220
00:04:56,210 --> 00:04:56,220
think I can know that's why it's
 

221
00:04:56,220 --> 00:04:57,950
think I can know that's why it's
mysterious if we had a perfectly

222
00:04:57,950 --> 00:04:57,960
mysterious if we had a perfectly
 

223
00:04:57,960 --> 00:05:00,560
mysterious if we had a perfectly
lifelike robot that was behaviorally

224
00:05:00,560 --> 00:05:00,570
lifelike robot that was behaviorally
 

225
00:05:00,570 --> 00:05:03,410
lifelike robot that was behaviorally
indistinguishable from a human would we

226
00:05:03,410 --> 00:05:03,420
indistinguishable from a human would we
 

227
00:05:03,420 --> 00:05:05,930
indistinguishable from a human would we
attribute consciousness to it or ought

228
00:05:05,930 --> 00:05:05,940
attribute consciousness to it or ought
 

229
00:05:05,940 --> 00:05:07,940
attribute consciousness to it or ought
we to attribute consciousness to it and

230
00:05:07,940 --> 00:05:07,950
we to attribute consciousness to it and
 

231
00:05:07,950 --> 00:05:10,790
we to attribute consciousness to it and
that's something that it's very hard to

232
00:05:10,790 --> 00:05:10,800
that's something that it's very hard to
 

233
00:05:10,800 --> 00:05:12,680
that's something that it's very hard to
know but putting that aside put inside

234
00:05:12,680 --> 00:05:12,690
know but putting that aside put inside
 

235
00:05:12,690 --> 00:05:15,070
know but putting that aside put inside
that that largely philosophical question

236
00:05:15,070 --> 00:05:15,080
that that largely philosophical question
 

237
00:05:15,080 --> 00:05:18,680
that that largely philosophical question
the question is is there some difference

238
00:05:18,680 --> 00:05:18,690
the question is is there some difference
 

239
00:05:18,690 --> 00:05:20,810
the question is is there some difference
between the human neural network and the

240
00:05:20,810 --> 00:05:20,820
between the human neural network and the
 

241
00:05:20,820 --> 00:05:22,670
between the human neural network and the
ones that we were building in artificial

242
00:05:22,670 --> 00:05:22,680
ones that we were building in artificial
 

243
00:05:22,680 --> 00:05:24,680
ones that we were building in artificial
intelligence will mean that we're on the

244
00:05:24,680 --> 00:05:24,690
intelligence will mean that we're on the
 

245
00:05:24,690 --> 00:05:26,810
intelligence will mean that we're on the
current trajectory not going to reach

246
00:05:26,810 --> 00:05:26,820
current trajectory not going to reach
 

247
00:05:26,820 --> 00:05:29,960
current trajectory not going to reach
the point where we've got a lifelike

248
00:05:29,960 --> 00:05:29,970
the point where we've got a lifelike
 

249
00:05:29,970 --> 00:05:31,760
the point where we've got a lifelike
robot indistinguishable from a human

250
00:05:31,760 --> 00:05:31,770
robot indistinguishable from a human
 

251
00:05:31,770 --> 00:05:33,920
robot indistinguishable from a human
because the way their neural so-called

252
00:05:33,920 --> 00:05:33,930
because the way their neural so-called
 

253
00:05:33,930 --> 00:05:35,330
because the way their neural so-called
neural networks were organized are

254
00:05:35,330 --> 00:05:35,340
neural networks were organized are
 

255
00:05:35,340 --> 00:05:36,620
neural networks were organized are
different from the way ours are

256
00:05:36,620 --> 00:05:36,630
different from the way ours are
 

257
00:05:36,630 --> 00:05:39,410
different from the way ours are
organized having there's overlap but I

258
00:05:39,410 --> 00:05:39,420
organized having there's overlap but I
 

259
00:05:39,420 --> 00:05:40,910
organized having there's overlap but I
think there are some some big

260
00:05:40,910 --> 00:05:40,920
think there are some some big
 

261
00:05:40,920 --> 00:05:44,200
think there are some some big
differences that they're the current

262
00:05:44,200 --> 00:05:44,210
differences that they're the current
 

263
00:05:44,210 --> 00:05:46,760
differences that they're the current
neural networks current so called deep

264
00:05:46,760 --> 00:05:46,770
neural networks current so called deep
 

265
00:05:46,770 --> 00:05:49,580
neural networks current so called deep
learning systems are in reality not all

266
00:05:49,580 --> 00:05:49,590
learning systems are in reality not all
 

267
00:05:49,590 --> 00:05:51,800
learning systems are in reality not all
that deep that is they are very good at

268
00:05:51,800 --> 00:05:51,810
that deep that is they are very good at
 

269
00:05:51,810 --> 00:05:53,990
that deep that is they are very good at
extracting high order statistical

270
00:05:53,990 --> 00:05:54,000
extracting high order statistical
 

271
00:05:54,000 --> 00:05:56,060
extracting high order statistical
regularities but most of the systems

272
00:05:56,060 --> 00:05:56,070
regularities but most of the systems
 

273
00:05:56,070 --> 00:05:58,990
regularities but most of the systems
don't have a semantic level a level of

274
00:05:58,990 --> 00:05:59,000
don't have a semantic level a level of
 

275
00:05:59,000 --> 00:06:01,880
don't have a semantic level a level of
actual understanding of who did what to

276
00:06:01,880 --> 00:06:01,890
actual understanding of who did what to
 

277
00:06:01,890 --> 00:06:05,630
actual understanding of who did what to
who why where how things work what

278
00:06:05,630 --> 00:06:05,640
who why where how things work what
 

279
00:06:05,640 --> 00:06:07,790
who why where how things work what
causes what else do you think that kind

280
00:06:07,790 --> 00:06:07,800
causes what else do you think that kind
 

281
00:06:07,800 --> 00:06:09,280
causes what else do you think that kind
of thing can emerge as it does so

282
00:06:09,280 --> 00:06:09,290
of thing can emerge as it does so
 

283
00:06:09,290 --> 00:06:11,660
of thing can emerge as it does so
artificial you know so much smaller the

284
00:06:11,660 --> 00:06:11,670
artificial you know so much smaller the
 

285
00:06:11,670 --> 00:06:13,190
artificial you know so much smaller the
number of connections and so on in the

286
00:06:13,190 --> 00:06:13,200
number of connections and so on in the
 

287
00:06:13,200 --> 00:06:16,460
number of connections and so on in the
current human biological networks but do

288
00:06:16,460 --> 00:06:16,470
current human biological networks but do
 

289
00:06:16,470 --> 00:06:18,740
current human biological networks but do
you think sort of go to go to

290
00:06:18,740 --> 00:06:18,750
you think sort of go to go to
 

291
00:06:18,750 --> 00:06:20,840
you think sort of go to go to
consciousness or to go to this higher

292
00:06:20,840 --> 00:06:20,850
consciousness or to go to this higher
 

293
00:06:20,850 --> 00:06:22,850
consciousness or to go to this higher
level semantic reasoning about things do

294
00:06:22,850 --> 00:06:22,860
level semantic reasoning about things do
 

295
00:06:22,860 --> 00:06:24,680
level semantic reasoning about things do
you think that can emerge with just a

296
00:06:24,680 --> 00:06:24,690
you think that can emerge with just a
 

297
00:06:24,690 --> 00:06:27,310
you think that can emerge with just a
larger network with a more richly

298
00:06:27,310 --> 00:06:27,320
larger network with a more richly
 

299
00:06:27,320 --> 00:06:29,770
larger network with a more richly
weirdly interconnected network

300
00:06:29,770 --> 00:06:29,780
weirdly interconnected network
 

301
00:06:29,780 --> 00:06:31,730
weirdly interconnected network
separating consciousness because

302
00:06:31,730 --> 00:06:31,740
separating consciousness because
 

303
00:06:31,740 --> 00:06:32,810
separating consciousness because
consciousness is even a matter of

304
00:06:32,810 --> 00:06:32,820
consciousness is even a matter of
 

305
00:06:32,820 --> 00:06:34,250
consciousness is even a matter of
complex a really good one

306
00:06:34,250 --> 00:06:34,260
complex a really good one
 

307
00:06:34,260 --> 00:06:36,110
complex a really good one
yeah you could have you could sensibly

308
00:06:36,110 --> 00:06:36,120
yeah you could have you could sensibly
 

309
00:06:36,120 --> 00:06:37,610
yeah you could have you could sensibly
ask the question of whether shrimp are

310
00:06:37,610 --> 00:06:37,620
ask the question of whether shrimp are
 

311
00:06:37,620 --> 00:06:38,780
ask the question of whether shrimp are
conscious for example they're not

312
00:06:38,780 --> 00:06:38,790
conscious for example they're not
 

313
00:06:38,790 --> 00:06:41,420
conscious for example they're not
terribly complex but maybe they feel

314
00:06:41,420 --> 00:06:41,430
terribly complex but maybe they feel
 

315
00:06:41,430 --> 00:06:43,310
terribly complex but maybe they feel
pain so let's just put that one that

316
00:06:43,310 --> 00:06:43,320
pain so let's just put that one that
 

317
00:06:43,320 --> 00:06:47,060
pain so let's just put that one that
part of it aside yet but I think sheer

318
00:06:47,060 --> 00:06:47,070
part of it aside yet but I think sheer
 

319
00:06:47,070 --> 00:06:49,550
part of it aside yet but I think sheer
size of a neural network is not enough

320
00:06:49,550 --> 00:06:49,560
size of a neural network is not enough
 

321
00:06:49,560 --> 00:06:52,730
size of a neural network is not enough
to give it structure and knowledge but

322
00:06:52,730 --> 00:06:52,740
to give it structure and knowledge but
 

323
00:06:52,740 --> 00:06:55,490
to give it structure and knowledge but
if it's suitably engineered then then

324
00:06:55,490 --> 00:06:55,500
if it's suitably engineered then then
 

325
00:06:55,500 --> 00:06:56,240
if it's suitably engineered then then
why not

326
00:06:56,240 --> 00:06:56,250
why not
 

327
00:06:56,250 --> 00:06:58,970
why not
that is where neural networks natural

328
00:06:58,970 --> 00:06:58,980
that is where neural networks natural
 

329
00:06:58,980 --> 00:07:01,820
that is where neural networks natural
selection did a kind of equivalent of

330
00:07:01,820 --> 00:07:01,830
selection did a kind of equivalent of
 

331
00:07:01,830 --> 00:07:03,710
selection did a kind of equivalent of
engineering of our brains so I don't

332
00:07:03,710 --> 00:07:03,720
engineering of our brains so I don't
 

333
00:07:03,720 --> 00:07:05,120
engineering of our brains so I don't
know there's anything mysterious in the

334
00:07:05,120 --> 00:07:05,130
know there's anything mysterious in the
 

335
00:07:05,130 --> 00:07:08,900
know there's anything mysterious in the
sense that no no system made out of

336
00:07:08,900 --> 00:07:08,910
sense that no no system made out of
 

337
00:07:08,910 --> 00:07:11,090
sense that no no system made out of
silicon could ever do what a human brain

338
00:07:11,090 --> 00:07:11,100
silicon could ever do what a human brain
 

339
00:07:11,100 --> 00:07:12,560
silicon could ever do what a human brain
can do I think it's possible in

340
00:07:12,560 --> 00:07:12,570
can do I think it's possible in
 

341
00:07:12,570 --> 00:07:14,200
can do I think it's possible in
principle whether it'll ever happen

342
00:07:14,200 --> 00:07:14,210
principle whether it'll ever happen
 

343
00:07:14,210 --> 00:07:17,390
principle whether it'll ever happen
depends not only on how clever we are in

344
00:07:17,390 --> 00:07:17,400
depends not only on how clever we are in
 

345
00:07:17,400 --> 00:07:19,520
depends not only on how clever we are in
engineering these systems but whether

346
00:07:19,520 --> 00:07:19,530
engineering these systems but whether
 

347
00:07:19,530 --> 00:07:21,260
engineering these systems but whether
even we even want to whether that's even

348
00:07:21,260 --> 00:07:21,270
even we even want to whether that's even
 

349
00:07:21,270 --> 00:07:23,480
even we even want to whether that's even
a sensible goal that is you can ask the

350
00:07:23,480 --> 00:07:23,490
a sensible goal that is you can ask the
 

351
00:07:23,490 --> 00:07:26,660
a sensible goal that is you can ask the
question is there any locomotion system

352
00:07:26,660 --> 00:07:26,670
question is there any locomotion system
 

353
00:07:26,670 --> 00:07:30,350
question is there any locomotion system
that is as as good as a human well we

354
00:07:30,350 --> 00:07:30,360
that is as as good as a human well we
 

355
00:07:30,360 --> 00:07:31,790
that is as as good as a human well we
kind of want to do better than a human

356
00:07:31,790 --> 00:07:31,800
kind of want to do better than a human
 

357
00:07:31,800 --> 00:07:34,900
kind of want to do better than a human
ultimately in terms of legged locomotion

358
00:07:34,900 --> 00:07:34,910
ultimately in terms of legged locomotion
 

359
00:07:34,910 --> 00:07:37,370
ultimately in terms of legged locomotion
there's no reason that humans should be

360
00:07:37,370 --> 00:07:37,380
there's no reason that humans should be
 

361
00:07:37,380 --> 00:07:39,380
there's no reason that humans should be
our benchmark they're their tools that

362
00:07:39,380 --> 00:07:39,390
our benchmark they're their tools that
 

363
00:07:39,390 --> 00:07:40,880
our benchmark they're their tools that
might be better in some ways it may just

364
00:07:40,880 --> 00:07:40,890
might be better in some ways it may just
 

365
00:07:40,890 --> 00:07:45,770
might be better in some ways it may just
be not as maybe that we can't duplicate

366
00:07:45,770 --> 00:07:45,780
be not as maybe that we can't duplicate
 

367
00:07:45,780 --> 00:07:49,220
be not as maybe that we can't duplicate
a natural system because at some point

368
00:07:49,220 --> 00:07:49,230
a natural system because at some point
 

369
00:07:49,230 --> 00:07:50,720
a natural system because at some point
it's so much cheaper to use a natural

370
00:07:50,720 --> 00:07:50,730
it's so much cheaper to use a natural
 

371
00:07:50,730 --> 00:07:52,340
it's so much cheaper to use a natural
system that we're not going to invest

372
00:07:52,340 --> 00:07:52,350
system that we're not going to invest
 

373
00:07:52,350 --> 00:07:55,040
system that we're not going to invest
more brainpower and resources so for

374
00:07:55,040 --> 00:07:55,050
more brainpower and resources so for
 

375
00:07:55,050 --> 00:07:57,980
more brainpower and resources so for
example we don't really have a subsidy

376
00:07:57,980 --> 00:07:57,990
example we don't really have a subsidy
 

377
00:07:57,990 --> 00:07:59,990
example we don't really have a subsidy
and exact substitute for wood we still

378
00:07:59,990 --> 00:08:00,000
and exact substitute for wood we still
 

379
00:08:00,000 --> 00:08:01,280
and exact substitute for wood we still
build houses out of would we still go

380
00:08:01,280 --> 00:08:01,290
build houses out of would we still go
 

381
00:08:01,290 --> 00:08:03,380
build houses out of would we still go
furniture out of wood we like the look

382
00:08:03,380 --> 00:08:03,390
furniture out of wood we like the look
 

383
00:08:03,390 --> 00:08:05,120
furniture out of wood we like the look
we like the feel it's wood has certain

384
00:08:05,120 --> 00:08:05,130
we like the feel it's wood has certain
 

385
00:08:05,130 --> 00:08:07,760
we like the feel it's wood has certain
properties that synthetics don't there's

386
00:08:07,760 --> 00:08:07,770
properties that synthetics don't there's
 

387
00:08:07,770 --> 00:08:09,410
properties that synthetics don't there's
not that there's any magical or

388
00:08:09,410 --> 00:08:09,420
not that there's any magical or
 

389
00:08:09,420 --> 00:08:12,490
not that there's any magical or
mysterious about wood it's just that the

390
00:08:12,490 --> 00:08:12,500
mysterious about wood it's just that the
 

391
00:08:12,500 --> 00:08:15,710
mysterious about wood it's just that the
extra steps of duplicating everything

392
00:08:15,710 --> 00:08:15,720
extra steps of duplicating everything
 

393
00:08:15,720 --> 00:08:17,780
extra steps of duplicating everything
about wood is something we just haven't

394
00:08:17,780 --> 00:08:17,790
about wood is something we just haven't
 

395
00:08:17,790 --> 00:08:19,160
about wood is something we just haven't
bothered because we have wood likewise a

396
00:08:19,160 --> 00:08:19,170
bothered because we have wood likewise a
 

397
00:08:19,170 --> 00:08:20,480
bothered because we have wood likewise a
cotton I mean I'm wearing cotton

398
00:08:20,480 --> 00:08:20,490
cotton I mean I'm wearing cotton
 

399
00:08:20,490 --> 00:08:22,670
cotton I mean I'm wearing cotton
clothing now feels much better than the

400
00:08:22,670 --> 00:08:22,680
clothing now feels much better than the
 

401
00:08:22,680 --> 00:08:26,000
clothing now feels much better than the
polyester it's not that cotton has

402
00:08:26,000 --> 00:08:26,010
polyester it's not that cotton has
 

403
00:08:26,010 --> 00:08:28,370
polyester it's not that cotton has
something magic in it and it's not that

404
00:08:28,370 --> 00:08:28,380
something magic in it and it's not that
 

405
00:08:28,380 --> 00:08:30,050
something magic in it and it's not that
if there was that we couldn't ever

406
00:08:30,050 --> 00:08:30,060
if there was that we couldn't ever
 

407
00:08:30,060 --> 00:08:32,810
if there was that we couldn't ever
synthesize something exactly like cotton

408
00:08:32,810 --> 00:08:32,820
synthesize something exactly like cotton
 

409
00:08:32,820 --> 00:08:35,240
synthesize something exactly like cotton
but at some point it just it's just not

410
00:08:35,240 --> 00:08:35,250
but at some point it just it's just not
 

411
00:08:35,250 --> 00:08:36,920
but at some point it just it's just not
worth it we've got cotton and likewise

412
00:08:36,920 --> 00:08:36,930
worth it we've got cotton and likewise
 

413
00:08:36,930 --> 00:08:38,960
worth it we've got cotton and likewise
in the case of human intelligence the

414
00:08:38,960 --> 00:08:38,970
in the case of human intelligence the
 

415
00:08:38,970 --> 00:08:41,900
in the case of human intelligence the
goal of making an artificial system that

416
00:08:41,900 --> 00:08:41,910
goal of making an artificial system that
 

417
00:08:41,910 --> 00:08:44,690
goal of making an artificial system that
is exactly like the human brain is a

418
00:08:44,690 --> 00:08:44,700
is exactly like the human brain is a
 

419
00:08:44,700 --> 00:08:46,490
is exactly like the human brain is a
goal that we

420
00:08:46,490 --> 00:08:46,500
goal that we
 

421
00:08:46,500 --> 00:08:48,140
goal that we
no one's gonna pursue to the bitter end

422
00:08:48,140 --> 00:08:48,150
no one's gonna pursue to the bitter end
 

423
00:08:48,150 --> 00:08:51,530
no one's gonna pursue to the bitter end
I suspect because if you want tools that

424
00:08:51,530 --> 00:08:51,540
I suspect because if you want tools that
 

425
00:08:51,540 --> 00:08:53,000
I suspect because if you want tools that
do things better than humans you're not

426
00:08:53,000 --> 00:08:53,010
do things better than humans you're not
 

427
00:08:53,010 --> 00:08:54,110
do things better than humans you're not
going to care whether it does something

428
00:08:54,110 --> 00:08:54,120
going to care whether it does something
 

429
00:08:54,120 --> 00:08:55,610
going to care whether it does something
like humans so for example you're

430
00:08:55,610 --> 00:08:55,620
like humans so for example you're
 

431
00:08:55,620 --> 00:08:58,220
like humans so for example you're
diagnosing cancer or particularly

432
00:08:58,220 --> 00:08:58,230
diagnosing cancer or particularly
 

433
00:08:58,230 --> 00:09:00,410
diagnosing cancer or particularly
whether why set humans as your benchmark

434
00:09:00,410 --> 00:09:00,420
whether why set humans as your benchmark
 

435
00:09:00,420 --> 00:09:04,460
whether why set humans as your benchmark
but in in general I suspect you also

436
00:09:04,460 --> 00:09:04,470
but in in general I suspect you also
 

437
00:09:04,470 --> 00:09:07,610
but in in general I suspect you also
believe that even if the human should

438
00:09:07,610 --> 00:09:07,620
believe that even if the human should
 

439
00:09:07,620 --> 00:09:09,440
believe that even if the human should
not be a benchmark on women's don't want

440
00:09:09,440 --> 00:09:09,450
not be a benchmark on women's don't want
 

441
00:09:09,450 --> 00:09:10,880
not be a benchmark on women's don't want
to imitate humans in their system

442
00:09:10,880 --> 00:09:10,890
to imitate humans in their system
 

443
00:09:10,890 --> 00:09:13,490
to imitate humans in their system
there's a lot to be learned about how to

444
00:09:13,490 --> 00:09:13,500
there's a lot to be learned about how to
 

445
00:09:13,500 --> 00:09:15,260
there's a lot to be learned about how to
create an artificial intelligence system

446
00:09:15,260 --> 00:09:15,270
create an artificial intelligence system
 

447
00:09:15,270 --> 00:09:17,750
create an artificial intelligence system
by studying the human yeah III think

448
00:09:17,750 --> 00:09:17,760
by studying the human yeah III think
 

449
00:09:17,760 --> 00:09:18,440
by studying the human yeah III think
that's right

450
00:09:18,440 --> 00:09:18,450
that's right
 

451
00:09:18,450 --> 00:09:21,740
that's right
there in in the same way that to build

452
00:09:21,740 --> 00:09:21,750
there in in the same way that to build
 

453
00:09:21,750 --> 00:09:23,660
there in in the same way that to build
flying machines we want understand the

454
00:09:23,660 --> 00:09:23,670
flying machines we want understand the
 

455
00:09:23,670 --> 00:09:26,180
flying machines we want understand the
laws of aerodynamics and including birds

456
00:09:26,180 --> 00:09:26,190
laws of aerodynamics and including birds
 

457
00:09:26,190 --> 00:09:28,460
laws of aerodynamics and including birds
but not mimic the birds right but the

458
00:09:28,460 --> 00:09:28,470
but not mimic the birds right but the
 

459
00:09:28,470 --> 00:09:33,260
but not mimic the birds right but the
same laws you have a view on AI

460
00:09:33,260 --> 00:09:33,270
same laws you have a view on AI
 

461
00:09:33,270 --> 00:09:36,640
same laws you have a view on AI
artificial intelligence and safety that

462
00:09:36,640 --> 00:09:36,650
artificial intelligence and safety that
 

463
00:09:36,650 --> 00:09:41,560
artificial intelligence and safety that
from my perspective is refreshingly

464
00:09:41,560 --> 00:09:41,570
from my perspective is refreshingly
 

465
00:09:41,570 --> 00:09:46,820
from my perspective is refreshingly
rational or perhaps more importantly has

466
00:09:46,820 --> 00:09:46,830
rational or perhaps more importantly has
 

467
00:09:46,830 --> 00:09:49,820
rational or perhaps more importantly has
elements of positivity to it which I

468
00:09:49,820 --> 00:09:49,830
elements of positivity to it which I
 

469
00:09:49,830 --> 00:09:52,190
elements of positivity to it which I
think can be inspiring and empowering as

470
00:09:52,190 --> 00:09:52,200
think can be inspiring and empowering as
 

471
00:09:52,200 --> 00:09:55,070
think can be inspiring and empowering as
opposed to paralyzing for many people

472
00:09:55,070 --> 00:09:55,080
opposed to paralyzing for many people
 

473
00:09:55,080 --> 00:09:57,620
opposed to paralyzing for many people
including AI researchers the eventual

474
00:09:57,620 --> 00:09:57,630
including AI researchers the eventual
 

475
00:09:57,630 --> 00:10:01,190
including AI researchers the eventual
existential threat of AI is obvious not

476
00:10:01,190 --> 00:10:01,200
existential threat of AI is obvious not
 

477
00:10:01,200 --> 00:10:03,800
existential threat of AI is obvious not
only possible but obvious and for many

478
00:10:03,800 --> 00:10:03,810
only possible but obvious and for many
 

479
00:10:03,810 --> 00:10:06,500
only possible but obvious and for many
others including a researchers the

480
00:10:06,500 --> 00:10:06,510
others including a researchers the
 

481
00:10:06,510 --> 00:10:11,060
others including a researchers the
threat is not obvious so Elon Musk is is

482
00:10:11,060 --> 00:10:11,070
threat is not obvious so Elon Musk is is
 

483
00:10:11,070 --> 00:10:14,000
threat is not obvious so Elon Musk is is
famously in the highly concerned about

484
00:10:14,000 --> 00:10:14,010
famously in the highly concerned about
 

485
00:10:14,010 --> 00:10:16,550
famously in the highly concerned about
AI camp saying things like AI is far

486
00:10:16,550 --> 00:10:16,560
AI camp saying things like AI is far
 

487
00:10:16,560 --> 00:10:18,290
AI camp saying things like AI is far
more dangerous and nuclear weapons and

488
00:10:18,290 --> 00:10:18,300
more dangerous and nuclear weapons and
 

489
00:10:18,300 --> 00:10:21,620
more dangerous and nuclear weapons and
that AI will likely destroy human

490
00:10:21,620 --> 00:10:21,630
that AI will likely destroy human
 

491
00:10:21,630 --> 00:10:24,500
that AI will likely destroy human
civilization so in February you said

492
00:10:24,500 --> 00:10:24,510
civilization so in February you said
 

493
00:10:24,510 --> 00:10:27,550
civilization so in February you said
that if Elon was really serious about AI

494
00:10:27,550 --> 00:10:27,560
that if Elon was really serious about AI
 

495
00:10:27,560 --> 00:10:31,220
that if Elon was really serious about AI
they the threat of AI he would stop

496
00:10:31,220 --> 00:10:31,230
they the threat of AI he would stop
 

497
00:10:31,230 --> 00:10:33,260
they the threat of AI he would stop
building self-driving cars that he's

498
00:10:33,260 --> 00:10:33,270
building self-driving cars that he's
 

499
00:10:33,270 --> 00:10:35,050
building self-driving cars that he's
doing very successfully as part of Tesla

500
00:10:35,050 --> 00:10:35,060
doing very successfully as part of Tesla
 

501
00:10:35,060 --> 00:10:39,079
doing very successfully as part of Tesla
then he said Wow if even Pinker doesn't

502
00:10:39,079 --> 00:10:39,089
then he said Wow if even Pinker doesn't
 

503
00:10:39,089 --> 00:10:40,370
then he said Wow if even Pinker doesn't
understand the difference between arrow

504
00:10:40,370 --> 00:10:40,380
understand the difference between arrow
 

505
00:10:40,380 --> 00:10:43,790
understand the difference between arrow
AI like a car in general AI when the

506
00:10:43,790 --> 00:10:43,800
AI like a car in general AI when the
 

507
00:10:43,800 --> 00:10:45,740
AI like a car in general AI when the
latter literally has a million times

508
00:10:45,740 --> 00:10:45,750
latter literally has a million times
 

509
00:10:45,750 --> 00:10:48,320
latter literally has a million times
more compute power and an open-ended

510
00:10:48,320 --> 00:10:48,330
more compute power and an open-ended
 

511
00:10:48,330 --> 00:10:51,020
more compute power and an open-ended
utility function humanity is in deep

512
00:10:51,020 --> 00:10:51,030
utility function humanity is in deep
 

513
00:10:51,030 --> 00:10:54,260
utility function humanity is in deep
trouble so first what did you mean by

514
00:10:54,260 --> 00:10:54,270
trouble so first what did you mean by
 

515
00:10:54,270 --> 00:10:56,720
trouble so first what did you mean by
the statement about Elon Musk should

516
00:10:56,720 --> 00:10:56,730
the statement about Elon Musk should
 

517
00:10:56,730 --> 00:10:58,340
the statement about Elon Musk should
stop Bill ourselves driving cars if he's

518
00:10:58,340 --> 00:10:58,350
stop Bill ourselves driving cars if he's
 

519
00:10:58,350 --> 00:10:59,720
stop Bill ourselves driving cars if he's
deeply concerned

520
00:10:59,720 --> 00:10:59,730
deeply concerned
 

521
00:10:59,730 --> 00:11:02,179
deeply concerned
not last time that Elon Musk has fired

522
00:11:02,179 --> 00:11:02,189
not last time that Elon Musk has fired
 

523
00:11:02,189 --> 00:11:05,509
not last time that Elon Musk has fired
off an intemperate tweet well we live in

524
00:11:05,509 --> 00:11:05,519
off an intemperate tweet well we live in
 

525
00:11:05,519 --> 00:11:09,049
off an intemperate tweet well we live in
a world where Twitter has power yes yeah

526
00:11:09,049 --> 00:11:09,059
a world where Twitter has power yes yeah
 

527
00:11:09,059 --> 00:11:14,720
a world where Twitter has power yes yeah
I think the the that there are two kinds

528
00:11:14,720 --> 00:11:14,730
I think the the that there are two kinds
 

529
00:11:14,730 --> 00:11:16,159
I think the the that there are two kinds
of existential threat that have been

530
00:11:16,159 --> 00:11:16,169
of existential threat that have been
 

531
00:11:16,169 --> 00:11:17,809
of existential threat that have been
discussed in connection with artificial

532
00:11:17,809 --> 00:11:17,819
discussed in connection with artificial
 

533
00:11:17,819 --> 00:11:19,009
discussed in connection with artificial
intelligence and I think that they're

534
00:11:19,009 --> 00:11:19,019
intelligence and I think that they're
 

535
00:11:19,019 --> 00:11:21,949
intelligence and I think that they're
both incoherent one of them is vague

536
00:11:21,949 --> 00:11:21,959
both incoherent one of them is vague
 

537
00:11:21,959 --> 00:11:26,299
both incoherent one of them is vague
fear of AI takeover that it just as we

538
00:11:26,299 --> 00:11:26,309
fear of AI takeover that it just as we
 

539
00:11:26,309 --> 00:11:28,759
fear of AI takeover that it just as we
subjugated animals and less

540
00:11:28,759 --> 00:11:28,769
subjugated animals and less
 

541
00:11:28,769 --> 00:11:31,249
subjugated animals and less
technologically advanced people's so if

542
00:11:31,249 --> 00:11:31,259
technologically advanced people's so if
 

543
00:11:31,259 --> 00:11:32,869
technologically advanced people's so if
we build something that's more advanced

544
00:11:32,869 --> 00:11:32,879
we build something that's more advanced
 

545
00:11:32,879 --> 00:11:34,849
we build something that's more advanced
than us it will inevitably turn us into

546
00:11:34,849 --> 00:11:34,859
than us it will inevitably turn us into
 

547
00:11:34,859 --> 00:11:38,720
than us it will inevitably turn us into
pets or slaves or or domesticated animal

548
00:11:38,720 --> 00:11:38,730
pets or slaves or or domesticated animal
 

549
00:11:38,730 --> 00:11:39,590
pets or slaves or or domesticated animal
equivalents

550
00:11:39,590 --> 00:11:39,600
equivalents
 

551
00:11:39,600 --> 00:11:42,559
equivalents
I think this confuses intelligence with

552
00:11:42,559 --> 00:11:42,569
I think this confuses intelligence with
 

553
00:11:42,569 --> 00:11:46,280
I think this confuses intelligence with
a will to power that it so happens that

554
00:11:46,280 --> 00:11:46,290
a will to power that it so happens that
 

555
00:11:46,290 --> 00:11:48,619
a will to power that it so happens that
in the intelligence system we are most

556
00:11:48,619 --> 00:11:48,629
in the intelligence system we are most
 

557
00:11:48,629 --> 00:11:51,319
in the intelligence system we are most
familiar with namely Homo sapiens we are

558
00:11:51,319 --> 00:11:51,329
familiar with namely Homo sapiens we are
 

559
00:11:51,329 --> 00:11:53,119
familiar with namely Homo sapiens we are
products of natural selection which is a

560
00:11:53,119 --> 00:11:53,129
products of natural selection which is a
 

561
00:11:53,129 --> 00:11:55,069
products of natural selection which is a
competitive process and so bundled

562
00:11:55,069 --> 00:11:55,079
competitive process and so bundled
 

563
00:11:55,079 --> 00:11:56,299
competitive process and so bundled
together with our problem-solving

564
00:11:56,299 --> 00:11:56,309
together with our problem-solving
 

565
00:11:56,309 --> 00:11:59,299
together with our problem-solving
capacity are a number of nasty traits

566
00:11:59,299 --> 00:11:59,309
capacity are a number of nasty traits
 

567
00:11:59,309 --> 00:12:03,769
capacity are a number of nasty traits
like dominance and exploitation and

568
00:12:03,769 --> 00:12:03,779
like dominance and exploitation and
 

569
00:12:03,779 --> 00:12:06,289
like dominance and exploitation and
maximization of power and glory and

570
00:12:06,289 --> 00:12:06,299
maximization of power and glory and
 

571
00:12:06,299 --> 00:12:09,229
maximization of power and glory and
resources and influence there's no

572
00:12:09,229 --> 00:12:09,239
resources and influence there's no
 

573
00:12:09,239 --> 00:12:10,579
resources and influence there's no
reason to think that sheer

574
00:12:10,579 --> 00:12:10,589
reason to think that sheer
 

575
00:12:10,589 --> 00:12:12,739
reason to think that sheer
problem-solving capability will set that

576
00:12:12,739 --> 00:12:12,749
problem-solving capability will set that
 

577
00:12:12,749 --> 00:12:14,629
problem-solving capability will set that
as one of its goals its goals will be

578
00:12:14,629 --> 00:12:14,639
as one of its goals its goals will be
 

579
00:12:14,639 --> 00:12:17,539
as one of its goals its goals will be
whatever we set it its goals as and as

580
00:12:17,539 --> 00:12:17,549
whatever we set it its goals as and as
 

581
00:12:17,549 --> 00:12:18,829
whatever we set it its goals as and as
long as someone isn't building a

582
00:12:18,829 --> 00:12:18,839
long as someone isn't building a
 

583
00:12:18,839 --> 00:12:21,819
long as someone isn't building a
megalomaniacal artificial intelligence

584
00:12:21,819 --> 00:12:21,829
megalomaniacal artificial intelligence
 

585
00:12:21,829 --> 00:12:23,900
megalomaniacal artificial intelligence
and there's no reason to think that it

586
00:12:23,900 --> 00:12:23,910
and there's no reason to think that it
 

587
00:12:23,910 --> 00:12:25,099
and there's no reason to think that it
would naturally evolve in that direction

588
00:12:25,099 --> 00:12:25,109
would naturally evolve in that direction
 

589
00:12:25,109 --> 00:12:26,659
would naturally evolve in that direction
now you might say well what if we gave

590
00:12:26,659 --> 00:12:26,669
now you might say well what if we gave
 

591
00:12:26,669 --> 00:12:30,650
now you might say well what if we gave
it the goal of maximizing its own power

592
00:12:30,650 --> 00:12:30,660
it the goal of maximizing its own power
 

593
00:12:30,660 --> 00:12:32,629
it the goal of maximizing its own power
source well that's a pretty stupid goal

594
00:12:32,629 --> 00:12:32,639
source well that's a pretty stupid goal
 

595
00:12:32,639 --> 00:12:34,729
source well that's a pretty stupid goal
to give a an autonomous system you don't

596
00:12:34,729 --> 00:12:34,739
to give a an autonomous system you don't
 

597
00:12:34,739 --> 00:12:36,559
to give a an autonomous system you don't
give it that goal I mean that's just

598
00:12:36,559 --> 00:12:36,569
give it that goal I mean that's just
 

599
00:12:36,569 --> 00:12:39,710
give it that goal I mean that's just
self-evident we idiotic so if you look

600
00:12:39,710 --> 00:12:39,720
self-evident we idiotic so if you look
 

601
00:12:39,720 --> 00:12:41,809
self-evident we idiotic so if you look
at the history of the world there's been

602
00:12:41,809 --> 00:12:41,819
at the history of the world there's been
 

603
00:12:41,819 --> 00:12:43,279
at the history of the world there's been
a lot of opportunities where engineers

604
00:12:43,279 --> 00:12:43,289
a lot of opportunities where engineers
 

605
00:12:43,289 --> 00:12:45,739
a lot of opportunities where engineers
could instill in a system destructive

606
00:12:45,739 --> 00:12:45,749
could instill in a system destructive
 

607
00:12:45,749 --> 00:12:47,419
could instill in a system destructive
power and they choose not to because

608
00:12:47,419 --> 00:12:47,429
power and they choose not to because
 

609
00:12:47,429 --> 00:12:48,739
power and they choose not to because
that's the natural process of

610
00:12:48,739 --> 00:12:48,749
that's the natural process of
 

611
00:12:48,749 --> 00:12:50,989
that's the natural process of
Engineering well weapons I mean if

612
00:12:50,989 --> 00:12:50,999
Engineering well weapons I mean if
 

613
00:12:50,999 --> 00:12:52,519
Engineering well weapons I mean if
you're building a weapon its goal is to

614
00:12:52,519 --> 00:12:52,529
you're building a weapon its goal is to
 

615
00:12:52,529 --> 00:12:55,129
you're building a weapon its goal is to
destroy people and so I think they're

616
00:12:55,129 --> 00:12:55,139
destroy people and so I think they're
 

617
00:12:55,139 --> 00:12:57,409
destroy people and so I think they're
good reasons to not not build certain

618
00:12:57,409 --> 00:12:57,419
good reasons to not not build certain
 

619
00:12:57,419 --> 00:12:58,759
good reasons to not not build certain
kinds of weapons I think the building

620
00:12:58,759 --> 00:12:58,769
kinds of weapons I think the building
 

621
00:12:58,769 --> 00:13:00,489
kinds of weapons I think the building
nuclear weapons was a massive mistake

622
00:13:00,489 --> 00:13:00,499
nuclear weapons was a massive mistake
 

623
00:13:00,499 --> 00:13:04,669
nuclear weapons was a massive mistake
but probably do you think so

624
00:13:04,669 --> 00:13:04,679
but probably do you think so
 

625
00:13:04,679 --> 00:13:06,529
but probably do you think so
maybe pause on that because that is one

626
00:13:06,529 --> 00:13:06,539
maybe pause on that because that is one
 

627
00:13:06,539 --> 00:13:09,199
maybe pause on that because that is one
of the serious threats do you think that

628
00:13:09,199 --> 00:13:09,209
of the serious threats do you think that
 

629
00:13:09,209 --> 00:13:12,439
of the serious threats do you think that
it was a mistake in a sense that it was

630
00:13:12,439 --> 00:13:12,449
it was a mistake in a sense that it was
 

631
00:13:12,449 --> 00:13:13,250
it was a mistake in a sense that it was
should have been stopped

632
00:13:13,250 --> 00:13:13,260
should have been stopped
 

633
00:13:13,260 --> 00:13:15,650
should have been stopped
early on or do you think it's just an

634
00:13:15,650 --> 00:13:15,660
early on or do you think it's just an
 

635
00:13:15,660 --> 00:13:18,860
early on or do you think it's just an
unfortunate event of invention that this

636
00:13:18,860 --> 00:13:18,870
unfortunate event of invention that this
 

637
00:13:18,870 --> 00:13:21,050
unfortunate event of invention that this
was invented we think it's possible to

638
00:13:21,050 --> 00:13:21,060
was invented we think it's possible to
 

639
00:13:21,060 --> 00:13:22,820
was invented we think it's possible to
stop I guess is the question it's hard

640
00:13:22,820 --> 00:13:22,830
stop I guess is the question it's hard
 

641
00:13:22,830 --> 00:13:24,410
stop I guess is the question it's hard
to rewind the clock because of course it

642
00:13:24,410 --> 00:13:24,420
to rewind the clock because of course it
 

643
00:13:24,420 --> 00:13:26,300
to rewind the clock because of course it
was invented in the context of World War

644
00:13:26,300 --> 00:13:26,310
was invented in the context of World War
 

645
00:13:26,310 --> 00:13:29,120
was invented in the context of World War
two and the fear that the Nazis might

646
00:13:29,120 --> 00:13:29,130
two and the fear that the Nazis might
 

647
00:13:29,130 --> 00:13:32,030
two and the fear that the Nazis might
develop one first then once was

648
00:13:32,030 --> 00:13:32,040
develop one first then once was
 

649
00:13:32,040 --> 00:13:34,640
develop one first then once was
initiated for that reason it was it it

650
00:13:34,640 --> 00:13:34,650
initiated for that reason it was it it
 

651
00:13:34,650 --> 00:13:36,680
initiated for that reason it was it it
was hard to turn off especially since

652
00:13:36,680 --> 00:13:36,690
was hard to turn off especially since
 

653
00:13:36,690 --> 00:13:39,530
was hard to turn off especially since
winning the war against the Japanese and

654
00:13:39,530 --> 00:13:39,540
winning the war against the Japanese and
 

655
00:13:39,540 --> 00:13:42,380
winning the war against the Japanese and
the Nazis was such an overwhelming goal

656
00:13:42,380 --> 00:13:42,390
the Nazis was such an overwhelming goal
 

657
00:13:42,390 --> 00:13:45,350
the Nazis was such an overwhelming goal
of every responsible person that there's

658
00:13:45,350 --> 00:13:45,360
of every responsible person that there's
 

659
00:13:45,360 --> 00:13:46,550
of every responsible person that there's
just nothing that people wouldn't have

660
00:13:46,550 --> 00:13:46,560
just nothing that people wouldn't have
 

661
00:13:46,560 --> 00:13:49,730
just nothing that people wouldn't have
done then to ensure victory it's quite

662
00:13:49,730 --> 00:13:49,740
done then to ensure victory it's quite
 

663
00:13:49,740 --> 00:13:51,080
done then to ensure victory it's quite
possible if World War two hadn't

664
00:13:51,080 --> 00:13:51,090
possible if World War two hadn't
 

665
00:13:51,090 --> 00:13:52,910
possible if World War two hadn't
happened that nuclear weapons wouldn't

666
00:13:52,910 --> 00:13:52,920
happened that nuclear weapons wouldn't
 

667
00:13:52,920 --> 00:13:55,520
happened that nuclear weapons wouldn't
have been invented we can't know but I

668
00:13:55,520 --> 00:13:55,530
have been invented we can't know but I
 

669
00:13:55,530 --> 00:13:56,930
have been invented we can't know but I
don't think it was by any means a

670
00:13:56,930 --> 00:13:56,940
don't think it was by any means a
 

671
00:13:56,940 --> 00:13:58,640
don't think it was by any means a
necessity any more than some of the

672
00:13:58,640 --> 00:13:58,650
necessity any more than some of the
 

673
00:13:58,650 --> 00:14:00,440
necessity any more than some of the
other weapon systems that were

674
00:14:00,440 --> 00:14:00,450
other weapon systems that were
 

675
00:14:00,450 --> 00:14:02,990
other weapon systems that were
envisioned but never implemented like

676
00:14:02,990 --> 00:14:03,000
envisioned but never implemented like
 

677
00:14:03,000 --> 00:14:06,080
envisioned but never implemented like
planes that would disperse poison gas

678
00:14:06,080 --> 00:14:06,090
planes that would disperse poison gas
 

679
00:14:06,090 --> 00:14:08,990
planes that would disperse poison gas
over cities like crop dusters or systems

680
00:14:08,990 --> 00:14:09,000
over cities like crop dusters or systems
 

681
00:14:09,000 --> 00:14:12,320
over cities like crop dusters or systems
to try to do to create earthquakes and

682
00:14:12,320 --> 00:14:12,330
to try to do to create earthquakes and
 

683
00:14:12,330 --> 00:14:15,320
to try to do to create earthquakes and
tsunamis in enemy countries to weaponize

684
00:14:15,320 --> 00:14:15,330
tsunamis in enemy countries to weaponize
 

685
00:14:15,330 --> 00:14:17,570
tsunamis in enemy countries to weaponize
the weather weaponize solar flares all

686
00:14:17,570 --> 00:14:17,580
the weather weaponize solar flares all
 

687
00:14:17,580 --> 00:14:20,360
the weather weaponize solar flares all
kinds of crazy schemes that that we

688
00:14:20,360 --> 00:14:20,370
kinds of crazy schemes that that we
 

689
00:14:20,370 --> 00:14:22,160
kinds of crazy schemes that that we
thought the better off I think analogies

690
00:14:22,160 --> 00:14:22,170
thought the better off I think analogies
 

691
00:14:22,170 --> 00:14:24,170
thought the better off I think analogies
between nuclear weapons and artificial

692
00:14:24,170 --> 00:14:24,180
between nuclear weapons and artificial
 

693
00:14:24,180 --> 00:14:26,240
between nuclear weapons and artificial
intelligence are fundamentally misguided

694
00:14:26,240 --> 00:14:26,250
intelligence are fundamentally misguided
 

695
00:14:26,250 --> 00:14:28,100
intelligence are fundamentally misguided
because the whole point of nuclear

696
00:14:28,100 --> 00:14:28,110
because the whole point of nuclear
 

697
00:14:28,110 --> 00:14:30,560
because the whole point of nuclear
weapons is to destroy things the point

698
00:14:30,560 --> 00:14:30,570
weapons is to destroy things the point
 

699
00:14:30,570 --> 00:14:32,030
weapons is to destroy things the point
of artificial intelligence is not to

700
00:14:32,030 --> 00:14:32,040
of artificial intelligence is not to
 

701
00:14:32,040 --> 00:14:35,600
of artificial intelligence is not to
destroy things so the analogy is is

702
00:14:35,600 --> 00:14:35,610
destroy things so the analogy is is
 

703
00:14:35,610 --> 00:14:37,370
destroy things so the analogy is is
misleading so there's two artificial

704
00:14:37,370 --> 00:14:37,380
misleading so there's two artificial
 

705
00:14:37,380 --> 00:14:38,810
misleading so there's two artificial
intelligence you mentioned the first one

706
00:14:38,810 --> 00:14:38,820
intelligence you mentioned the first one
 

707
00:14:38,820 --> 00:14:42,020
intelligence you mentioned the first one
was the intelligence all know hungry

708
00:14:42,020 --> 00:14:42,030
was the intelligence all know hungry
 

709
00:14:42,030 --> 00:14:43,490
was the intelligence all know hungry
yeah the system that we design ourselves

710
00:14:43,490 --> 00:14:43,500
yeah the system that we design ourselves
 

711
00:14:43,500 --> 00:14:45,440
yeah the system that we design ourselves
where we give it the goals goals are

712
00:14:45,440 --> 00:14:45,450
where we give it the goals goals are
 

713
00:14:45,450 --> 00:14:49,310
where we give it the goals goals are
external to the means to attain the

714
00:14:49,310 --> 00:14:49,320
external to the means to attain the
 

715
00:14:49,320 --> 00:14:53,900
external to the means to attain the
goals I if we don't design an artificial

716
00:14:53,900 --> 00:14:53,910
goals I if we don't design an artificial
 

717
00:14:53,910 --> 00:14:56,140
goals I if we don't design an artificial
intelligence system to maximize

718
00:14:56,140 --> 00:14:56,150
intelligence system to maximize
 

719
00:14:56,150 --> 00:14:58,370
intelligence system to maximize
dominance then it won't maximize

720
00:14:58,370 --> 00:14:58,380
dominance then it won't maximize
 

721
00:14:58,380 --> 00:15:00,560
dominance then it won't maximize
dominance it just that we're so familiar

722
00:15:00,560 --> 00:15:00,570
dominance it just that we're so familiar
 

723
00:15:00,570 --> 00:15:03,350
dominance it just that we're so familiar
with Homo sapiens when these two traits

724
00:15:03,350 --> 00:15:03,360
with Homo sapiens when these two traits
 

725
00:15:03,360 --> 00:15:05,660
with Homo sapiens when these two traits
come bundled together particularly in

726
00:15:05,660 --> 00:15:05,670
come bundled together particularly in
 

727
00:15:05,670 --> 00:15:09,110
come bundled together particularly in
men that we are apt to confuse high

728
00:15:09,110 --> 00:15:09,120
men that we are apt to confuse high
 

729
00:15:09,120 --> 00:15:13,340
men that we are apt to confuse high
intelligence with a will to power but

730
00:15:13,340 --> 00:15:13,350
intelligence with a will to power but
 

731
00:15:13,350 --> 00:15:16,550
intelligence with a will to power but
that's just an error the other fear is

732
00:15:16,550 --> 00:15:16,560
that's just an error the other fear is
 

733
00:15:16,560 --> 00:15:18,380
that's just an error the other fear is
that we'll be collateral damage that

734
00:15:18,380 --> 00:15:18,390
that we'll be collateral damage that
 

735
00:15:18,390 --> 00:15:21,260
that we'll be collateral damage that
will give artificial intelligence a goal

736
00:15:21,260 --> 00:15:21,270
will give artificial intelligence a goal
 

737
00:15:21,270 --> 00:15:24,410
will give artificial intelligence a goal
like make paperclips and it will pursue

738
00:15:24,410 --> 00:15:24,420
like make paperclips and it will pursue
 

739
00:15:24,420 --> 00:15:26,740
like make paperclips and it will pursue
that goal so brilliantly that

740
00:15:26,740 --> 00:15:26,750
that goal so brilliantly that
 

741
00:15:26,750 --> 00:15:28,000
that goal so brilliantly that
before we can stop it it turns us into

742
00:15:28,000 --> 00:15:28,010
before we can stop it it turns us into
 

743
00:15:28,010 --> 00:15:30,460
before we can stop it it turns us into
paperclips we'll give it the goal of

744
00:15:30,460 --> 00:15:30,470
paperclips we'll give it the goal of
 

745
00:15:30,470 --> 00:15:32,890
paperclips we'll give it the goal of
curing cancer and it will turn us into

746
00:15:32,890 --> 00:15:32,900
curing cancer and it will turn us into
 

747
00:15:32,900 --> 00:15:35,290
curing cancer and it will turn us into
guinea pigs for lethal experiments or

748
00:15:35,290 --> 00:15:35,300
guinea pigs for lethal experiments or
 

749
00:15:35,300 --> 00:15:37,900
guinea pigs for lethal experiments or
give it the goal of world peace and its

750
00:15:37,900 --> 00:15:37,910
give it the goal of world peace and its
 

751
00:15:37,910 --> 00:15:39,820
give it the goal of world peace and its
conception of world pieces no people

752
00:15:39,820 --> 00:15:39,830
conception of world pieces no people
 

753
00:15:39,830 --> 00:15:41,620
conception of world pieces no people
therefore no fighting and so it'll kill

754
00:15:41,620 --> 00:15:41,630
therefore no fighting and so it'll kill
 

755
00:15:41,630 --> 00:15:43,570
therefore no fighting and so it'll kill
us all now I think these are utterly

756
00:15:43,570 --> 00:15:43,580
us all now I think these are utterly
 

757
00:15:43,580 --> 00:15:45,010
us all now I think these are utterly
fanciful in fact I think they're

758
00:15:45,010 --> 00:15:45,020
fanciful in fact I think they're
 

759
00:15:45,020 --> 00:15:47,620
fanciful in fact I think they're
actually self-defeating they first of

760
00:15:47,620 --> 00:15:47,630
actually self-defeating they first of
 

761
00:15:47,630 --> 00:15:49,690
actually self-defeating they first of
all assume that we're going to be so

762
00:15:49,690 --> 00:15:49,700
all assume that we're going to be so
 

763
00:15:49,700 --> 00:15:51,130
all assume that we're going to be so
brilliant that we can design an

764
00:15:51,130 --> 00:15:51,140
brilliant that we can design an
 

765
00:15:51,140 --> 00:15:52,780
brilliant that we can design an
artificial intelligence that can cure

766
00:15:52,780 --> 00:15:52,790
artificial intelligence that can cure
 

767
00:15:52,790 --> 00:15:55,900
artificial intelligence that can cure
cancer but so stupid that we don't

768
00:15:55,900 --> 00:15:55,910
cancer but so stupid that we don't
 

769
00:15:55,910 --> 00:15:58,360
cancer but so stupid that we don't
specify what we mean by curing cancer in

770
00:15:58,360 --> 00:15:58,370
specify what we mean by curing cancer in
 

771
00:15:58,370 --> 00:16:00,160
specify what we mean by curing cancer in
enough detail that it won't kill us in

772
00:16:00,160 --> 00:16:00,170
enough detail that it won't kill us in
 

773
00:16:00,170 --> 00:16:02,980
enough detail that it won't kill us in
the process and it assumes that the

774
00:16:02,980 --> 00:16:02,990
the process and it assumes that the
 

775
00:16:02,990 --> 00:16:05,530
the process and it assumes that the
system will be so smart that it can cure

776
00:16:05,530 --> 00:16:05,540
system will be so smart that it can cure
 

777
00:16:05,540 --> 00:16:08,320
system will be so smart that it can cure
cancer but so idiotic that it doesn't

778
00:16:08,320 --> 00:16:08,330
cancer but so idiotic that it doesn't
 

779
00:16:08,330 --> 00:16:09,820
cancer but so idiotic that it doesn't
can't figure out that what we mean by

780
00:16:09,820 --> 00:16:09,830
can't figure out that what we mean by
 

781
00:16:09,830 --> 00:16:13,000
can't figure out that what we mean by
curing cancer is not killing everyone so

782
00:16:13,000 --> 00:16:13,010
curing cancer is not killing everyone so
 

783
00:16:13,010 --> 00:16:14,890
curing cancer is not killing everyone so
I think that the the collateral damage

784
00:16:14,890 --> 00:16:14,900
I think that the the collateral damage
 

785
00:16:14,900 --> 00:16:16,960
I think that the the collateral damage
scenario the value alignment problem is

786
00:16:16,960 --> 00:16:16,970
scenario the value alignment problem is
 

787
00:16:16,970 --> 00:16:19,870
scenario the value alignment problem is
is also based on a misconception so one

788
00:16:19,870 --> 00:16:19,880
is also based on a misconception so one
 

789
00:16:19,880 --> 00:16:21,490
is also based on a misconception so one
of the challenges of course we don't

790
00:16:21,490 --> 00:16:21,500
of the challenges of course we don't
 

791
00:16:21,500 --> 00:16:23,260
of the challenges of course we don't
know how to build either system

792
00:16:23,260 --> 00:16:23,270
know how to build either system
 

793
00:16:23,270 --> 00:16:25,120
know how to build either system
currently or are we even close to

794
00:16:25,120 --> 00:16:25,130
currently or are we even close to
 

795
00:16:25,130 --> 00:16:26,800
currently or are we even close to
knowing of course those things can

796
00:16:26,800 --> 00:16:26,810
knowing of course those things can
 

797
00:16:26,810 --> 00:16:28,710
knowing of course those things can
change overnight but at this time

798
00:16:28,710 --> 00:16:28,720
change overnight but at this time
 

799
00:16:28,720 --> 00:16:31,079
change overnight but at this time
theorizing about it is very challenging

800
00:16:31,079 --> 00:16:31,089
theorizing about it is very challenging
 

801
00:16:31,089 --> 00:16:33,910
theorizing about it is very challenging
in either direction so that that's

802
00:16:33,910 --> 00:16:33,920
in either direction so that that's
 

803
00:16:33,920 --> 00:16:35,550
in either direction so that that's
probably at the core the problem is

804
00:16:35,550 --> 00:16:35,560
probably at the core the problem is
 

805
00:16:35,560 --> 00:16:38,800
probably at the core the problem is
without that ability to reason about the

806
00:16:38,800 --> 00:16:38,810
without that ability to reason about the
 

807
00:16:38,810 --> 00:16:40,950
without that ability to reason about the
real engineering things here at hand is

808
00:16:40,950 --> 00:16:40,960
real engineering things here at hand is
 

809
00:16:40,960 --> 00:16:43,570
real engineering things here at hand is
your imagination runs away with things

810
00:16:43,570 --> 00:16:43,580
your imagination runs away with things
 

811
00:16:43,580 --> 00:16:47,260
your imagination runs away with things
exactly but let me sort of ask what do

812
00:16:47,260 --> 00:16:47,270
exactly but let me sort of ask what do
 

813
00:16:47,270 --> 00:16:49,420
exactly but let me sort of ask what do
you think was the motivation the thought

814
00:16:49,420 --> 00:16:49,430
you think was the motivation the thought
 

815
00:16:49,430 --> 00:16:51,760
you think was the motivation the thought
process of elam Wasco i build autonomous

816
00:16:51,760 --> 00:16:51,770
process of elam Wasco i build autonomous
 

817
00:16:51,770 --> 00:16:53,860
process of elam Wasco i build autonomous
vehicles I study autonomous vehicles I

818
00:16:53,860 --> 00:16:53,870
vehicles I study autonomous vehicles I
 

819
00:16:53,870 --> 00:16:56,590
vehicles I study autonomous vehicles I
studied Tesla autopilot I think it is

820
00:16:56,590 --> 00:16:56,600
studied Tesla autopilot I think it is
 

821
00:16:56,600 --> 00:16:58,680
studied Tesla autopilot I think it is
one of the greatest currently

822
00:16:58,680 --> 00:16:58,690
one of the greatest currently
 

823
00:16:58,690 --> 00:17:00,670
one of the greatest currently
application large scale application of

824
00:17:00,670 --> 00:17:00,680
application large scale application of
 

825
00:17:00,680 --> 00:17:02,800
application large scale application of
artificial intelligence in the world it

826
00:17:02,800 --> 00:17:02,810
artificial intelligence in the world it
 

827
00:17:02,810 --> 00:17:05,319
artificial intelligence in the world it
has a potentially a very positive impact

828
00:17:05,319 --> 00:17:05,329
has a potentially a very positive impact
 

829
00:17:05,329 --> 00:17:08,319
has a potentially a very positive impact
on society so how does a person who's

830
00:17:08,319 --> 00:17:08,329
on society so how does a person who's
 

831
00:17:08,329 --> 00:17:10,809
on society so how does a person who's
creating this very good quote/unquote

832
00:17:10,809 --> 00:17:10,819
creating this very good quote/unquote
 

833
00:17:10,819 --> 00:17:14,470
creating this very good quote/unquote
narrow AI system also seem to be so

834
00:17:14,470 --> 00:17:14,480
narrow AI system also seem to be so
 

835
00:17:14,480 --> 00:17:18,340
narrow AI system also seem to be so
concerned about this other general AI

836
00:17:18,340 --> 00:17:18,350
concerned about this other general AI
 

837
00:17:18,350 --> 00:17:19,990
concerned about this other general AI
what do you think is the motivation

838
00:17:19,990 --> 00:17:20,000
what do you think is the motivation
 

839
00:17:20,000 --> 00:17:21,130
what do you think is the motivation
there what do you think is the thing

840
00:17:21,130 --> 00:17:21,140
there what do you think is the thing
 

841
00:17:21,140 --> 00:17:23,319
there what do you think is the thing
really you probably have to ask him but

842
00:17:23,319 --> 00:17:23,329
really you probably have to ask him but
 

843
00:17:23,329 --> 00:17:27,179
really you probably have to ask him but
there and and he is notoriously

844
00:17:27,179 --> 00:17:27,189
there and and he is notoriously
 

845
00:17:27,189 --> 00:17:31,180
there and and he is notoriously
flamboyant impulsive to the as we have

846
00:17:31,180 --> 00:17:31,190
flamboyant impulsive to the as we have
 

847
00:17:31,190 --> 00:17:32,680
flamboyant impulsive to the as we have
just seen to the detriment of his own

848
00:17:32,680 --> 00:17:32,690
just seen to the detriment of his own
 

849
00:17:32,690 --> 00:17:36,280
just seen to the detriment of his own
goals of the health of a company so I

850
00:17:36,280 --> 00:17:36,290
goals of the health of a company so I
 

851
00:17:36,290 --> 00:17:38,030
goals of the health of a company so I
don't know what's going on

852
00:17:38,030 --> 00:17:38,040
don't know what's going on
 

853
00:17:38,040 --> 00:17:39,770
don't know what's going on
on his mind you probably have to ask him

854
00:17:39,770 --> 00:17:39,780
on his mind you probably have to ask him
 

855
00:17:39,780 --> 00:17:42,110
on his mind you probably have to ask him
but I don't think the and I don't think

856
00:17:42,110 --> 00:17:42,120
but I don't think the and I don't think
 

857
00:17:42,120 --> 00:17:44,180
but I don't think the and I don't think
the distinction between special-purpose

858
00:17:44,180 --> 00:17:44,190
the distinction between special-purpose
 

859
00:17:44,190 --> 00:17:48,380
the distinction between special-purpose
a and so-called general is relevant that

860
00:17:48,380 --> 00:17:48,390
a and so-called general is relevant that
 

861
00:17:48,390 --> 00:17:50,900
a and so-called general is relevant that
in the same way that special-purpose AI

862
00:17:50,900 --> 00:17:50,910
in the same way that special-purpose AI
 

863
00:17:50,910 --> 00:17:53,990
in the same way that special-purpose AI
is not going to do anything conceivable

864
00:17:53,990 --> 00:17:54,000
is not going to do anything conceivable
 

865
00:17:54,000 --> 00:17:55,400
is not going to do anything conceivable
in order to attain a goal all

866
00:17:55,400 --> 00:17:55,410
in order to attain a goal all
 

867
00:17:55,410 --> 00:17:58,520
in order to attain a goal all
engineering systems have to are designed

868
00:17:58,520 --> 00:17:58,530
engineering systems have to are designed
 

869
00:17:58,530 --> 00:18:00,680
engineering systems have to are designed
to trade off across multiple goals

870
00:18:00,680 --> 00:18:00,690
to trade off across multiple goals
 

871
00:18:00,690 --> 00:18:03,080
to trade off across multiple goals
well we build cars in the first place we

872
00:18:03,080 --> 00:18:03,090
well we build cars in the first place we
 

873
00:18:03,090 --> 00:18:05,420
well we build cars in the first place we
didn't forget to install brakes because

874
00:18:05,420 --> 00:18:05,430
didn't forget to install brakes because
 

875
00:18:05,430 --> 00:18:07,850
didn't forget to install brakes because
the goal of a car is to go fast it

876
00:18:07,850 --> 00:18:07,860
the goal of a car is to go fast it
 

877
00:18:07,860 --> 00:18:09,800
the goal of a car is to go fast it
occurred to people yes you want to go

878
00:18:09,800 --> 00:18:09,810
occurred to people yes you want to go
 

879
00:18:09,810 --> 00:18:12,530
occurred to people yes you want to go
fast but not always so you build an

880
00:18:12,530 --> 00:18:12,540
fast but not always so you build an
 

881
00:18:12,540 --> 00:18:15,080
fast but not always so you build an
brakes too likewise if a car is going to

882
00:18:15,080 --> 00:18:15,090
brakes too likewise if a car is going to
 

883
00:18:15,090 --> 00:18:18,530
brakes too likewise if a car is going to
be autonomous that doesn't and program

884
00:18:18,530 --> 00:18:18,540
be autonomous that doesn't and program
 

885
00:18:18,540 --> 00:18:19,790
be autonomous that doesn't and program
it to take the shortest route to the

886
00:18:19,790 --> 00:18:19,800
it to take the shortest route to the
 

887
00:18:19,800 --> 00:18:21,230
it to take the shortest route to the
airport it's not going to take the

888
00:18:21,230 --> 00:18:21,240
airport it's not going to take the
 

889
00:18:21,240 --> 00:18:23,390
airport it's not going to take the
diagonal and mow down people and trees

890
00:18:23,390 --> 00:18:23,400
diagonal and mow down people and trees
 

891
00:18:23,400 --> 00:18:24,980
diagonal and mow down people and trees
and fences because that's the shortest

892
00:18:24,980 --> 00:18:24,990
and fences because that's the shortest
 

893
00:18:24,990 --> 00:18:26,840
and fences because that's the shortest
route that's not what we mean by the

894
00:18:26,840 --> 00:18:26,850
route that's not what we mean by the
 

895
00:18:26,850 --> 00:18:28,820
route that's not what we mean by the
shortest route when we program it and

896
00:18:28,820 --> 00:18:28,830
shortest route when we program it and
 

897
00:18:28,830 --> 00:18:31,100
shortest route when we program it and
that's just what and an intelligent

898
00:18:31,100 --> 00:18:31,110
that's just what and an intelligent
 

899
00:18:31,110 --> 00:18:34,400
that's just what and an intelligent
system is by definition it takes into

900
00:18:34,400 --> 00:18:34,410
system is by definition it takes into
 

901
00:18:34,410 --> 00:18:36,590
system is by definition it takes into
account multiple constraints the same is

902
00:18:36,590 --> 00:18:36,600
account multiple constraints the same is
 

903
00:18:36,600 --> 00:18:38,810
account multiple constraints the same is
true in fact even more true of so-called

904
00:18:38,810 --> 00:18:38,820
true in fact even more true of so-called
 

905
00:18:38,820 --> 00:18:41,780
true in fact even more true of so-called
general intelligence that is if it's

906
00:18:41,780 --> 00:18:41,790
general intelligence that is if it's
 

907
00:18:41,790 --> 00:18:44,420
general intelligence that is if it's
genuinely intelligent it's not going to

908
00:18:44,420 --> 00:18:44,430
genuinely intelligent it's not going to
 

909
00:18:44,430 --> 00:18:46,660
genuinely intelligent it's not going to
pursue some goal single-mindedly

910
00:18:46,660 --> 00:18:46,670
pursue some goal single-mindedly
 

911
00:18:46,670 --> 00:18:50,810
pursue some goal single-mindedly
omitting every other consideration and

912
00:18:50,810 --> 00:18:50,820
omitting every other consideration and
 

913
00:18:50,820 --> 00:18:53,330
omitting every other consideration and
collateral effect that's not artificial

914
00:18:53,330 --> 00:18:53,340
collateral effect that's not artificial
 

915
00:18:53,340 --> 00:18:54,980
collateral effect that's not artificial
in general intelligence that's that's

916
00:18:54,980 --> 00:18:54,990
in general intelligence that's that's
 

917
00:18:54,990 --> 00:18:58,280
in general intelligence that's that's
artificial stupidity I agree with you by

918
00:18:58,280 --> 00:18:58,290
artificial stupidity I agree with you by
 

919
00:18:58,290 --> 00:19:00,440
artificial stupidity I agree with you by
the way on the promise of autonomous

920
00:19:00,440 --> 00:19:00,450
the way on the promise of autonomous
 

921
00:19:00,450 --> 00:19:02,150
the way on the promise of autonomous
vehicles for improving human welfare

922
00:19:02,150 --> 00:19:02,160
vehicles for improving human welfare
 

923
00:19:02,160 --> 00:19:03,860
vehicles for improving human welfare
I think it's spectacular and I'm

924
00:19:03,860 --> 00:19:03,870
I think it's spectacular and I'm
 

925
00:19:03,870 --> 00:19:05,690
I think it's spectacular and I'm
surprised at how little press coverage

926
00:19:05,690 --> 00:19:05,700
surprised at how little press coverage
 

927
00:19:05,700 --> 00:19:07,760
surprised at how little press coverage
notes that in the United States alone

928
00:19:07,760 --> 00:19:07,770
notes that in the United States alone
 

929
00:19:07,770 --> 00:19:10,190
notes that in the United States alone
something like 40,000 people die every

930
00:19:10,190 --> 00:19:10,200
something like 40,000 people die every
 

931
00:19:10,200 --> 00:19:12,650
something like 40,000 people die every
year on the highways vastly more than

932
00:19:12,650 --> 00:19:12,660
year on the highways vastly more than
 

933
00:19:12,660 --> 00:19:15,500
year on the highways vastly more than
are killed by terrorists and we spend we

934
00:19:15,500 --> 00:19:15,510
are killed by terrorists and we spend we
 

935
00:19:15,510 --> 00:19:17,300
are killed by terrorists and we spend we
spent a trillion dollars on a war to

936
00:19:17,300 --> 00:19:17,310
spent a trillion dollars on a war to
 

937
00:19:17,310 --> 00:19:20,060
spent a trillion dollars on a war to
combat deaths by terrorism but half a

938
00:19:20,060 --> 00:19:20,070
combat deaths by terrorism but half a
 

939
00:19:20,070 --> 00:19:22,640
combat deaths by terrorism but half a
dozen a year whereas if you're an year

940
00:19:22,640 --> 00:19:22,650
dozen a year whereas if you're an year
 

941
00:19:22,650 --> 00:19:24,770
dozen a year whereas if you're an year
out 40,000 people are massacred on the

942
00:19:24,770 --> 00:19:24,780
out 40,000 people are massacred on the
 

943
00:19:24,780 --> 00:19:26,450
out 40,000 people are massacred on the
highways which could be brought down to

944
00:19:26,450 --> 00:19:26,460
highways which could be brought down to
 

945
00:19:26,460 --> 00:19:29,720
highways which could be brought down to
very close to zero so I'm with you on

946
00:19:29,720 --> 00:19:29,730
very close to zero so I'm with you on
 

947
00:19:29,730 --> 00:19:32,480
very close to zero so I'm with you on
the humanitarian benefit let me just

948
00:19:32,480 --> 00:19:32,490
the humanitarian benefit let me just
 

949
00:19:32,490 --> 00:19:34,010
the humanitarian benefit let me just
mention that it's as a person who's

950
00:19:34,010 --> 00:19:34,020
mention that it's as a person who's
 

951
00:19:34,020 --> 00:19:35,390
mention that it's as a person who's
building these cars it is it a little

952
00:19:35,390 --> 00:19:35,400
building these cars it is it a little
 

953
00:19:35,400 --> 00:19:36,920
building these cars it is it a little
bit offensive to me to say that

954
00:19:36,920 --> 00:19:36,930
bit offensive to me to say that
 

955
00:19:36,930 --> 00:19:38,810
bit offensive to me to say that
engineers would be clueless enough not

956
00:19:38,810 --> 00:19:38,820
engineers would be clueless enough not
 

957
00:19:38,820 --> 00:19:41,390
engineers would be clueless enough not
to engineer safety into systems I often

958
00:19:41,390 --> 00:19:41,400
to engineer safety into systems I often
 

959
00:19:41,400 --> 00:19:43,460
to engineer safety into systems I often
stay up at night thinking about those

960
00:19:43,460 --> 00:19:43,470
stay up at night thinking about those
 

961
00:19:43,470 --> 00:19:45,170
stay up at night thinking about those
40,000 people that are dying and

962
00:19:45,170 --> 00:19:45,180
40,000 people that are dying and
 

963
00:19:45,180 --> 00:19:48,020
40,000 people that are dying and
everything I tried to engineer is to

964
00:19:48,020 --> 00:19:48,030
everything I tried to engineer is to
 

965
00:19:48,030 --> 00:19:50,180
everything I tried to engineer is to
save those people's lives so every new

966
00:19:50,180 --> 00:19:50,190
save those people's lives so every new
 

967
00:19:50,190 --> 00:19:51,379
save those people's lives so every new
invention that I'm super

968
00:19:51,379 --> 00:19:51,389
invention that I'm super
 

969
00:19:51,389 --> 00:19:55,579
invention that I'm super
excited about every new and the in all

970
00:19:55,579 --> 00:19:55,589
excited about every new and the in all
 

971
00:19:55,589 --> 00:19:57,109
excited about every new and the in all
the deep learning literature and cvpr

972
00:19:57,109 --> 00:19:57,119
the deep learning literature and cvpr
 

973
00:19:57,119 --> 00:19:59,479
the deep learning literature and cvpr
conferences and nips everything I'm

974
00:19:59,479 --> 00:19:59,489
conferences and nips everything I'm
 

975
00:19:59,489 --> 00:20:03,079
conferences and nips everything I'm
super excited about is all grounded in

976
00:20:03,079 --> 00:20:03,089
super excited about is all grounded in
 

977
00:20:03,089 --> 00:20:07,339
super excited about is all grounded in
making it safe and help people so I just

978
00:20:07,339 --> 00:20:07,349
making it safe and help people so I just
 

979
00:20:07,349 --> 00:20:09,379
making it safe and help people so I just
don't see how that trajectory can all a

980
00:20:09,379 --> 00:20:09,389
don't see how that trajectory can all a
 

981
00:20:09,389 --> 00:20:11,449
don't see how that trajectory can all a
sudden slip into a situation where

982
00:20:11,449 --> 00:20:11,459
sudden slip into a situation where
 

983
00:20:11,459 --> 00:20:14,569
sudden slip into a situation where
intelligence will be highly negative you

984
00:20:14,569 --> 00:20:14,579
intelligence will be highly negative you
 

985
00:20:14,579 --> 00:20:16,039
intelligence will be highly negative you
know you and I certainly agree on that

986
00:20:16,039 --> 00:20:16,049
know you and I certainly agree on that
 

987
00:20:16,049 --> 00:20:17,449
know you and I certainly agree on that
and I think that's only the beginning of

988
00:20:17,449 --> 00:20:17,459
and I think that's only the beginning of
 

989
00:20:17,459 --> 00:20:19,879
and I think that's only the beginning of
the potential humanitarian benefits of

990
00:20:19,879 --> 00:20:19,889
the potential humanitarian benefits of
 

991
00:20:19,889 --> 00:20:22,159
the potential humanitarian benefits of
artificial intelligence there's been

992
00:20:22,159 --> 00:20:22,169
artificial intelligence there's been
 

993
00:20:22,169 --> 00:20:24,859
artificial intelligence there's been
enormous attention to what are we going

994
00:20:24,859 --> 00:20:24,869
enormous attention to what are we going
 

995
00:20:24,869 --> 00:20:26,299
enormous attention to what are we going
to do with the people whose jobs are

996
00:20:26,299 --> 00:20:26,309
to do with the people whose jobs are
 

997
00:20:26,309 --> 00:20:28,279
to do with the people whose jobs are
made obsolete by artificial intelligence

998
00:20:28,279 --> 00:20:28,289
made obsolete by artificial intelligence
 

999
00:20:28,289 --> 00:20:30,709
made obsolete by artificial intelligence
but very little attention given to the

1000
00:20:30,709 --> 00:20:30,719
but very little attention given to the
 

1001
00:20:30,719 --> 00:20:31,849
but very little attention given to the
fact that the jobs that hooni made

1002
00:20:31,849 --> 00:20:31,859
fact that the jobs that hooni made
 

1003
00:20:31,859 --> 00:20:34,369
fact that the jobs that hooni made
obsolete are horrible jobs the fact that

1004
00:20:34,369 --> 00:20:34,379
obsolete are horrible jobs the fact that
 

1005
00:20:34,379 --> 00:20:36,469
obsolete are horrible jobs the fact that
people aren't going to be picking crops

1006
00:20:36,469 --> 00:20:36,479
people aren't going to be picking crops
 

1007
00:20:36,479 --> 00:20:39,619
people aren't going to be picking crops
and making beds and driving trucks and

1008
00:20:39,619 --> 00:20:39,629
and making beds and driving trucks and
 

1009
00:20:39,629 --> 00:20:42,259
and making beds and driving trucks and
mining coal these are you know soul

1010
00:20:42,259 --> 00:20:42,269
mining coal these are you know soul
 

1011
00:20:42,269 --> 00:20:43,699
mining coal these are you know soul
deadening jobs and we have a whole

1012
00:20:43,699 --> 00:20:43,709
deadening jobs and we have a whole
 

1013
00:20:43,709 --> 00:20:46,159
deadening jobs and we have a whole
literature sympathizing with the people

1014
00:20:46,159 --> 00:20:46,169
literature sympathizing with the people
 

1015
00:20:46,169 --> 00:20:49,839
literature sympathizing with the people
stuck in these menial mind deadening

1016
00:20:49,839 --> 00:20:49,849
stuck in these menial mind deadening
 

1017
00:20:49,849 --> 00:20:53,239
stuck in these menial mind deadening
dangerous jobs if we can eliminate them

1018
00:20:53,239 --> 00:20:53,249
dangerous jobs if we can eliminate them
 

1019
00:20:53,249 --> 00:20:55,849
dangerous jobs if we can eliminate them
this is a fantastic boon to humanity now

1020
00:20:55,849 --> 00:20:55,859
this is a fantastic boon to humanity now
 

1021
00:20:55,859 --> 00:20:58,039
this is a fantastic boon to humanity now
granted we you solve one problem and

1022
00:20:58,039 --> 00:20:58,049
granted we you solve one problem and
 

1023
00:20:58,049 --> 00:21:00,499
granted we you solve one problem and
there's another one namely how do we get

1024
00:21:00,499 --> 00:21:00,509
there's another one namely how do we get
 

1025
00:21:00,509 --> 00:21:03,379
there's another one namely how do we get
these people a a decent income but if

1026
00:21:03,379 --> 00:21:03,389
these people a a decent income but if
 

1027
00:21:03,389 --> 00:21:05,449
these people a a decent income but if
we're smart enough to invent machines

1028
00:21:05,449 --> 00:21:05,459
we're smart enough to invent machines
 

1029
00:21:05,459 --> 00:21:08,209
we're smart enough to invent machines
that can make beds and put away dishes

1030
00:21:08,209 --> 00:21:08,219
that can make beds and put away dishes
 

1031
00:21:08,219 --> 00:21:11,389
that can make beds and put away dishes
and and handle hospital patients well I

1032
00:21:11,389 --> 00:21:11,399
and and handle hospital patients well I
 

1033
00:21:11,399 --> 00:21:12,829
and and handle hospital patients well I
think we're smart enough to figure out

1034
00:21:12,829 --> 00:21:12,839
think we're smart enough to figure out
 

1035
00:21:12,839 --> 00:21:15,199
think we're smart enough to figure out
how to redistribute income to apportion

1036
00:21:15,199 --> 00:21:15,209
how to redistribute income to apportion
 

1037
00:21:15,209 --> 00:21:18,829
how to redistribute income to apportion
some of the vast economic savings to the

1038
00:21:18,829 --> 00:21:18,839
some of the vast economic savings to the
 

1039
00:21:18,839 --> 00:21:20,449
some of the vast economic savings to the
human beings who will no longer be

1040
00:21:20,449 --> 00:21:20,459
human beings who will no longer be
 

1041
00:21:20,459 --> 00:21:23,810
human beings who will no longer be
needed to to make beds okay Sam Harris

1042
00:21:23,810 --> 00:21:23,820
needed to to make beds okay Sam Harris
 

1043
00:21:23,820 --> 00:21:26,329
needed to to make beds okay Sam Harris
says that it's obvious that eventually

1044
00:21:26,329 --> 00:21:26,339
says that it's obvious that eventually
 

1045
00:21:26,339 --> 00:21:29,779
says that it's obvious that eventually
AI will be in existential risk he's one

1046
00:21:29,779 --> 00:21:29,789
AI will be in existential risk he's one
 

1047
00:21:29,789 --> 00:21:32,299
AI will be in existential risk he's one
of the people says it's obvious we don't

1048
00:21:32,299 --> 00:21:32,309
of the people says it's obvious we don't
 

1049
00:21:32,309 --> 00:21:36,229
of the people says it's obvious we don't
know when the claim goes but eventually

1050
00:21:36,229 --> 00:21:36,239
know when the claim goes but eventually
 

1051
00:21:36,239 --> 00:21:38,479
know when the claim goes but eventually
it's obvious and because we don't know

1052
00:21:38,479 --> 00:21:38,489
it's obvious and because we don't know
 

1053
00:21:38,489 --> 00:21:40,639
it's obvious and because we don't know
when we should worry about it now this

1054
00:21:40,639 --> 00:21:40,649
when we should worry about it now this
 

1055
00:21:40,649 --> 00:21:42,919
when we should worry about it now this
is a very interesting argument in my

1056
00:21:42,919 --> 00:21:42,929
is a very interesting argument in my
 

1057
00:21:42,929 --> 00:21:46,389
is a very interesting argument in my
eyes so how do you how do we think about

1058
00:21:46,389 --> 00:21:46,399
eyes so how do you how do we think about
 

1059
00:21:46,399 --> 00:21:48,440
eyes so how do you how do we think about
time scale how do we think about

1060
00:21:48,440 --> 00:21:48,450
time scale how do we think about
 

1061
00:21:48,450 --> 00:21:50,509
time scale how do we think about
existential threats when we don't really

1062
00:21:50,509 --> 00:21:50,519
existential threats when we don't really
 

1063
00:21:50,519 --> 00:21:53,539
existential threats when we don't really
know so little about the threat unlike

1064
00:21:53,539 --> 00:21:53,549
know so little about the threat unlike
 

1065
00:21:53,549 --> 00:21:55,609
know so little about the threat unlike
nuclear weapons perhaps about this

1066
00:21:55,609 --> 00:21:55,619
nuclear weapons perhaps about this
 

1067
00:21:55,619 --> 00:21:58,819
nuclear weapons perhaps about this
particular threat that it could happen

1068
00:21:58,819 --> 00:21:58,829
particular threat that it could happen
 

1069
00:21:58,829 --> 00:21:59,479
particular threat that it could happen
tomorrow

1070
00:21:59,479 --> 00:21:59,489
tomorrow
 

1071
00:21:59,489 --> 00:22:03,049
tomorrow
right so but very likely won't yeah

1072
00:22:03,049 --> 00:22:03,059
right so but very likely won't yeah
 

1073
00:22:03,059 --> 00:22:04,339
right so but very likely won't yeah
they're likely to be a hundred years

1074
00:22:04,339 --> 00:22:04,349
they're likely to be a hundred years
 

1075
00:22:04,349 --> 00:22:05,360
they're likely to be a hundred years
away so how do

1076
00:22:05,360 --> 00:22:05,370
away so how do
 

1077
00:22:05,370 --> 00:22:07,940
away so how do
do we ignore it do how do we talk about

1078
00:22:07,940 --> 00:22:07,950
do we ignore it do how do we talk about
 

1079
00:22:07,950 --> 00:22:10,850
do we ignore it do how do we talk about
it do we worry about it what how do we

1080
00:22:10,850 --> 00:22:10,860
it do we worry about it what how do we
 

1081
00:22:10,860 --> 00:22:14,480
it do we worry about it what how do we
think about those what is it a threat

1082
00:22:14,480 --> 00:22:14,490
think about those what is it a threat
 

1083
00:22:14,490 --> 00:22:17,090
think about those what is it a threat
that we can imagine it's within the

1084
00:22:17,090 --> 00:22:17,100
that we can imagine it's within the
 

1085
00:22:17,100 --> 00:22:19,490
that we can imagine it's within the
limits of our imagination but not within

1086
00:22:19,490 --> 00:22:19,500
limits of our imagination but not within
 

1087
00:22:19,500 --> 00:22:22,280
limits of our imagination but not within
our limits of understanding - sufficient

1088
00:22:22,280 --> 00:22:22,290
our limits of understanding - sufficient
 

1089
00:22:22,290 --> 00:22:24,770
our limits of understanding - sufficient
to accurately predict it but but what

1090
00:22:24,770 --> 00:22:24,780
to accurately predict it but but what
 

1091
00:22:24,780 --> 00:22:29,030
to accurately predict it but but what
what is what is the ether asre AI xai

1092
00:22:29,030 --> 00:22:29,040
what is what is the ether asre AI xai
 

1093
00:22:29,040 --> 00:22:30,950
what is what is the ether asre AI xai
being the existential threat AI can

1094
00:22:30,950 --> 00:22:30,960
being the existential threat AI can
 

1095
00:22:30,960 --> 00:22:33,620
being the existential threat AI can
always know like enslaving us or turning

1096
00:22:33,620 --> 00:22:33,630
always know like enslaving us or turning
 

1097
00:22:33,630 --> 00:22:35,930
always know like enslaving us or turning
us into paperclips I think the most

1098
00:22:35,930 --> 00:22:35,940
us into paperclips I think the most
 

1099
00:22:35,940 --> 00:22:37,370
us into paperclips I think the most
compelling from the Sam Harris was fact

1100
00:22:37,370 --> 00:22:37,380
compelling from the Sam Harris was fact
 

1101
00:22:37,380 --> 00:22:39,770
compelling from the Sam Harris was fact
it would be the paperclip situation yeah

1102
00:22:39,770 --> 00:22:39,780
it would be the paperclip situation yeah
 

1103
00:22:39,780 --> 00:22:41,240
it would be the paperclip situation yeah
I mean I just think it's totally

1104
00:22:41,240 --> 00:22:41,250
I mean I just think it's totally
 

1105
00:22:41,250 --> 00:22:43,070
I mean I just think it's totally
fanciful I just don't build a system

1106
00:22:43,070 --> 00:22:43,080
fanciful I just don't build a system
 

1107
00:22:43,080 --> 00:22:47,690
fanciful I just don't build a system
don't give it a don't first of all the

1108
00:22:47,690 --> 00:22:47,700
don't give it a don't first of all the
 

1109
00:22:47,700 --> 00:22:49,130
don't give it a don't first of all the
code of engineering is you don't

1110
00:22:49,130 --> 00:22:49,140
code of engineering is you don't
 

1111
00:22:49,140 --> 00:22:50,840
code of engineering is you don't
implement a system with massive control

1112
00:22:50,840 --> 00:22:50,850
implement a system with massive control
 

1113
00:22:50,850 --> 00:22:53,270
implement a system with massive control
before testing it now perhaps the

1114
00:22:53,270 --> 00:22:53,280
before testing it now perhaps the
 

1115
00:22:53,280 --> 00:22:54,440
before testing it now perhaps the
culture of engineering will radically

1116
00:22:54,440 --> 00:22:54,450
culture of engineering will radically
 

1117
00:22:54,450 --> 00:22:56,750
culture of engineering will radically
change then I would worry I don't see

1118
00:22:56,750 --> 00:22:56,760
change then I would worry I don't see
 

1119
00:22:56,760 --> 00:22:58,430
change then I would worry I don't see
any signs that engineers will suddenly

1120
00:22:58,430 --> 00:22:58,440
any signs that engineers will suddenly
 

1121
00:22:58,440 --> 00:23:03,020
any signs that engineers will suddenly
do idiotic things like put a electrical

1122
00:23:03,020 --> 00:23:03,030
do idiotic things like put a electrical
 

1123
00:23:03,030 --> 00:23:04,610
do idiotic things like put a electrical
power plant in control of a system that

1124
00:23:04,610 --> 00:23:04,620
power plant in control of a system that
 

1125
00:23:04,620 --> 00:23:08,150
power plant in control of a system that
they haven't tested first or all of

1126
00:23:08,150 --> 00:23:08,160
they haven't tested first or all of
 

1127
00:23:08,160 --> 00:23:11,800
they haven't tested first or all of
these scenarios not only imagine a

1128
00:23:11,800 --> 00:23:11,810
these scenarios not only imagine a
 

1129
00:23:11,810 --> 00:23:14,750
these scenarios not only imagine a
almost a magically powered intelligence

1130
00:23:14,750 --> 00:23:14,760
almost a magically powered intelligence
 

1131
00:23:14,760 --> 00:23:17,780
almost a magically powered intelligence
you know including things like cure

1132
00:23:17,780 --> 00:23:17,790
you know including things like cure
 

1133
00:23:17,790 --> 00:23:19,370
you know including things like cure
cancer which is probably an incoherent

1134
00:23:19,370 --> 00:23:19,380
cancer which is probably an incoherent
 

1135
00:23:19,380 --> 00:23:20,840
cancer which is probably an incoherent
goal because there's so many different

1136
00:23:20,840 --> 00:23:20,850
goal because there's so many different
 

1137
00:23:20,850 --> 00:23:23,540
goal because there's so many different
kinds of cancer or bring about world

1138
00:23:23,540 --> 00:23:23,550
kinds of cancer or bring about world
 

1139
00:23:23,550 --> 00:23:25,490
kinds of cancer or bring about world
peace I mean how do you even specify

1140
00:23:25,490 --> 00:23:25,500
peace I mean how do you even specify
 

1141
00:23:25,500 --> 00:23:27,590
peace I mean how do you even specify
that as a goal but the scenarios also

1142
00:23:27,590 --> 00:23:27,600
that as a goal but the scenarios also
 

1143
00:23:27,600 --> 00:23:30,500
that as a goal but the scenarios also
imagine some degree of control of every

1144
00:23:30,500 --> 00:23:30,510
imagine some degree of control of every
 

1145
00:23:30,510 --> 00:23:33,440
imagine some degree of control of every
molecule in the universe which not only

1146
00:23:33,440 --> 00:23:33,450
molecule in the universe which not only
 

1147
00:23:33,450 --> 00:23:36,200
molecule in the universe which not only
is itself unlikely but we would not

1148
00:23:36,200 --> 00:23:36,210
is itself unlikely but we would not
 

1149
00:23:36,210 --> 00:23:39,010
is itself unlikely but we would not
start to connect these systems to

1150
00:23:39,010 --> 00:23:39,020
start to connect these systems to
 

1151
00:23:39,020 --> 00:23:42,590
start to connect these systems to
infrastructure without without testing

1152
00:23:42,590 --> 00:23:42,600
infrastructure without without testing
 

1153
00:23:42,600 --> 00:23:45,230
infrastructure without without testing
as we would any kind of engineering

1154
00:23:45,230 --> 00:23:45,240
as we would any kind of engineering
 

1155
00:23:45,240 --> 00:23:46,970
as we would any kind of engineering
system now maybe some engineers will be

1156
00:23:46,970 --> 00:23:46,980
system now maybe some engineers will be
 

1157
00:23:46,980 --> 00:23:50,830
system now maybe some engineers will be
irresponsible and we need legal and

1158
00:23:50,830 --> 00:23:50,840
irresponsible and we need legal and
 

1159
00:23:50,840 --> 00:23:54,890
irresponsible and we need legal and
regulatory and legal responsibilities

1160
00:23:54,890 --> 00:23:54,900
regulatory and legal responsibilities
 

1161
00:23:54,900 --> 00:23:57,050
regulatory and legal responsibilities
implemented so that engineers don't do

1162
00:23:57,050 --> 00:23:57,060
implemented so that engineers don't do
 

1163
00:23:57,060 --> 00:23:58,910
implemented so that engineers don't do
things that are stupid by their own

1164
00:23:58,910 --> 00:23:58,920
things that are stupid by their own
 

1165
00:23:58,920 --> 00:24:02,960
things that are stupid by their own
standards but the ii-i've never seen

1166
00:24:02,960 --> 00:24:02,970
standards but the ii-i've never seen
 

1167
00:24:02,970 --> 00:24:06,070
standards but the ii-i've never seen
enough of a plausible scenario of

1168
00:24:06,070 --> 00:24:06,080
enough of a plausible scenario of
 

1169
00:24:06,080 --> 00:24:08,570
enough of a plausible scenario of
existential threat to devote large

1170
00:24:08,570 --> 00:24:08,580
existential threat to devote large
 

1171
00:24:08,580 --> 00:24:10,910
existential threat to devote large
amounts of brain power to to forestall

1172
00:24:10,910 --> 00:24:10,920
amounts of brain power to to forestall
 

1173
00:24:10,920 --> 00:24:13,820
amounts of brain power to to forestall
it so you believe in the sort of the

1174
00:24:13,820 --> 00:24:13,830
it so you believe in the sort of the
 

1175
00:24:13,830 --> 00:24:16,070
it so you believe in the sort of the
power and mass of the engineering of

1176
00:24:16,070 --> 00:24:16,080
power and mass of the engineering of
 

1177
00:24:16,080 --> 00:24:17,779
power and mass of the engineering of
reason as the argue

1178
00:24:17,779 --> 00:24:17,789
reason as the argue
 

1179
00:24:17,789 --> 00:24:19,690
reason as the argue
this book of Reason science and sort of

1180
00:24:19,690 --> 00:24:19,700
this book of Reason science and sort of
 

1181
00:24:19,700 --> 00:24:23,570
this book of Reason science and sort of
be the very thing that puts the

1182
00:24:23,570 --> 00:24:23,580
be the very thing that puts the
 

1183
00:24:23,580 --> 00:24:25,099
be the very thing that puts the
development of new technology so it's

1184
00:24:25,099 --> 00:24:25,109
development of new technology so it's
 

1185
00:24:25,109 --> 00:24:27,109
development of new technology so it's
safe and also keeps us safe it's the

1186
00:24:27,109 --> 00:24:27,119
safe and also keeps us safe it's the
 

1187
00:24:27,119 --> 00:24:29,029
safe and also keeps us safe it's the
same and you know granted the same

1188
00:24:29,029 --> 00:24:29,039
same and you know granted the same
 

1189
00:24:29,039 --> 00:24:32,180
same and you know granted the same
culture of safety that currently is part

1190
00:24:32,180 --> 00:24:32,190
culture of safety that currently is part
 

1191
00:24:32,190 --> 00:24:35,060
culture of safety that currently is part
of the engineering mindset for airplanes

1192
00:24:35,060 --> 00:24:35,070
of the engineering mindset for airplanes
 

1193
00:24:35,070 --> 00:24:37,729
of the engineering mindset for airplanes
for example so yeah I don't think that

1194
00:24:37,729 --> 00:24:37,739
for example so yeah I don't think that
 

1195
00:24:37,739 --> 00:24:39,619
for example so yeah I don't think that
that that should be thrown out the

1196
00:24:39,619 --> 00:24:39,629
that that should be thrown out the
 

1197
00:24:39,629 --> 00:24:42,289
that that should be thrown out the
window and that untested all-powerful

1198
00:24:42,289 --> 00:24:42,299
window and that untested all-powerful
 

1199
00:24:42,299 --> 00:24:44,359
window and that untested all-powerful
system should be suddenly implemented

1200
00:24:44,359 --> 00:24:44,369
system should be suddenly implemented
 

1201
00:24:44,369 --> 00:24:46,219
system should be suddenly implemented
but there's no reason to think they are

1202
00:24:46,219 --> 00:24:46,229
but there's no reason to think they are
 

1203
00:24:46,229 --> 00:24:48,499
but there's no reason to think they are
and in fact if you look at the progress

1204
00:24:48,499 --> 00:24:48,509
and in fact if you look at the progress
 

1205
00:24:48,509 --> 00:24:50,599
and in fact if you look at the progress
of artificial intelligence it's been you

1206
00:24:50,599 --> 00:24:50,609
of artificial intelligence it's been you
 

1207
00:24:50,609 --> 00:24:51,859
of artificial intelligence it's been you
know it's been impressive especially in

1208
00:24:51,859 --> 00:24:51,869
know it's been impressive especially in
 

1209
00:24:51,869 --> 00:24:53,749
know it's been impressive especially in
the last ten years or so but the idea

1210
00:24:53,749 --> 00:24:53,759
the last ten years or so but the idea
 

1211
00:24:53,759 --> 00:24:54,859
the last ten years or so but the idea
that suddenly there'll be a step

1212
00:24:54,859 --> 00:24:54,869
that suddenly there'll be a step
 

1213
00:24:54,869 --> 00:24:57,499
that suddenly there'll be a step
function that all of a sudden before we

1214
00:24:57,499 --> 00:24:57,509
function that all of a sudden before we
 

1215
00:24:57,509 --> 00:25:00,379
function that all of a sudden before we
know it it will be all powerful that

1216
00:25:00,379 --> 00:25:00,389
know it it will be all powerful that
 

1217
00:25:00,389 --> 00:25:01,789
know it it will be all powerful that
there'll be some kind of recursive

1218
00:25:01,789 --> 00:25:01,799
there'll be some kind of recursive
 

1219
00:25:01,799 --> 00:25:05,089
there'll be some kind of recursive
self-improvement some kind of Foom is

1220
00:25:05,089 --> 00:25:05,099
self-improvement some kind of Foom is
 

1221
00:25:05,099 --> 00:25:08,749
self-improvement some kind of Foom is
also fanciful we certainly by the

1222
00:25:08,749 --> 00:25:08,759
also fanciful we certainly by the
 

1223
00:25:08,759 --> 00:25:11,419
also fanciful we certainly by the
technology that we that were now

1224
00:25:11,419 --> 00:25:11,429
technology that we that were now
 

1225
00:25:11,429 --> 00:25:13,489
technology that we that were now
impresses us such as deep learning when

1226
00:25:13,489 --> 00:25:13,499
impresses us such as deep learning when
 

1227
00:25:13,499 --> 00:25:16,190
impresses us such as deep learning when
you train something on hundreds of

1228
00:25:16,190 --> 00:25:16,200
you train something on hundreds of
 

1229
00:25:16,200 --> 00:25:17,619
you train something on hundreds of
thousands or millions of examples

1230
00:25:17,619 --> 00:25:17,629
thousands or millions of examples
 

1231
00:25:17,629 --> 00:25:20,139
thousands or millions of examples
they're not hundreds of thousands of

1232
00:25:20,139 --> 00:25:20,149
they're not hundreds of thousands of
 

1233
00:25:20,149 --> 00:25:23,869
they're not hundreds of thousands of
problems of which curing cancer is a

1234
00:25:23,869 --> 00:25:23,879
problems of which curing cancer is a
 

1235
00:25:23,879 --> 00:25:27,139
problems of which curing cancer is a
typical example and so the kind of

1236
00:25:27,139 --> 00:25:27,149
typical example and so the kind of
 

1237
00:25:27,149 --> 00:25:29,109
typical example and so the kind of
techniques that have allowed AI to

1238
00:25:29,109 --> 00:25:29,119
techniques that have allowed AI to
 

1239
00:25:29,119 --> 00:25:31,519
techniques that have allowed AI to
increase in the last five years are not

1240
00:25:31,519 --> 00:25:31,529
increase in the last five years are not
 

1241
00:25:31,529 --> 00:25:32,919
increase in the last five years are not
the claim that are going to lead to this

1242
00:25:32,919 --> 00:25:32,929
the claim that are going to lead to this
 

1243
00:25:32,929 --> 00:25:37,759
the claim that are going to lead to this
fantasy of of exponential sudden

1244
00:25:37,759 --> 00:25:37,769
fantasy of of exponential sudden
 

1245
00:25:37,769 --> 00:25:40,009
fantasy of of exponential sudden
self-improvement so it's may I think

1246
00:25:40,009 --> 00:25:40,019
self-improvement so it's may I think
 

1247
00:25:40,019 --> 00:25:41,539
self-improvement so it's may I think
it's it's kind of a magical thinking

1248
00:25:41,539 --> 00:25:41,549
it's it's kind of a magical thinking
 

1249
00:25:41,549 --> 00:25:43,940
it's it's kind of a magical thinking
it's not based on our understanding of

1250
00:25:43,940 --> 00:25:43,950
it's not based on our understanding of
 

1251
00:25:43,950 --> 00:25:46,460
it's not based on our understanding of
how AI actually works now give me a

1252
00:25:46,460 --> 00:25:46,470
how AI actually works now give me a
 

1253
00:25:46,470 --> 00:25:48,859
how AI actually works now give me a
chance here so you said fanciful magical

1254
00:25:48,859 --> 00:25:48,869
chance here so you said fanciful magical
 

1255
00:25:48,869 --> 00:25:51,950
chance here so you said fanciful magical
thinking in his TED talk Sam Harris says

1256
00:25:51,950 --> 00:25:51,960
thinking in his TED talk Sam Harris says
 

1257
00:25:51,960 --> 00:25:54,080
thinking in his TED talk Sam Harris says
that thinking about AI killing all human

1258
00:25:54,080 --> 00:25:54,090
that thinking about AI killing all human
 

1259
00:25:54,090 --> 00:25:55,639
that thinking about AI killing all human
civilization is somehow fun

1260
00:25:55,639 --> 00:25:55,649
civilization is somehow fun
 

1261
00:25:55,649 --> 00:25:58,489
civilization is somehow fun
intellectually now I have to say as a

1262
00:25:58,489 --> 00:25:58,499
intellectually now I have to say as a
 

1263
00:25:58,499 --> 00:26:00,519
intellectually now I have to say as a
scientist engineer I don't find it fun

1264
00:26:00,519 --> 00:26:00,529
scientist engineer I don't find it fun
 

1265
00:26:00,529 --> 00:26:03,560
scientist engineer I don't find it fun
but when I'm having beer with my non-ai

1266
00:26:03,560 --> 00:26:03,570
but when I'm having beer with my non-ai
 

1267
00:26:03,570 --> 00:26:07,609
but when I'm having beer with my non-ai
friends there is indeed something fun

1268
00:26:07,609 --> 00:26:07,619
friends there is indeed something fun
 

1269
00:26:07,619 --> 00:26:09,440
friends there is indeed something fun
and appealing about it like talking

1270
00:26:09,440 --> 00:26:09,450
and appealing about it like talking
 

1271
00:26:09,450 --> 00:26:10,669
and appealing about it like talking
about an episode of black mirror

1272
00:26:10,669 --> 00:26:10,679
about an episode of black mirror
 

1273
00:26:10,679 --> 00:26:14,479
about an episode of black mirror
considering if a large meteor is headed

1274
00:26:14,479 --> 00:26:14,489
considering if a large meteor is headed
 

1275
00:26:14,489 --> 00:26:16,519
considering if a large meteor is headed
towards Earth we were just told a large

1276
00:26:16,519 --> 00:26:16,529
towards Earth we were just told a large
 

1277
00:26:16,529 --> 00:26:18,589
towards Earth we were just told a large
meteors headed towards Earth something

1278
00:26:18,589 --> 00:26:18,599
meteors headed towards Earth something
 

1279
00:26:18,599 --> 00:26:19,369
meteors headed towards Earth something
like this

1280
00:26:19,369 --> 00:26:19,379
like this
 

1281
00:26:19,379 --> 00:26:22,099
like this
and can you relate to this sense of fun

1282
00:26:22,099 --> 00:26:22,109
and can you relate to this sense of fun
 

1283
00:26:22,109 --> 00:26:24,440
and can you relate to this sense of fun
and do you understand the psychology of

1284
00:26:24,440 --> 00:26:24,450
and do you understand the psychology of
 

1285
00:26:24,450 --> 00:26:26,419
and do you understand the psychology of
it yeah that's a good question

1286
00:26:26,419 --> 00:26:26,429
it yeah that's a good question
 

1287
00:26:26,429 --> 00:26:29,400
it yeah that's a good question
III personally don't find it fun

1288
00:26:29,400 --> 00:26:29,410
III personally don't find it fun
 

1289
00:26:29,410 --> 00:26:32,250
III personally don't find it fun
I find it kind of actually a waste of

1290
00:26:32,250 --> 00:26:32,260
I find it kind of actually a waste of
 

1291
00:26:32,260 --> 00:26:34,950
I find it kind of actually a waste of
time because there are genuine threats

1292
00:26:34,950 --> 00:26:34,960
time because there are genuine threats
 

1293
00:26:34,960 --> 00:26:37,470
time because there are genuine threats
that we ought to be thinking about like

1294
00:26:37,470 --> 00:26:37,480
that we ought to be thinking about like
 

1295
00:26:37,480 --> 00:26:39,720
that we ought to be thinking about like
like pandemics like like a cyber

1296
00:26:39,720 --> 00:26:39,730
like pandemics like like a cyber
 

1297
00:26:39,730 --> 00:26:43,410
like pandemics like like a cyber
security vulnerabilities like the

1298
00:26:43,410 --> 00:26:43,420
security vulnerabilities like the
 

1299
00:26:43,420 --> 00:26:45,540
security vulnerabilities like the
possibility of nuclear war and certainly

1300
00:26:45,540 --> 00:26:45,550
possibility of nuclear war and certainly
 

1301
00:26:45,550 --> 00:26:49,110
possibility of nuclear war and certainly
climate change this is enough to film it

1302
00:26:49,110 --> 00:26:49,120
climate change this is enough to film it
 

1303
00:26:49,120 --> 00:26:52,890
climate change this is enough to film it
many conversations without and I think

1304
00:26:52,890 --> 00:26:52,900
many conversations without and I think
 

1305
00:26:52,900 --> 00:26:54,690
many conversations without and I think
there I think Sam did put his finger on

1306
00:26:54,690 --> 00:26:54,700
there I think Sam did put his finger on
 

1307
00:26:54,700 --> 00:26:56,400
there I think Sam did put his finger on
something namely that there is a

1308
00:26:56,400 --> 00:26:56,410
something namely that there is a
 

1309
00:26:56,410 --> 00:26:59,250
something namely that there is a
community us sometimes called the

1310
00:26:59,250 --> 00:26:59,260
community us sometimes called the
 

1311
00:26:59,260 --> 00:27:02,430
community us sometimes called the
rationality community that delights in

1312
00:27:02,430 --> 00:27:02,440
rationality community that delights in
 

1313
00:27:02,440 --> 00:27:05,250
rationality community that delights in
using its brain power to come up with

1314
00:27:05,250 --> 00:27:05,260
using its brain power to come up with
 

1315
00:27:05,260 --> 00:27:08,160
using its brain power to come up with
scenarios that would not occur to mere

1316
00:27:08,160 --> 00:27:08,170
scenarios that would not occur to mere
 

1317
00:27:08,170 --> 00:27:12,240
scenarios that would not occur to mere
mortals to less cerebral people so there

1318
00:27:12,240 --> 00:27:12,250
mortals to less cerebral people so there
 

1319
00:27:12,250 --> 00:27:13,980
mortals to less cerebral people so there
is a kind of intellectual thrill in

1320
00:27:13,980 --> 00:27:13,990
is a kind of intellectual thrill in
 

1321
00:27:13,990 --> 00:27:15,720
is a kind of intellectual thrill in
finding new things to worry about that

1322
00:27:15,720 --> 00:27:15,730
finding new things to worry about that
 

1323
00:27:15,730 --> 00:27:17,640
finding new things to worry about that
no one has worried about yet

1324
00:27:17,640 --> 00:27:17,650
no one has worried about yet
 

1325
00:27:17,650 --> 00:27:20,100
no one has worried about yet
I actually think though that it's not

1326
00:27:20,100 --> 00:27:20,110
I actually think though that it's not
 

1327
00:27:20,110 --> 00:27:21,990
I actually think though that it's not
only is it is a kind of fun that doesn't

1328
00:27:21,990 --> 00:27:22,000
only is it is a kind of fun that doesn't
 

1329
00:27:22,000 --> 00:27:24,480
only is it is a kind of fun that doesn't
give me particular pleasure but I think

1330
00:27:24,480 --> 00:27:24,490
give me particular pleasure but I think
 

1331
00:27:24,490 --> 00:27:26,160
give me particular pleasure but I think
there is there can be a pernicious side

1332
00:27:26,160 --> 00:27:26,170
there is there can be a pernicious side
 

1333
00:27:26,170 --> 00:27:28,830
there is there can be a pernicious side
to it namely that you overcome people

1334
00:27:28,830 --> 00:27:28,840
to it namely that you overcome people
 

1335
00:27:28,840 --> 00:27:32,250
to it namely that you overcome people
with such dread such fatalism that

1336
00:27:32,250 --> 00:27:32,260
with such dread such fatalism that
 

1337
00:27:32,260 --> 00:27:35,190
with such dread such fatalism that
there's so many ways to die to

1338
00:27:35,190 --> 00:27:35,200
there's so many ways to die to
 

1339
00:27:35,200 --> 00:27:38,070
there's so many ways to die to
annihilate our civilization that we may

1340
00:27:38,070 --> 00:27:38,080
annihilate our civilization that we may
 

1341
00:27:38,080 --> 00:27:39,960
annihilate our civilization that we may
as well enjoy life while we can there's

1342
00:27:39,960 --> 00:27:39,970
as well enjoy life while we can there's
 

1343
00:27:39,970 --> 00:27:41,190
as well enjoy life while we can there's
nothing we can do about it if climate

1344
00:27:41,190 --> 00:27:41,200
nothing we can do about it if climate
 

1345
00:27:41,200 --> 00:27:42,930
nothing we can do about it if climate
change doesn't do us in then runaway

1346
00:27:42,930 --> 00:27:42,940
change doesn't do us in then runaway
 

1347
00:27:42,940 --> 00:27:46,410
change doesn't do us in then runaway
robots will so let's enjoy ourselves now

1348
00:27:46,410 --> 00:27:46,420
robots will so let's enjoy ourselves now
 

1349
00:27:46,420 --> 00:27:51,540
robots will so let's enjoy ourselves now
we've got to prioritize we have to look

1350
00:27:51,540 --> 00:27:51,550
we've got to prioritize we have to look
 

1351
00:27:51,550 --> 00:27:54,270
we've got to prioritize we have to look
at threats that are close to certainty

1352
00:27:54,270 --> 00:27:54,280
at threats that are close to certainty
 

1353
00:27:54,280 --> 00:27:56,640
at threats that are close to certainty
such as climate change and distinguish

1354
00:27:56,640 --> 00:27:56,650
such as climate change and distinguish
 

1355
00:27:56,650 --> 00:27:58,140
such as climate change and distinguish
those from ones that are merely

1356
00:27:58,140 --> 00:27:58,150
those from ones that are merely
 

1357
00:27:58,150 --> 00:28:00,180
those from ones that are merely
imaginable but with infinitesimal

1358
00:28:00,180 --> 00:28:00,190
imaginable but with infinitesimal
 

1359
00:28:00,190 --> 00:28:04,350
imaginable but with infinitesimal
probabilities and we have to take into

1360
00:28:04,350 --> 00:28:04,360
probabilities and we have to take into
 

1361
00:28:04,360 --> 00:28:06,240
probabilities and we have to take into
account people's worry budget you can't

1362
00:28:06,240 --> 00:28:06,250
account people's worry budget you can't
 

1363
00:28:06,250 --> 00:28:08,460
account people's worry budget you can't
worry about everything and if you so

1364
00:28:08,460 --> 00:28:08,470
worry about everything and if you so
 

1365
00:28:08,470 --> 00:28:11,730
worry about everything and if you so
dread and fear and terror and numb and

1366
00:28:11,730 --> 00:28:11,740
dread and fear and terror and numb and
 

1367
00:28:11,740 --> 00:28:13,740
dread and fear and terror and numb and
fatalism it can lead to a kind of

1368
00:28:13,740 --> 00:28:13,750
fatalism it can lead to a kind of
 

1369
00:28:13,750 --> 00:28:14,970
fatalism it can lead to a kind of
numbness well they're just these

1370
00:28:14,970 --> 00:28:14,980
numbness well they're just these
 

1371
00:28:14,980 --> 00:28:16,500
numbness well they're just these
problems are overwhelming and the

1372
00:28:16,500 --> 00:28:16,510
problems are overwhelming and the
 

1373
00:28:16,510 --> 00:28:19,400
problems are overwhelming and the
engineers are just gonna kill us all so

1374
00:28:19,400 --> 00:28:19,410
engineers are just gonna kill us all so
 

1375
00:28:19,410 --> 00:28:22,430
engineers are just gonna kill us all so
let's either destroy the entire

1376
00:28:22,430 --> 00:28:22,440
let's either destroy the entire
 

1377
00:28:22,440 --> 00:28:26,880
let's either destroy the entire
infrastructure of science technology or

1378
00:28:26,880 --> 00:28:26,890
infrastructure of science technology or
 

1379
00:28:26,890 --> 00:28:29,820
infrastructure of science technology or
let's just enjoy life while we can so

1380
00:28:29,820 --> 00:28:29,830
let's just enjoy life while we can so
 

1381
00:28:29,830 --> 00:28:31,860
let's just enjoy life while we can so
there's a certain line of worry which

1382
00:28:31,860 --> 00:28:31,870
there's a certain line of worry which
 

1383
00:28:31,870 --> 00:28:33,150
there's a certain line of worry which
I'm worried about a lot of things

1384
00:28:33,150 --> 00:28:33,160
I'm worried about a lot of things
 

1385
00:28:33,160 --> 00:28:34,380
I'm worried about a lot of things
engineering there's a certain line of

1386
00:28:34,380 --> 00:28:34,390
engineering there's a certain line of
 

1387
00:28:34,390 --> 00:28:38,070
engineering there's a certain line of
worry when you cross a lot across

1388
00:28:38,070 --> 00:28:38,080
worry when you cross a lot across
 

1389
00:28:38,080 --> 00:28:40,500
worry when you cross a lot across
that it becomes paralyzing fear as

1390
00:28:40,500 --> 00:28:40,510
that it becomes paralyzing fear as
 

1391
00:28:40,510 --> 00:28:42,840
that it becomes paralyzing fear as
opposed to productive fear and that's

1392
00:28:42,840 --> 00:28:42,850
opposed to productive fear and that's
 

1393
00:28:42,850 --> 00:28:45,720
opposed to productive fear and that's
kind of what they're highlighting there

1394
00:28:45,720 --> 00:28:45,730
kind of what they're highlighting there
 

1395
00:28:45,730 --> 00:28:48,210
kind of what they're highlighting there
exactly right and we've seen some we

1396
00:28:48,210 --> 00:28:48,220
exactly right and we've seen some we
 

1397
00:28:48,220 --> 00:28:50,460
exactly right and we've seen some we
know that human effort is not well

1398
00:28:50,460 --> 00:28:50,470
know that human effort is not well
 

1399
00:28:50,470 --> 00:28:54,060
know that human effort is not well
calibrated against risk in that because

1400
00:28:54,060 --> 00:28:54,070
calibrated against risk in that because
 

1401
00:28:54,070 --> 00:28:57,720
calibrated against risk in that because
a basic tenet of cognitive psychology is

1402
00:28:57,720 --> 00:28:57,730
a basic tenet of cognitive psychology is
 

1403
00:28:57,730 --> 00:29:00,899
a basic tenet of cognitive psychology is
that perception of risk and hence

1404
00:29:00,899 --> 00:29:00,909
that perception of risk and hence
 

1405
00:29:00,909 --> 00:29:03,240
that perception of risk and hence
perception of fear is driven by imagined

1406
00:29:03,240 --> 00:29:03,250
perception of fear is driven by imagined
 

1407
00:29:03,250 --> 00:29:07,350
perception of fear is driven by imagined
ability not by data and so we miss

1408
00:29:07,350 --> 00:29:07,360
ability not by data and so we miss
 

1409
00:29:07,360 --> 00:29:09,840
ability not by data and so we miss
allocate vast amounts of resources to

1410
00:29:09,840 --> 00:29:09,850
allocate vast amounts of resources to
 

1411
00:29:09,850 --> 00:29:11,909
allocate vast amounts of resources to
avoiding terrorism which kills on

1412
00:29:11,909 --> 00:29:11,919
avoiding terrorism which kills on
 

1413
00:29:11,919 --> 00:29:13,889
avoiding terrorism which kills on
average about six Americans a year with

1414
00:29:13,889 --> 00:29:13,899
average about six Americans a year with
 

1415
00:29:13,899 --> 00:29:16,230
average about six Americans a year with
a one exception of 9/11 we invade

1416
00:29:16,230 --> 00:29:16,240
a one exception of 9/11 we invade
 

1417
00:29:16,240 --> 00:29:19,070
a one exception of 9/11 we invade
countries we invent entire new

1418
00:29:19,070 --> 00:29:19,080
countries we invent entire new
 

1419
00:29:19,080 --> 00:29:21,990
countries we invent entire new
departments of government with massive

1420
00:29:21,990 --> 00:29:22,000
departments of government with massive
 

1421
00:29:22,000 --> 00:29:24,659
departments of government with massive
massive expenditure of resources and

1422
00:29:24,659 --> 00:29:24,669
massive expenditure of resources and
 

1423
00:29:24,669 --> 00:29:26,430
massive expenditure of resources and
lives to defend ourselves against a

1424
00:29:26,430 --> 00:29:26,440
lives to defend ourselves against a
 

1425
00:29:26,440 --> 00:29:30,659
lives to defend ourselves against a
trivial risk whereas guaranteed risks

1426
00:29:30,659 --> 00:29:30,669
trivial risk whereas guaranteed risks
 

1427
00:29:30,669 --> 00:29:32,370
trivial risk whereas guaranteed risks
and you mentioned as one of them you

1428
00:29:32,370 --> 00:29:32,380
and you mentioned as one of them you
 

1429
00:29:32,380 --> 00:29:35,700
and you mentioned as one of them you
mentioned traffic fatalities and even

1430
00:29:35,700 --> 00:29:35,710
mentioned traffic fatalities and even
 

1431
00:29:35,710 --> 00:29:41,029
mentioned traffic fatalities and even
risks that are not here but are

1432
00:29:41,029 --> 00:29:41,039
risks that are not here but are
 

1433
00:29:41,039 --> 00:29:42,899
risks that are not here but are
plausible enough to worry about

1434
00:29:42,899 --> 00:29:42,909
plausible enough to worry about
 

1435
00:29:42,909 --> 00:29:47,549
plausible enough to worry about
like pandemics like nuclear war receive

1436
00:29:47,549 --> 00:29:47,559
like pandemics like nuclear war receive
 

1437
00:29:47,559 --> 00:29:49,200
like pandemics like nuclear war receive
far too little attention the in

1438
00:29:49,200 --> 00:29:49,210
far too little attention the in
 

1439
00:29:49,210 --> 00:29:50,610
far too little attention the in
presidential debates there's no

1440
00:29:50,610 --> 00:29:50,620
presidential debates there's no
 

1441
00:29:50,620 --> 00:29:53,009
presidential debates there's no
discussion of how to minimize the risk

1442
00:29:53,009 --> 00:29:53,019
discussion of how to minimize the risk
 

1443
00:29:53,019 --> 00:29:55,230
discussion of how to minimize the risk
of nuclear war lots of discussion of

1444
00:29:55,230 --> 00:29:55,240
of nuclear war lots of discussion of
 

1445
00:29:55,240 --> 00:29:58,950
of nuclear war lots of discussion of
terrorism for example and and so we I

1446
00:29:58,950 --> 00:29:58,960
terrorism for example and and so we I
 

1447
00:29:58,960 --> 00:30:01,430
terrorism for example and and so we I
think it's essential to calibrate our

1448
00:30:01,430 --> 00:30:01,440
think it's essential to calibrate our
 

1449
00:30:01,440 --> 00:30:05,789
think it's essential to calibrate our
budget of fear worry concern planning to

1450
00:30:05,789 --> 00:30:05,799
budget of fear worry concern planning to
 

1451
00:30:05,799 --> 00:30:10,769
budget of fear worry concern planning to
the actual probability of harm yep so

1452
00:30:10,769 --> 00:30:10,779
the actual probability of harm yep so
 

1453
00:30:10,779 --> 00:30:12,960
the actual probability of harm yep so
let me ask this then this question

1454
00:30:12,960 --> 00:30:12,970
let me ask this then this question
 

1455
00:30:12,970 --> 00:30:16,200
let me ask this then this question
so speaking of imagined ability you said

1456
00:30:16,200 --> 00:30:16,210
so speaking of imagined ability you said
 

1457
00:30:16,210 --> 00:30:18,000
so speaking of imagined ability you said
it's important to think about reason and

1458
00:30:18,000 --> 00:30:18,010
it's important to think about reason and
 

1459
00:30:18,010 --> 00:30:21,120
it's important to think about reason and
one of my favorite people who who likes

1460
00:30:21,120 --> 00:30:21,130
one of my favorite people who who likes
 

1461
00:30:21,130 --> 00:30:23,360
one of my favorite people who who likes
to dip into the outskirts of reason

1462
00:30:23,360 --> 00:30:23,370
to dip into the outskirts of reason
 

1463
00:30:23,370 --> 00:30:26,610
to dip into the outskirts of reason
through fascinating exploration of his

1464
00:30:26,610 --> 00:30:26,620
through fascinating exploration of his
 

1465
00:30:26,620 --> 00:30:30,919
through fascinating exploration of his
imagination is Joe Rogan oh yes you so

1466
00:30:30,919 --> 00:30:30,929
imagination is Joe Rogan oh yes you so
 

1467
00:30:30,929 --> 00:30:33,600
imagination is Joe Rogan oh yes you so
who has through reason used to believe a

1468
00:30:33,600 --> 00:30:33,610
who has through reason used to believe a
 

1469
00:30:33,610 --> 00:30:35,700
who has through reason used to believe a
lot of conspiracies and through a reason

1470
00:30:35,700 --> 00:30:35,710
lot of conspiracies and through a reason
 

1471
00:30:35,710 --> 00:30:37,440
lot of conspiracies and through a reason
has stripped away a lot of his beliefs

1472
00:30:37,440 --> 00:30:37,450
has stripped away a lot of his beliefs
 

1473
00:30:37,450 --> 00:30:39,899
has stripped away a lot of his beliefs
in that way so it's fascinating actually

1474
00:30:39,899 --> 00:30:39,909
in that way so it's fascinating actually
 

1475
00:30:39,909 --> 00:30:42,899
in that way so it's fascinating actually
to watch him through rationality kind of

1476
00:30:42,899 --> 00:30:42,909
to watch him through rationality kind of
 

1477
00:30:42,909 --> 00:30:46,250
to watch him through rationality kind of
throw away that ideas of Bigfoot and

1478
00:30:46,250 --> 00:30:46,260
throw away that ideas of Bigfoot and
 

1479
00:30:46,260 --> 00:30:49,200
throw away that ideas of Bigfoot and
9/11 I'm not sure exactly trails I don't

1480
00:30:49,200 --> 00:30:49,210
9/11 I'm not sure exactly trails I don't
 

1481
00:30:49,210 --> 00:30:50,200
9/11 I'm not sure exactly trails I don't
know what the leaves in yet

1482
00:30:50,200 --> 00:30:50,210
know what the leaves in yet
 

1483
00:30:50,210 --> 00:30:52,450
know what the leaves in yet
but you no longer know believed in

1484
00:30:52,450 --> 00:30:52,460
but you no longer know believed in
 

1485
00:30:52,460 --> 00:30:53,710
but you no longer know believed in
that's right no either he's become a

1486
00:30:53,710 --> 00:30:53,720
that's right no either he's become a
 

1487
00:30:53,720 --> 00:30:56,500
that's right no either he's become a
real force for for good yeah so you were

1488
00:30:56,500 --> 00:30:56,510
real force for for good yeah so you were
 

1489
00:30:56,510 --> 00:30:58,660
real force for for good yeah so you were
on the Joe Rogan podcast in February and

1490
00:30:58,660 --> 00:30:58,670
on the Joe Rogan podcast in February and
 

1491
00:30:58,670 --> 00:31:00,850
on the Joe Rogan podcast in February and
had a fascinating conversation but as

1492
00:31:00,850 --> 00:31:00,860
had a fascinating conversation but as
 

1493
00:31:00,860 --> 00:31:02,490
had a fascinating conversation but as
far as I remember didn't talk much about

1494
00:31:02,490 --> 00:31:02,500
far as I remember didn't talk much about
 

1495
00:31:02,500 --> 00:31:05,500
far as I remember didn't talk much about
artificial intelligence I will be on his

1496
00:31:05,500 --> 00:31:05,510
artificial intelligence I will be on his
 

1497
00:31:05,510 --> 00:31:06,760
artificial intelligence I will be on his
podcast in a couple weeks

1498
00:31:06,760 --> 00:31:06,770
podcast in a couple weeks
 

1499
00:31:06,770 --> 00:31:09,400
podcast in a couple weeks
Joe is very much concerned about

1500
00:31:09,400 --> 00:31:09,410
Joe is very much concerned about
 

1501
00:31:09,410 --> 00:31:11,560
Joe is very much concerned about
existential threat away I am not sure if

1502
00:31:11,560 --> 00:31:11,570
existential threat away I am not sure if
 

1503
00:31:11,570 --> 00:31:13,810
existential threat away I am not sure if
you're this is why I was I was hoping

1504
00:31:13,810 --> 00:31:13,820
you're this is why I was I was hoping
 

1505
00:31:13,820 --> 00:31:15,760
you're this is why I was I was hoping
that you would get into that topic and

1506
00:31:15,760 --> 00:31:15,770
that you would get into that topic and
 

1507
00:31:15,770 --> 00:31:18,490
that you would get into that topic and
in this way he represents quite a lot of

1508
00:31:18,490 --> 00:31:18,500
in this way he represents quite a lot of
 

1509
00:31:18,500 --> 00:31:20,650
in this way he represents quite a lot of
people who look at the topic of AI from

1510
00:31:20,650 --> 00:31:20,660
people who look at the topic of AI from
 

1511
00:31:20,660 --> 00:31:25,410
people who look at the topic of AI from
10,000 foot level so as an exercise of

1512
00:31:25,410 --> 00:31:25,420
10,000 foot level so as an exercise of
 

1513
00:31:25,420 --> 00:31:27,730
10,000 foot level so as an exercise of
communication he said it's important to

1514
00:31:27,730 --> 00:31:27,740
communication he said it's important to
 

1515
00:31:27,740 --> 00:31:29,530
communication he said it's important to
be rational and reason about these

1516
00:31:29,530 --> 00:31:29,540
be rational and reason about these
 

1517
00:31:29,540 --> 00:31:31,600
be rational and reason about these
things let me ask if you were to coach

1518
00:31:31,600 --> 00:31:31,610
things let me ask if you were to coach
 

1519
00:31:31,610 --> 00:31:34,150
things let me ask if you were to coach
me as AI researcher about how to speak

1520
00:31:34,150 --> 00:31:34,160
me as AI researcher about how to speak
 

1521
00:31:34,160 --> 00:31:36,640
me as AI researcher about how to speak
to Joe and the general public about AI

1522
00:31:36,640 --> 00:31:36,650
to Joe and the general public about AI
 

1523
00:31:36,650 --> 00:31:39,850
to Joe and the general public about AI
what would you advise well I'd the short

1524
00:31:39,850 --> 00:31:39,860
what would you advise well I'd the short
 

1525
00:31:39,860 --> 00:31:41,410
what would you advise well I'd the short
answer would be to read the sections

1526
00:31:41,410 --> 00:31:41,420
answer would be to read the sections
 

1527
00:31:41,420 --> 00:31:42,760
answer would be to read the sections
that I wrote an Enlightenment I know

1528
00:31:42,760 --> 00:31:42,770
that I wrote an Enlightenment I know
 

1529
00:31:42,770 --> 00:31:45,160
that I wrote an Enlightenment I know
about AI but a longer reason would be I

1530
00:31:45,160 --> 00:31:45,170
about AI but a longer reason would be I
 

1531
00:31:45,170 --> 00:31:47,440
about AI but a longer reason would be I
think to emphasize and I think you're

1532
00:31:47,440 --> 00:31:47,450
think to emphasize and I think you're
 

1533
00:31:47,450 --> 00:31:49,510
think to emphasize and I think you're
very well positioned as an engineer to

1534
00:31:49,510 --> 00:31:49,520
very well positioned as an engineer to
 

1535
00:31:49,520 --> 00:31:51,070
very well positioned as an engineer to
remind people about the culture of

1536
00:31:51,070 --> 00:31:51,080
remind people about the culture of
 

1537
00:31:51,080 --> 00:31:53,830
remind people about the culture of
engineering that it really is safety

1538
00:31:53,830 --> 00:31:53,840
engineering that it really is safety
 

1539
00:31:53,840 --> 00:31:56,770
engineering that it really is safety
oriented that another discussion in

1540
00:31:56,770 --> 00:31:56,780
oriented that another discussion in
 

1541
00:31:56,780 --> 00:32:00,790
oriented that another discussion in
enlightenment now I plot rates an

1542
00:32:00,790 --> 00:32:00,800
enlightenment now I plot rates an
 

1543
00:32:00,800 --> 00:32:02,440
enlightenment now I plot rates an
accidental death from various causes

1544
00:32:02,440 --> 00:32:02,450
accidental death from various causes
 

1545
00:32:02,450 --> 00:32:06,580
accidental death from various causes
plane crashes car crashes Occupational

1546
00:32:06,580 --> 00:32:06,590
plane crashes car crashes Occupational
 

1547
00:32:06,590 --> 00:32:08,590
plane crashes car crashes Occupational
accidents even death by lightning

1548
00:32:08,590 --> 00:32:08,600
accidents even death by lightning
 

1549
00:32:08,600 --> 00:32:12,910
accidents even death by lightning
strikes and they all plummet because the

1550
00:32:12,910 --> 00:32:12,920
strikes and they all plummet because the
 

1551
00:32:12,920 --> 00:32:14,830
strikes and they all plummet because the
culture of engineering is how do you

1552
00:32:14,830 --> 00:32:14,840
culture of engineering is how do you
 

1553
00:32:14,840 --> 00:32:17,200
culture of engineering is how do you
squeeze out the the lethal risks death

1554
00:32:17,200 --> 00:32:17,210
squeeze out the the lethal risks death
 

1555
00:32:17,210 --> 00:32:20,380
squeeze out the the lethal risks death
by fire death by drowning death by

1556
00:32:20,380 --> 00:32:20,390
by fire death by drowning death by
 

1557
00:32:20,390 --> 00:32:22,990
by fire death by drowning death by
asphyxiation all of them drastically

1558
00:32:22,990 --> 00:32:23,000
asphyxiation all of them drastically
 

1559
00:32:23,000 --> 00:32:24,310
asphyxiation all of them drastically
declined because of advances in

1560
00:32:24,310 --> 00:32:24,320
declined because of advances in
 

1561
00:32:24,320 --> 00:32:26,290
declined because of advances in
engineering then I gotta say I did not

1562
00:32:26,290 --> 00:32:26,300
engineering then I gotta say I did not
 

1563
00:32:26,300 --> 00:32:28,810
engineering then I gotta say I did not
appreciate until I saw those graphs and

1564
00:32:28,810 --> 00:32:28,820
appreciate until I saw those graphs and
 

1565
00:32:28,820 --> 00:32:32,140
appreciate until I saw those graphs and
it is because exactly people like you

1566
00:32:32,140 --> 00:32:32,150
it is because exactly people like you
 

1567
00:32:32,150 --> 00:32:34,450
it is because exactly people like you
who stamp at night thing oh my god it is

1568
00:32:34,450 --> 00:32:34,460
who stamp at night thing oh my god it is
 

1569
00:32:34,460 --> 00:32:36,490
who stamp at night thing oh my god it is
what a mime is what I mean what I'm

1570
00:32:36,490 --> 00:32:36,500
what a mime is what I mean what I'm
 

1571
00:32:36,500 --> 00:32:39,300
what a mime is what I mean what I'm
inventing likely to hurt people and to

1572
00:32:39,300 --> 00:32:39,310
inventing likely to hurt people and to
 

1573
00:32:39,310 --> 00:32:41,980
inventing likely to hurt people and to
deploy ingenuity to prevent that from

1574
00:32:41,980 --> 00:32:41,990
deploy ingenuity to prevent that from
 

1575
00:32:41,990 --> 00:32:43,690
deploy ingenuity to prevent that from
happening now I'm not an engineer

1576
00:32:43,690 --> 00:32:43,700
happening now I'm not an engineer
 

1577
00:32:43,700 --> 00:32:46,180
happening now I'm not an engineer
although I spent 22 years at MIT so I

1578
00:32:46,180 --> 00:32:46,190
although I spent 22 years at MIT so I
 

1579
00:32:46,190 --> 00:32:47,500
although I spent 22 years at MIT so I
know something about the culture of

1580
00:32:47,500 --> 00:32:47,510
know something about the culture of
 

1581
00:32:47,510 --> 00:32:49,090
know something about the culture of
engineering my understanding is that

1582
00:32:49,090 --> 00:32:49,100
engineering my understanding is that
 

1583
00:32:49,100 --> 00:32:50,710
engineering my understanding is that
this is the way this is what you think

1584
00:32:50,710 --> 00:32:50,720
this is the way this is what you think
 

1585
00:32:50,720 --> 00:32:51,890
this is the way this is what you think
if you're an engineer

1586
00:32:51,890 --> 00:32:51,900
if you're an engineer
 

1587
00:32:51,900 --> 00:32:54,919
if you're an engineer
and it's essential that that culture not

1588
00:32:54,919 --> 00:32:54,929
and it's essential that that culture not
 

1589
00:32:54,929 --> 00:32:57,860
and it's essential that that culture not
be suddenly switched off when come start

1590
00:32:57,860 --> 00:32:57,870
be suddenly switched off when come start
 

1591
00:32:57,870 --> 00:32:59,750
be suddenly switched off when come start
official intelligence so I mean fact

1592
00:32:59,750 --> 00:32:59,760
official intelligence so I mean fact
 

1593
00:32:59,760 --> 00:33:01,430
official intelligence so I mean fact
that could be a problem but is there any

1594
00:33:01,430 --> 00:33:01,440
that could be a problem but is there any
 

1595
00:33:01,440 --> 00:33:02,570
that could be a problem but is there any
reason to think it would be switched off

1596
00:33:02,570 --> 00:33:02,580
reason to think it would be switched off
 

1597
00:33:02,580 --> 00:33:04,519
reason to think it would be switched off
I don't think so and one there's not

1598
00:33:04,519 --> 00:33:04,529
I don't think so and one there's not
 

1599
00:33:04,529 --> 00:33:06,710
I don't think so and one there's not
enough engineers speaking up for this

1600
00:33:06,710 --> 00:33:06,720
enough engineers speaking up for this
 

1601
00:33:06,720 --> 00:33:10,490
enough engineers speaking up for this
way for this the excitement for the

1602
00:33:10,490 --> 00:33:10,500
way for this the excitement for the
 

1603
00:33:10,500 --> 00:33:12,560
way for this the excitement for the
positive view of human nature what

1604
00:33:12,560 --> 00:33:12,570
positive view of human nature what
 

1605
00:33:12,570 --> 00:33:13,850
positive view of human nature what
you're trying to create is the

1606
00:33:13,850 --> 00:33:13,860
you're trying to create is the
 

1607
00:33:13,860 --> 00:33:15,560
you're trying to create is the
positivity like everything we try to

1608
00:33:15,560 --> 00:33:15,570
positivity like everything we try to
 

1609
00:33:15,570 --> 00:33:17,659
positivity like everything we try to
invent is trying to do good for the

1610
00:33:17,659 --> 00:33:17,669
invent is trying to do good for the
 

1611
00:33:17,669 --> 00:33:19,490
invent is trying to do good for the
world but let me ask you about the

1612
00:33:19,490 --> 00:33:19,500
world but let me ask you about the
 

1613
00:33:19,500 --> 00:33:22,330
world but let me ask you about the
psychology of negativity it seems just

1614
00:33:22,330 --> 00:33:22,340
psychology of negativity it seems just
 

1615
00:33:22,340 --> 00:33:25,130
psychology of negativity it seems just
objectively not considering the topic it

1616
00:33:25,130 --> 00:33:25,140
objectively not considering the topic it
 

1617
00:33:25,140 --> 00:33:26,690
objectively not considering the topic it
seems that being negative about the

1618
00:33:26,690 --> 00:33:26,700
seems that being negative about the
 

1619
00:33:26,700 --> 00:33:29,000
seems that being negative about the
future makes you sound smarter than me

1620
00:33:29,000 --> 00:33:29,010
future makes you sound smarter than me
 

1621
00:33:29,010 --> 00:33:31,039
future makes you sound smarter than me
positive about the future irregardless

1622
00:33:31,039 --> 00:33:31,049
positive about the future irregardless
 

1623
00:33:31,049 --> 00:33:33,409
positive about the future irregardless
of topic am I correct in the observation

1624
00:33:33,409 --> 00:33:33,419
of topic am I correct in the observation
 

1625
00:33:33,419 --> 00:33:36,019
of topic am I correct in the observation
and if you if so why do you think that

1626
00:33:36,019 --> 00:33:36,029
and if you if so why do you think that
 

1627
00:33:36,029 --> 00:33:37,610
and if you if so why do you think that
is yeah I think that I think there is

1628
00:33:37,610 --> 00:33:37,620
is yeah I think that I think there is
 

1629
00:33:37,620 --> 00:33:41,120
is yeah I think that I think there is
that that phenomenon that as Tom Lehrer

1630
00:33:41,120 --> 00:33:41,130
that that phenomenon that as Tom Lehrer
 

1631
00:33:41,130 --> 00:33:42,889
that that phenomenon that as Tom Lehrer
the satirist said always predict the

1632
00:33:42,889 --> 00:33:42,899
the satirist said always predict the
 

1633
00:33:42,899 --> 00:33:44,560
the satirist said always predict the
worst and you'll be hailed as a prophet

1634
00:33:44,560 --> 00:33:44,570
worst and you'll be hailed as a prophet
 

1635
00:33:44,570 --> 00:33:48,680
worst and you'll be hailed as a prophet
it may be part of our overall negativity

1636
00:33:48,680 --> 00:33:48,690
it may be part of our overall negativity
 

1637
00:33:48,690 --> 00:33:51,799
it may be part of our overall negativity
bias we are as a species more attuned to

1638
00:33:51,799 --> 00:33:51,809
bias we are as a species more attuned to
 

1639
00:33:51,809 --> 00:33:53,299
bias we are as a species more attuned to
their negative than the positive we

1640
00:33:53,299 --> 00:33:53,309
their negative than the positive we
 

1641
00:33:53,309 --> 00:33:55,970
their negative than the positive we
dread losses more than we enjoy gains

1642
00:33:55,970 --> 00:33:55,980
dread losses more than we enjoy gains
 

1643
00:33:55,980 --> 00:34:01,960
dread losses more than we enjoy gains
and that mate might open up a space for

1644
00:34:01,960 --> 00:34:01,970

 

1645
00:34:01,970 --> 00:34:04,850

prophets to remind us of harms and risks

1646
00:34:04,850 --> 00:34:04,860
prophets to remind us of harms and risks
 

1647
00:34:04,860 --> 00:34:06,940
prophets to remind us of harms and risks
and losses that we may have overlooked

1648
00:34:06,940 --> 00:34:06,950
and losses that we may have overlooked
 

1649
00:34:06,950 --> 00:34:10,399
and losses that we may have overlooked
so I think there there there is that

1650
00:34:10,399 --> 00:34:10,409
so I think there there there is that
 

1651
00:34:10,409 --> 00:34:14,329
so I think there there there is that
asymmetry so you've written some of my

1652
00:34:14,329 --> 00:34:14,339
asymmetry so you've written some of my
 

1653
00:34:14,339 --> 00:34:17,300
asymmetry so you've written some of my
favorite books all over the place so

1654
00:34:17,300 --> 00:34:17,310
favorite books all over the place so
 

1655
00:34:17,310 --> 00:34:20,060
favorite books all over the place so
starting from enlightenment now to the

1656
00:34:20,060 --> 00:34:20,070
starting from enlightenment now to the
 

1657
00:34:20,070 --> 00:34:21,230
starting from enlightenment now to the
better angels of our nature

1658
00:34:21,230 --> 00:34:21,240
better angels of our nature
 

1659
00:34:21,240 --> 00:34:24,200
better angels of our nature
blank slate how the mind works the the

1660
00:34:24,200 --> 00:34:24,210
blank slate how the mind works the the
 

1661
00:34:24,210 --> 00:34:27,190
blank slate how the mind works the the
one about language language instinct

1662
00:34:27,190 --> 00:34:27,200
one about language language instinct
 

1663
00:34:27,200 --> 00:34:31,879
one about language language instinct
bill gates big fan to set of your most

1664
00:34:31,879 --> 00:34:31,889
bill gates big fan to set of your most
 

1665
00:34:31,889 --> 00:34:34,760
bill gates big fan to set of your most
recent book that it's my new favorite

1666
00:34:34,760 --> 00:34:34,770
recent book that it's my new favorite
 

1667
00:34:34,770 --> 00:34:38,720
recent book that it's my new favorite
book of all time so for you as an author

1668
00:34:38,720 --> 00:34:38,730
book of all time so for you as an author
 

1669
00:34:38,730 --> 00:34:41,180
book of all time so for you as an author
what was the book early on in your life

1670
00:34:41,180 --> 00:34:41,190
what was the book early on in your life
 

1671
00:34:41,190 --> 00:34:44,089
what was the book early on in your life
that had a profound impact on the way

1672
00:34:44,089 --> 00:34:44,099
that had a profound impact on the way
 

1673
00:34:44,099 --> 00:34:46,129
that had a profound impact on the way
you saw the world certainly this book

1674
00:34:46,129 --> 00:34:46,139
you saw the world certainly this book
 

1675
00:34:46,139 --> 00:34:49,129
you saw the world certainly this book
enlightenment now is influenced by David

1676
00:34:49,129 --> 00:34:49,139
enlightenment now is influenced by David
 

1677
00:34:49,139 --> 00:34:51,399
enlightenment now is influenced by David
Deutsch as the beginning of infinity a

1678
00:34:51,399 --> 00:34:51,409
Deutsch as the beginning of infinity a
 

1679
00:34:51,409 --> 00:34:54,950
Deutsch as the beginning of infinity a
rather deep reflection on knowledge and

1680
00:34:54,950 --> 00:34:54,960
rather deep reflection on knowledge and
 

1681
00:34:54,960 --> 00:34:57,770
rather deep reflection on knowledge and
the power of knowledge to improve the

1682
00:34:57,770 --> 00:34:57,780
the power of knowledge to improve the
 

1683
00:34:57,780 --> 00:35:00,890
the power of knowledge to improve the
human condition the and with bits of

1684
00:35:00,890 --> 00:35:00,900
human condition the and with bits of
 

1685
00:35:00,900 --> 00:35:02,420
human condition the and with bits of
wisdom such as that problems are

1686
00:35:02,420 --> 00:35:02,430
wisdom such as that problems are
 

1687
00:35:02,430 --> 00:35:04,220
wisdom such as that problems are
inevitable but problems are solvable

1688
00:35:04,220 --> 00:35:04,230
inevitable but problems are solvable
 

1689
00:35:04,230 --> 00:35:05,200
inevitable but problems are solvable
given the

1690
00:35:05,200 --> 00:35:05,210
given the
 

1691
00:35:05,210 --> 00:35:07,450
given the
knowledge and that solutions create new

1692
00:35:07,450 --> 00:35:07,460
knowledge and that solutions create new
 

1693
00:35:07,460 --> 00:35:09,160
knowledge and that solutions create new
problems have to be solved in their turn

1694
00:35:09,160 --> 00:35:09,170
problems have to be solved in their turn
 

1695
00:35:09,170 --> 00:35:11,320
problems have to be solved in their turn
that's I think a kind of wisdom about

1696
00:35:11,320 --> 00:35:11,330
that's I think a kind of wisdom about
 

1697
00:35:11,330 --> 00:35:13,480
that's I think a kind of wisdom about
the human condition that influenced the

1698
00:35:13,480 --> 00:35:13,490
the human condition that influenced the
 

1699
00:35:13,490 --> 00:35:15,550
the human condition that influenced the
writing of this book there's some books

1700
00:35:15,550 --> 00:35:15,560
writing of this book there's some books
 

1701
00:35:15,560 --> 00:35:17,349
writing of this book there's some books
that are excellent but obscure some of

1702
00:35:17,349 --> 00:35:17,359
that are excellent but obscure some of
 

1703
00:35:17,359 --> 00:35:20,589
that are excellent but obscure some of
which I have on my page of my website I

1704
00:35:20,589 --> 00:35:20,599
which I have on my page of my website I
 

1705
00:35:20,599 --> 00:35:22,589
which I have on my page of my website I
read a book called the history of force

1706
00:35:22,589 --> 00:35:22,599
read a book called the history of force
 

1707
00:35:22,599 --> 00:35:24,760
read a book called the history of force
self-published by a political scientist

1708
00:35:24,760 --> 00:35:24,770
self-published by a political scientist
 

1709
00:35:24,770 --> 00:35:27,040
self-published by a political scientist
named James Payne on the historical

1710
00:35:27,040 --> 00:35:27,050
named James Payne on the historical
 

1711
00:35:27,050 --> 00:35:28,450
named James Payne on the historical
decline of violence and that was one of

1712
00:35:28,450 --> 00:35:28,460
decline of violence and that was one of
 

1713
00:35:28,460 --> 00:35:30,339
decline of violence and that was one of
the inspirations for the better angels

1714
00:35:30,339 --> 00:35:30,349
the inspirations for the better angels
 

1715
00:35:30,349 --> 00:35:31,839
the inspirations for the better angels
of our nature

1716
00:35:31,839 --> 00:35:31,849
of our nature
 

1717
00:35:31,849 --> 00:35:35,710
of our nature
the what about early on if we look back

1718
00:35:35,710 --> 00:35:35,720
the what about early on if we look back
 

1719
00:35:35,720 --> 00:35:38,950
the what about early on if we look back
when you're maybe a teenager loved a

1720
00:35:38,950 --> 00:35:38,960
when you're maybe a teenager loved a
 

1721
00:35:38,960 --> 00:35:40,900
when you're maybe a teenager loved a
book called one two three infinity when

1722
00:35:40,900 --> 00:35:40,910
book called one two three infinity when
 

1723
00:35:40,910 --> 00:35:43,060
book called one two three infinity when
I was a young adult I read that book by

1724
00:35:43,060 --> 00:35:43,070
I was a young adult I read that book by
 

1725
00:35:43,070 --> 00:35:45,940
I was a young adult I read that book by
George gamma the physicist very

1726
00:35:45,940 --> 00:35:45,950
George gamma the physicist very
 

1727
00:35:45,950 --> 00:35:48,570
George gamma the physicist very
accessible in humorous explanations of

1728
00:35:48,570 --> 00:35:48,580
accessible in humorous explanations of
 

1729
00:35:48,580 --> 00:35:53,700
accessible in humorous explanations of
relativity of number theory of

1730
00:35:53,700 --> 00:35:53,710

 

1731
00:35:53,710 --> 00:35:57,040

dimensionality high multiple dimensional

1732
00:35:57,040 --> 00:35:57,050
dimensionality high multiple dimensional
 

1733
00:35:57,050 --> 00:35:59,980
dimensionality high multiple dimensional
spaces in a way that I think is still

1734
00:35:59,980 --> 00:35:59,990
spaces in a way that I think is still
 

1735
00:35:59,990 --> 00:36:01,810
spaces in a way that I think is still
delightful seventy years after it was

1736
00:36:01,810 --> 00:36:01,820
delightful seventy years after it was
 

1737
00:36:01,820 --> 00:36:04,420
delightful seventy years after it was
published I like that the time life

1738
00:36:04,420 --> 00:36:04,430
published I like that the time life
 

1739
00:36:04,430 --> 00:36:06,880
published I like that the time life
science series these were books that

1740
00:36:06,880 --> 00:36:06,890
science series these were books that
 

1741
00:36:06,890 --> 00:36:09,280
science series these were books that
would arrive every month my mother

1742
00:36:09,280 --> 00:36:09,290
would arrive every month my mother
 

1743
00:36:09,290 --> 00:36:11,160
would arrive every month my mother
subscribed to each one on a different

1744
00:36:11,160 --> 00:36:11,170
subscribed to each one on a different
 

1745
00:36:11,170 --> 00:36:14,829
subscribed to each one on a different
topic one would be on electricity what

1746
00:36:14,829 --> 00:36:14,839
topic one would be on electricity what
 

1747
00:36:14,839 --> 00:36:16,810
topic one would be on electricity what
would be on forests want to be learned

1748
00:36:16,810 --> 00:36:16,820
would be on forests want to be learned
 

1749
00:36:16,820 --> 00:36:18,040
would be on forests want to be learned
may evolution and then one was on the

1750
00:36:18,040 --> 00:36:18,050
may evolution and then one was on the
 

1751
00:36:18,050 --> 00:36:21,579
may evolution and then one was on the
mind and I was just intrigued that there

1752
00:36:21,579 --> 00:36:21,589
mind and I was just intrigued that there
 

1753
00:36:21,589 --> 00:36:23,980
mind and I was just intrigued that there
could be a science of mind and that that

1754
00:36:23,980 --> 00:36:23,990
could be a science of mind and that that
 

1755
00:36:23,990 --> 00:36:25,930
could be a science of mind and that that
book I would cite as an influence as

1756
00:36:25,930 --> 00:36:25,940
book I would cite as an influence as
 

1757
00:36:25,940 --> 00:36:28,450
book I would cite as an influence as
well then later on you fell in love with

1758
00:36:28,450 --> 00:36:28,460
well then later on you fell in love with
 

1759
00:36:28,460 --> 00:36:30,400
well then later on you fell in love with
the idea of studying the mind that's one

1760
00:36:30,400 --> 00:36:30,410
the idea of studying the mind that's one
 

1761
00:36:30,410 --> 00:36:32,109
the idea of studying the mind that's one
thing that grabbed you it was one of the

1762
00:36:32,109 --> 00:36:32,119
thing that grabbed you it was one of the
 

1763
00:36:32,119 --> 00:36:35,859
thing that grabbed you it was one of the
things I would say the I read as a

1764
00:36:35,859 --> 00:36:35,869
things I would say the I read as a
 

1765
00:36:35,869 --> 00:36:38,620
things I would say the I read as a
college student the book reflections on

1766
00:36:38,620 --> 00:36:38,630
college student the book reflections on
 

1767
00:36:38,630 --> 00:36:41,050
college student the book reflections on
language by Noam Chomsky spent most of

1768
00:36:41,050 --> 00:36:41,060
language by Noam Chomsky spent most of
 

1769
00:36:41,060 --> 00:36:44,290
language by Noam Chomsky spent most of
his career here at MIT Richard Dawkins

1770
00:36:44,290 --> 00:36:44,300
his career here at MIT Richard Dawkins
 

1771
00:36:44,300 --> 00:36:46,720
his career here at MIT Richard Dawkins
two books the blind watchmaker and The

1772
00:36:46,720 --> 00:36:46,730
two books the blind watchmaker and The
 

1773
00:36:46,730 --> 00:36:48,960
two books the blind watchmaker and The
Selfish Gene or enormous Li influential

1774
00:36:48,960 --> 00:36:48,970
Selfish Gene or enormous Li influential
 

1775
00:36:48,970 --> 00:36:52,180
Selfish Gene or enormous Li influential
partly for mainly for the content but

1776
00:36:52,180 --> 00:36:52,190
partly for mainly for the content but
 

1777
00:36:52,190 --> 00:36:55,570
partly for mainly for the content but
also for the writing style the ability

1778
00:36:55,570 --> 00:36:55,580
also for the writing style the ability
 

1779
00:36:55,580 --> 00:36:59,050
also for the writing style the ability
to explain abstract concepts in lively

1780
00:36:59,050 --> 00:36:59,060
to explain abstract concepts in lively
 

1781
00:36:59,060 --> 00:37:02,410
to explain abstract concepts in lively
prose Stephen Jay Gould first collection

1782
00:37:02,410 --> 00:37:02,420
prose Stephen Jay Gould first collection
 

1783
00:37:02,420 --> 00:37:05,980
prose Stephen Jay Gould first collection
ever since Darwin also excellent example

1784
00:37:05,980 --> 00:37:05,990
ever since Darwin also excellent example
 

1785
00:37:05,990 --> 00:37:09,060
ever since Darwin also excellent example
of lively writing George Miller

1786
00:37:09,060 --> 00:37:09,070
of lively writing George Miller
 

1787
00:37:09,070 --> 00:37:11,349
of lively writing George Miller
psychologist that most psychologists are

1788
00:37:11,349 --> 00:37:11,359
psychologist that most psychologists are
 

1789
00:37:11,359 --> 00:37:13,599
psychologist that most psychologists are
familiar with came up with the idea that

1790
00:37:13,599 --> 00:37:13,609
familiar with came up with the idea that
 

1791
00:37:13,609 --> 00:37:16,630
familiar with came up with the idea that
human memory has a capacity of seven

1792
00:37:16,630 --> 00:37:16,640
human memory has a capacity of seven
 

1793
00:37:16,640 --> 00:37:18,460
human memory has a capacity of seven
plus or minus two chunks and then

1794
00:37:18,460 --> 00:37:18,470
plus or minus two chunks and then
 

1795
00:37:18,470 --> 00:37:20,260
plus or minus two chunks and then
Sophia's biggest claim to fame but he

1796
00:37:20,260 --> 00:37:20,270
Sophia's biggest claim to fame but he
 

1797
00:37:20,270 --> 00:37:22,180
Sophia's biggest claim to fame but he
wrote a couple of books on language and

1798
00:37:22,180 --> 00:37:22,190
wrote a couple of books on language and
 

1799
00:37:22,190 --> 00:37:23,500
wrote a couple of books on language and
communication that I've read it's an

1800
00:37:23,500 --> 00:37:23,510
communication that I've read it's an
 

1801
00:37:23,510 --> 00:37:25,330
communication that I've read it's an
undergraduate again beautifully written

1802
00:37:25,330 --> 00:37:25,340
undergraduate again beautifully written
 

1803
00:37:25,340 --> 00:37:30,160
undergraduate again beautifully written
and intellectually deep wonderful Steven

1804
00:37:30,160 --> 00:37:30,170
and intellectually deep wonderful Steven
 

1805
00:37:30,170 --> 00:37:31,450
and intellectually deep wonderful Steven
thank you so much for taking the time

1806
00:37:31,450 --> 00:37:31,460
thank you so much for taking the time
 

1807
00:37:31,460 --> 00:37:40,520
thank you so much for taking the time
today my pleasure thanks a lot Lex

1808
00:37:40,520 --> 00:37:40,530

 

1809
00:37:40,530 --> 00:37:42,590

you


1
00:00:00,180 --> 00:00:02,700
The following is a conversation with Vladimir Vapnik.

2
00:00:02,900 --> 00:00:08,160
He is the co-inventor of support vector machines,
support vector clustering, VC theory, and

3
00:00:08,160 --> 00:00:10,660
many foundational ideas in statistical learning.

4
00:00:11,120 --> 00:00:16,160
He was born in the Soviet Union and worked
at the Institute of Control Sciences in Moscow.

5
00:00:16,160 --> 00:00:23,180
Then in the United States, he worked at AT&T,
NEC Labs, Facebook Research, and now

6
00:00:23,180 --> 00:00:25,220
is a professor at Columbia University.

7
00:00:25,840 --> 00:00:29,580
His work has been cited over 170,000 times.

8
00:00:30,240 --> 00:00:34,640
He has some very interesting ideas about artificial
intelligence and the nature of learning, especially

9
00:00:34,640 --> 00:00:39,580
especially, on the limits of our current approaches
and the open problems in the field.

10
00:00:40,380 --> 00:00:44,160
This conversation is part of the MIT course
on Artificial General Intelligence

11
00:00:44,160 --> 00:00:46,620
and the Artificial Intelligence Podcast.

12
00:00:46,620 --> 00:00:52,780
If you enjoy it, please subscribe on YouTube or
rate it on iTunes or your podcast provider of choice

13
00:00:52,780 --> 00:00:57,880
or simply connect with me on Twitter
or other social networks at Lex Fridman,

14
00:00:57,880 --> 00:00:59,780
spelled F-R-I-D.

15
00:01:00,120 --> 00:01:03,900
And now, here's my conversation with Vladimir Vapnik.

16
00:01:04,720 --> 00:01:08,760
Lex: Einstein famously said that God
doesn't play dice.

17
00:01:08,760 --> 00:01:09,480
Vladimir: Yeah.

18
00:01:09,880 --> 00:01:14,480
Lex: You have studied the world through the eyes
of statistics, so let me ask you in terms

19
00:01:14,480 --> 00:01:19,180
of the nature of reality--fundamental
nature of reality.

20
00:01:19,180 --> 00:01:20,460
Does God play dice?

21
00:01:21,240 --> 00:01:24,280
Vladimir: We don't know some factors.

22
00:01:25,280 --> 00:01:34,040
And because we don't know some factors, which could be important, it looks like God plays dice,

23
00:01:34,800 --> 00:01:37,980
but you should describe.

24
00:01:37,980 --> 00:01:44,440
In philosophy, they distinguish between two
positions: positions of instrumentalism,

25
00:01:44,800 --> 00:01:47,980
where you're creating theories of prediction

26
00:01:48,680 --> 00:01:51,960
and position of realism, where you're trying to

27
00:01:51,960 --> 00:01:54,040
understand what God did.

28
00:01:54,520 --> 00:01:57,680
Lex: Can you describe instrumentalism
and realism a little bit?

29
00:01:58,280 --> 00:02:03,300
For example, if you have some mechanical laws,

30
00:02:04,120 --> 00:02:05,020
what is that?

31
00:02:06,120 --> 00:02:11,180
Is it law which is true always and everywhere

32
00:02:11,400 --> 00:02:15,880
or is it a law which allows you to predict a position

33
00:02:15,880 --> 00:02:19,040
of moving elements?

34
00:02:21,400 --> 00:02:22,980
What do you believe?

35
00:02:22,980 --> 00:02:30,420
Do you believe that it is God's law, that
God created the world which is this

36
00:02:31,780 --> 00:02:36,200
physical law, or is it just law for predictions?

37
00:02:36,200 --> 00:02:38,480
Lex: And which one is instrumentalism?

38
00:02:38,480 --> 00:02:39,560
For predictions.

39
00:02:39,560 --> 00:02:46,840
If you believe that this is the law of God
and it is always true everywhere,

40
00:02:47,320 --> 00:02:49,720
that means that you're a realist.

41
00:02:49,720 --> 00:02:55,220
You're trying to understand God's thought.

42
00:02:55,220 --> 00:02:58,580
Lex: So the way you see the world is as an instrumentalist?

43
00:02:59,880 --> 00:03:02,780
Vladimir: You know I'm working from some models--

44
00:03:03,120 --> 00:03:06,560
Models of Machine Learning.

45
00:03:06,980 --> 00:03:11,700
So in this model, you can see settings

46
00:03:12,780 --> 00:03:17,600
and you try to resolve the problem.

47
00:03:18,160 --> 00:03:23,920
And you can do it in two different ways from
the point of view of the instrumentalist,

48
00:03:23,920 --> 00:03:31,580
and that's what everybody does now because
the goal of machine learning

49
00:03:31,580 --> 00:03:36,280
is to find the rule for classification.

50
00:03:36,700 --> 00:03:40,580
That is true, but it is an
instrument for prediction.

51
00:03:40,580 --> 00:03:50,220
But I can say, the goal of machine learning
is to learn about conditional probability,

52
00:03:50,220 --> 00:03:53,080
so how God play and use.

53
00:03:53,080 --> 00:03:57,560
Does he play what is the probability for one
and what is the probability for another

54
00:03:57,560 --> 00:03:59,500
in a given situation?

55
00:03:59,850 --> 00:04:02,340
But for prediction, I don't need this.

56
00:04:02,340 --> 00:04:03,600
I need the rule.

57
00:04:04,240 --> 00:04:08,360
But for understanding, I need conditional probability.

58
00:04:08,360 --> 00:04:13,360
Lex: So let me just step back a little bit first
to talk about, you mentioned which I read

59
00:04:13,360 --> 00:04:20,200
last night the parts of the 1960 paper by
Eugene Wigner,

60
00:04:20,640 --> 00:04:23,480
Unreasonable Effectiveness of Mathematics

61
00:04:23,480 --> 00:04:24,900
in the Natural Sciences.

62
00:04:24,900 --> 00:04:29,000
It's such a beautiful paper, by the way.

63
00:04:31,360 --> 00:04:38,060
To be honest, to confess my own work in the
past two years on deep learning heavily applied,

64
00:04:38,060 --> 00:04:44,100
it made me feel that I was missing out on
some of the beauty of nature in the way that

65
00:04:44,100 --> 00:04:45,720
math can uncover.

66
00:04:45,720 --> 00:04:50,500
So let me just step away from the poetry of
that for a second.

67
00:04:50,500 --> 00:04:52,940
How do you see the role of math in your life?

68
00:04:52,940 --> 00:04:53,940
Is it a tool?

69
00:04:53,940 --> 00:04:55,850
Is it poetry?

70
00:04:55,850 --> 00:04:57,220
Where does it sit?

71
00:04:57,220 --> 00:04:59,740
And does math, for you, have limits?

72
00:05:01,380 --> 00:05:07,220
Vladimir: Some people are saying that Math
is language which use god.

73
00:05:10,060 --> 00:05:12,100
Lex: Speak to god or use god?

74
00:05:12,160 --> 00:05:13,020
- Use God.

75
00:05:13,020 --> 00:05:13,740
Lex: Use God

76
00:05:17,420 --> 00:05:22,360
Vladimir: I believe that this article

77
00:05:23,560 --> 00:05:30,220
about Unreasonable Effectiveness
of Math is that if you look

78
00:05:30,220 --> 00:05:32,140
at mathematical structures,

79
00:05:33,720 --> 00:05:36,040
they know something about reality.

80
00:05:37,620 --> 00:05:44,440
And most scientists from Natural Science,
they look at an equation

81
00:05:44,920 --> 00:05:46,960
in trying to understand reality,

82
00:05:48,200 --> 00:05:50,360
so the same with machine learning.

83
00:05:51,160 --> 00:06:00,140
If you try to very carefully look on all the
equations which define conditional probability,

84
00:06:00,140 --> 00:06:08,000
you can understand something about reality
more than from your fantasy.

85
00:06:08,000 --> 00:06:13,860
Lex: So math can reveal the simple underlying
principles of reality, perhaps.

86
00:06:13,860 --> 00:06:19,440
Vladimir: You know, what may seem simple, it is very
hard to discover them.

87
00:06:20,100 --> 00:06:26,000
But then, when you discover them and look
at them, you see how beautiful they are.

88
00:06:26,640 --> 00:06:35,240
And it is surprising why people did not see
that before when you look at an equation and

89
00:06:35,260 --> 00:06:37,100
derive it from the equations.

90
00:06:37,530 --> 00:06:44,560
For example, I talked yesterday about the
Least Squares Method and people had a lot

91
00:06:44,560 --> 00:06:47,820
of fantasies about improving
least squares method.

92
00:06:48,020 --> 00:06:55,960
But if you look, going step by step by solving
some equations, you suddenly will get some terms

93
00:06:56,220 --> 00:07:03,600
which after thinking; you understand it,
the described position of an observation point.

94
00:07:04,100 --> 00:07:08,160
Least squares method, they throw out a lot
of information.

95
00:07:08,160 --> 00:07:11,580
You don't look at the composition of point
of observations.

96
00:07:11,580 --> 00:07:14,100
We're looking only on the details.

97
00:07:14,590 --> 00:07:21,780
But, when you understood that very simple
idea, which is not too simple to understand

98
00:07:21,789 --> 00:07:25,790
and you can derive this just from equations.

99
00:07:25,790 --> 00:07:31,710
Lex: So some simple Algebra, so a few steps will
take you to something surprising that when

100
00:07:31,710 --> 00:07:32,420
you think about--

101
00:07:32,420 --> 00:07:33,300
Vladimir: Absolutely, yes.

102
00:07:34,280 --> 00:07:40,600
And that is proof that human intuition is not too rich

103
00:07:40,600 --> 00:07:43,940
and very primitive, and it does not see

104
00:07:43,940 --> 00:07:48,640
very simple situations.

105
00:07:48,640 --> 00:07:53,020
Lex:  So let me take a step back, in general, yes.

106
00:07:54,440 --> 00:08:03,500
What about human ingenuity as opposed to intuition,
the moments of brilliance?

107
00:08:06,840 --> 00:08:09,600
Do you have to be so hard on human intuition?

108
00:08:09,600 --> 00:08:14,360
Are there moments of brilliance on human intuition
that can leap ahead of math,

109
00:08:14,780 --> 00:08:16,200
and then the math will catch up?

110
00:08:17,520 --> 00:08:19,080
Vladimir: I don't think so.

111
00:08:19,500 --> 00:08:28,200
I think the best human intuition, it is putting
in axioms, then it is technical

112
00:08:28,200 --> 00:08:29,540
where you have to arrive.

113
00:08:29,540 --> 00:08:31,000
Lex: See where the axioms take you.

114
00:08:31,020 --> 00:08:31,880
Vladimir: Yeah.

115
00:08:31,880 --> 00:08:34,860
But if they correctly take axioms.

116
00:08:36,340 --> 00:08:44,500
Axioms are polished during generations of
scientists and this is integral wisdom.

117
00:08:46,000 --> 00:08:47,740
Lex: That's beautifully put.

118
00:08:51,420 --> 00:08:59,000
When you think of Einstein and especially,
relativity, what is the role of imagination

119
00:08:59,000 --> 00:09:03,740
coming first there in the moment of
discovery of an idea?

120
00:09:04,380 --> 00:09:10,020
So, that's obviously a mix of math and out
of the box imagination there.

121
00:09:10,660 --> 00:09:11,920
Vladimir: That, I don't know.

122
00:09:12,520 --> 00:09:20,640
Whatever I did, I exclude any imagination
because whatever I saw in machine learning

123
00:09:20,640 --> 00:09:27,190
that come from imagination, like features,
like deep learning, they're not really one

124
00:09:27,190 --> 00:09:28,800
to the problem.

125
00:09:29,180 --> 00:09:36,540
When you're looking very clearly from a mathematical
equation, you'd arrive in very simple story

126
00:09:36,540 --> 00:09:42,830
which goes far beyond, theoretically, than
whatever people can imagine because it is

127
00:09:42,830 --> 00:09:44,760
not good fantasies.

128
00:09:44,760 --> 00:09:46,800
It is just interpretation.

129
00:09:46,800 --> 00:09:51,280
It is just fantasy, but it is not what you need.

130
00:09:51,280 --> 00:09:59,140
You don't need any imagination to derive mind
principle of machine learning.

131
00:09:59,840 --> 00:10:04,920
Lex: When you think about learning and intelligence,
maybe thinking about the human brain in trying

132
00:10:04,920 --> 00:10:11,990
to describe mathematically the process of
learning that is something like what happens

133
00:10:11,990 --> 00:10:16,000
in the human brain, do you think we have the
tools, currently?

134
00:10:17,200 --> 00:10:21,500
Do you think we will ever have the tools to
try to describe that process of learning?

135
00:10:22,560 --> 00:10:25,580
Vladimir: It is not description what's going on.

136
00:10:25,580 --> 00:10:27,500
It is interpretation.

137
00:10:27,500 --> 00:10:29,320
It is your interpretation.

138
00:10:29,320 --> 00:10:31,580
Your vision can be wrong.

139
00:10:32,000 --> 00:10:38,740
You know, when the guy who invented the microscope,
Leeuwenhoek, for the first time,

140
00:10:38,900 --> 00:10:43,320
only he got this instrument and he kept it secret.

141
00:10:45,500 --> 00:10:49,000
But he wrote a report in
the London Academy of Science.

142
00:10:49,000 --> 00:10:54,120
In his report, when he's looking on the blood,
he looked everywhere--on the water, on the

143
00:10:54,120 --> 00:11:03,460
blood on those film, but he described blood
like a fight between queens and kings.

144
00:11:04,080 --> 00:11:11,540
So he saw blood cells, red cells and he imagines
it is like an army fighting each other.

145
00:11:12,340 --> 00:11:15,240
And it was his interpretation of the situation.

146
00:11:17,020 --> 00:11:20,600
And he sent it as a report in the Academy
of Science.

147
00:11:20,600 --> 00:11:24,300
They very carefully looked because they believe
that he is right.

148
00:11:24,300 --> 00:11:27,779
He saw something, but he gave a wrong interpretation.

149
00:11:28,180 --> 00:11:31,500
And I believe the same can happen with the brain.

150
00:11:33,460 --> 00:11:38,380
The most important part, you know, I believe
in human language.

151
00:11:38,760 --> 00:11:42,820
In some proverbs, there's so much wisdom.

152
00:11:43,100 --> 00:11:51,420
For example, people say that it is better
than a thousand days of diligent study

153
00:11:51,420 --> 00:11:53,780
is one day with a great teacher.

154
00:11:54,080 --> 00:11:58,740
But if you'll ask what the teacher does,
nobody knows.

155
00:11:59,480 --> 00:12:01,000
And that is intelligence.

156
00:12:01,460 --> 00:12:08,400
But we know from history, and now
from machine learning

157
00:12:08,400 --> 00:12:12,240
is that a teacher can do a lot.

158
00:12:12,240 --> 00:12:16,070
Lex: So what from a mathematical
point of view is a great teacher?

159
00:12:16,070 --> 00:12:24,920
Vladimir: I don't know, but we can say
what a teacher can do.

160
00:12:25,120 --> 00:12:32,160
He can introduce some invariants, some predicate
for creating invariants.

161
00:12:32,160 --> 00:12:37,800
How is he doing it, I don't know, because
a teacher knows reality and can describe from

162
00:12:37,800 --> 00:12:41,240
his reality a predicate and invariants.

163
00:12:41,240 --> 00:12:45,920
But we know when you're using invariant, you
can decrease the number of observations

164
00:12:45,920 --> 00:12:47,420
a hundred times.

165
00:12:50,920 --> 00:12:55,180
Lex: Maybe try to pull that apart a little bit,
but I think you mentioned that like a piano

166
00:12:55,190 --> 00:12:59,540
teacher saying to the student,
"Play like a butterfly."

167
00:12:59,540 --> 00:13:00,700
I played piano.

168
00:13:00,700 --> 00:13:02,340
I played the guitar for a long time

169
00:13:07,020 --> 00:13:10,680
and maybe it's romantic
and poetic, but it feels like

170
00:13:10,680 --> 00:13:15,260
there's a lot of truth in that statement,
like there's a lot of instruction to that statement.

171
00:13:15,440 --> 00:13:18,220
Can you pull that apart?

172
00:13:18,220 --> 00:13:19,040
What is that?

173
00:13:19,520 --> 00:13:22,860
The language itself may not contain this information.

174
00:13:22,860 --> 00:13:26,860
Vladimir: It's not blah, blah, blah
because it affects you.

175
00:13:26,980 --> 00:13:27,620
It's what?

176
00:13:27,620 --> 00:13:29,940
Affects you, affects your playing.

177
00:13:29,940 --> 00:13:31,000
Lex: Yes it does,

178
00:13:35,800 --> 00:13:38,280
but what is the information being exchanged there?

179
00:13:38,280 --> 00:13:39,950
What is the nature of information?

180
00:13:39,950 --> 00:13:41,959
What is the representation in that information?

181
00:13:41,959 --> 00:13:45,329
Vladimir: I believe that it is a sort of predicate,
but I don't know.

182
00:13:45,329 --> 00:13:49,560
That is exactly what intelligence in machine
learning should be

183
00:13:50,020 --> 00:13:52,820
because the rest is just mathematical technique.

184
00:13:53,120 --> 00:14:02,500
I think that what was discovered recently
is that there are two mechanisms of learning.

185
00:14:03,200 --> 00:14:08,300
One is called strong convergence mechanism
and big convergence mechanism.

186
00:14:08,300 --> 00:14:11,029
Before, people used only one convergence.

187
00:14:11,520 --> 00:14:15,620
In big convergence, you can use predicate.

188
00:14:15,700 --> 00:14:23,580
That's what "fly like butterfly" is and if
you immediately effect your plan.

189
00:14:23,580 --> 00:14:31,340
You know there is an English proverb which
is "If it looks like a duck, sleeps like a duck,

190
00:14:31,340 --> 00:14:35,180
and quack like a duck, then it is
probably a duck."

191
00:14:36,180 --> 00:14:39,900
But this is exact about predicate.

192
00:14:40,340 --> 00:14:42,740
It looks like a duck, what does it mean?

193
00:14:42,740 --> 00:14:47,149
So, you saw many ducks--that's your training data.

194
00:14:47,149 --> 00:14:56,440
You have a description that looks like ducks.

195
00:14:56,440 --> 00:15:00,340
Lex: Yeah, the visual characteristics of a duck, yeah.

196
00:15:00,760 --> 00:15:04,140
Vladimir: Yeah, and you have a model
for recognizing ducks.

197
00:15:04,140 --> 00:15:10,660
So you would like that theoretical description
from the model to coincide.

198
00:15:10,660 --> 00:15:13,839
There's empirical description which you saw.

199
00:15:14,420 --> 00:15:18,260
So, about "it looks like a duck," it is general.

200
00:15:18,260 --> 00:15:20,700
But, what about swims like a duck?

201
00:15:21,460 --> 00:15:23,560
You should know that ducks swim.

202
00:15:23,560 --> 00:15:26,400
You can't say it plays chess like a duck.

203
00:15:26,400 --> 00:15:28,700
Okay, ducks doesn't play chess.

204
00:15:28,700 --> 00:15:34,460
It's a completely legal predicate but it is
useless.

205
00:15:35,500 --> 00:15:40,660
So, how can a teacher recognize a non-useless
predicate?

206
00:15:41,080 --> 00:15:46,600
So, up to now, we don't use this predicate
in existing machine learning,

207
00:15:47,020 --> 00:15:49,560
so why do we need zillions of data?

208
00:15:50,620 --> 00:15:56,480
But this English proverb say use only three
predicates--looks like a duck,

209
00:15:56,480 --> 00:15:59,060
swims like a duck and quack like a duck.

210
00:15:59,060 --> 00:16:05,260
Lex: So you can't deny the fact that swims like
a duck and quacks like a duck has humor

211
00:16:05,260 --> 00:16:08,180
in it, has ambiguity?

212
00:16:08,660 --> 00:16:11,460
Vladimir: Let's talk about "swims like a duck."

213
00:16:12,720 --> 00:16:17,220
It does not say jumps like a duck, why?

214
00:16:17,660 --> 00:16:19,980
Lex: It's not relevant.

215
00:16:20,640 --> 00:16:26,040
Vladimir: It means that you know ducks and you know
different birds.

216
00:16:26,040 --> 00:16:32,360
You know animals and you derived from this
that it is relevant to say "swim like a duck."

217
00:16:34,240 --> 00:16:38,960
Lex: So in order for us to understand "swims like
a duck," it feels like we need to know

218
00:16:38,960 --> 00:16:44,399
millions of other little pieces of information
we pick up along the way.

219
00:16:44,399 --> 00:16:45,399
You don't think so?

220
00:16:45,399 --> 00:16:52,600
That doesn't need to be this knowledge-based,
in those statements, carry some rich information

221
00:16:52,600 --> 00:16:55,260
that helps us understand the essence of duck?

222
00:16:55,260 --> 00:16:55,840
Vladimir: Yeah.

223
00:16:57,180 --> 00:17:01,600
Lex: How far are we from integrating predicates?

224
00:17:01,600 --> 00:17:09,480
Vladimir: You know that when you can see the
complete story of machine learning, so what it does,

225
00:17:09,480 --> 00:17:11,220
you have a lot of functions,

226
00:17:12,380 --> 00:17:16,420
and then you're talking it looks like a duck.

227
00:17:17,600 --> 00:17:20,360
You see your training data.

228
00:17:20,760 --> 00:17:30,140
From the training data, you recognize what
the expected duck should look like.

229
00:17:31,000 --> 00:17:38,500
Then, you remove all functions which do not
look like what you think it should look from

230
00:17:38,500 --> 00:17:39,400
the training data.

231
00:17:40,080 --> 00:17:45,360
So, you decrease the amount of function from
which you pick up one.

232
00:17:45,369 --> 00:17:51,759
Then, you give a second predicate and again,
they create a set of functions.

233
00:17:51,760 --> 00:17:55,019
And after that, you pick up
the best function you can.

234
00:17:55,460 --> 00:17:57,980
It is standard machine learning.

235
00:17:57,980 --> 00:18:02,000
So, why do you need not too many examples?

236
00:18:03,260 --> 00:18:05,240
Lex: Because your predicates are very good.

237
00:18:05,880 --> 00:18:14,619
Vladimir: Yeah, that's exactly basic predicate because
every predicate is invented to decrease the

238
00:18:14,620 --> 00:18:16,100
admissible set of functions.

239
00:18:17,540 --> 00:18:22,640
Lex: So you talk about admissible set of functions
and you talk about good functions.

240
00:18:22,649 --> 00:18:24,450
So what makes a good function?

241
00:18:24,450 --> 00:18:32,129
Vladimir: So admissible set of function is a set of
function which has a small capacity or small

242
00:18:32,129 --> 00:18:37,320
diversity, a small dimension, which contains
good functions inside.

243
00:18:37,320 --> 00:18:42,940
Lex: By the way, for people who don't know VC,
you're the V in the VC.

244
00:18:44,800 --> 00:18:49,640
So how would you describe to a lay person
what VC theories are?

245
00:18:50,480 --> 00:18:51,860
How would you describe VC?

246
00:18:51,860 --> 00:18:53,460
Vladimir: When you have a machine,

247
00:18:54,680 --> 00:18:58,920
a machine capable to pick up one function

248
00:18:58,920 --> 00:19:01,300
from the admissible set of function.

249
00:19:03,480 --> 00:19:06,420
But the set of admissible functions can be big.

250
00:19:07,560 --> 00:19:11,620
They contain all continuous functions and
theories.

251
00:19:11,620 --> 00:19:14,799
You don't have so many examples to pick up
functions.

252
00:19:14,800 --> 00:19:16,660
But it can be small--

253
00:19:20,260 --> 00:19:27,840
what we call capacity, but maybe diversity--
so not very different functions in the settings,

254
00:19:27,840 --> 00:19:30,970
an infinite set of functions but not very diverse.

255
00:19:30,970 --> 00:19:36,080
So, if it's a small VC dimension and when
the VC dimension is small,

256
00:19:36,080 --> 00:19:40,460
you need a small amount of training data.

257
00:19:41,620 --> 00:19:51,180
So the goal is to create admissible set of
functions which have small VC dimension

258
00:19:51,180 --> 00:19:53,100
and contains good functions.

259
00:19:53,100 --> 00:20:00,140
Then, you'll be able to pick up the function
using a small amount of observations.

260
00:20:02,180 --> 00:20:11,380
Lex: So that is the task of learning is creating
a set of admissible functions

261
00:20:11,380 --> 00:20:12,940
that has a small VC dimension

262
00:20:12,940 --> 00:20:18,020
and then you figure out a clever
way of picking up the good.

263
00:20:18,020 --> 00:20:22,200
Vladimir: That is the goal of learning
which I formulated yesterday.

264
00:20:22,200 --> 00:20:30,440
Statistical learning theory does not involve
creating admissible set of functions.

265
00:20:30,440 --> 00:20:37,950
In classical learning theory everywhere, in
100% of textbooks, the admissible set of functions

266
00:20:37,950 --> 00:20:44,609
is given, but this is telling us about nothing
because the most difficult problem is to create

267
00:20:44,609 --> 00:20:48,700
admissible set of functions given, say,

268
00:20:49,880 --> 00:20:53,020
a lot of functions, a continuous set of functions.

269
00:20:53,020 --> 00:21:00,380
Create admissible set of functions, that means
that the finite VC dimension, small VC dimension

270
00:21:00,389 --> 00:21:01,919
and contains good functions.

271
00:21:01,920 --> 00:21:05,280
So, this was out of consideration.

272
00:21:05,280 --> 00:21:08,399
Lex: So what's the process of doing that,
I mean, that's fascinating?

273
00:21:08,399 --> 00:21:13,429
What is the process of creating this admissible
set of functions?

274
00:21:13,429 --> 00:21:14,849
Vladimir: That is invariance.

275
00:21:14,849 --> 00:21:15,849
Lex: That's invariance.

276
00:21:15,849 --> 00:21:17,239
Can you describe invariance?

277
00:21:17,239 --> 00:21:18,239
Vladimir: Yeah.

278
00:21:18,239 --> 00:21:30,239
You have to think of properties of the training
data and properties means they have some function

279
00:21:30,240 --> 00:21:38,060
and you just count what is the average value
of function of training data.

280
00:21:39,000 --> 00:21:44,700
You have a model and what is the expectation
of this function on the model

281
00:21:44,700 --> 00:21:46,520
and they should coincide.

282
00:21:46,520 --> 00:21:51,320
So, the problem is about how to pick up functions.

283
00:21:51,720 --> 00:21:53,520
It can be any function.

284
00:21:56,160 --> 00:21:59,920
In fact, it is true for all functions,

285
00:22:02,820 --> 00:22:08,700
but when I say a duck doesn't jump, so you don't

286
00:22:08,700 --> 00:22:13,420
ask a question on "jumps like a duck"
because it is trivial.

287
00:22:13,420 --> 00:22:16,000
It does not jump, so it does not help you at all.

288
00:22:16,840 --> 00:22:24,880
But you know something on which questions
to ask like when you ask "swims like a duck."

289
00:22:24,880 --> 00:22:27,540
But "looks like a duck," it is a general situation.

290
00:22:27,560 --> 00:22:37,900
But, looks like, say, a guy who has this illness,
this disease, it is legal.

291
00:22:38,840 --> 00:22:47,240
So, there is a general type of predicate,
"It looks like," and a special type of predicate

292
00:22:47,260 --> 00:22:50,200
which is related to this specific problem.

293
00:22:50,900 --> 00:22:56,280
And that is the intelligence part of this
business and that is where a teacher is involved.

294
00:22:56,280 --> 00:22:59,120
Lex: Incorporating the specialized predicates.

295
00:22:59,380 --> 00:23:00,380
Vladimir: Yes.

296
00:23:00,389 --> 00:23:01,389
Lex: Okay.

297
00:23:01,389 --> 00:23:08,420
What do you think about deep learning
as neural networks, these architectures,

298
00:23:08,420 --> 00:23:13,240
as helping accomplish some of the tasks
you're thinking about?

299
00:23:13,240 --> 00:23:15,200
Their effectiveness or lack thereof,

300
00:23:15,200 --> 00:23:17,420
what are the weaknesses

301
00:23:17,420 --> 00:23:19,780
and what are the possible strengths?

302
00:23:19,980 --> 00:23:28,500
Vladimir: You know, I think that this is fantasy,
everything like deep learning, like features.

303
00:23:29,080 --> 00:23:32,720
Let me give you this example.

304
00:23:33,940 --> 00:23:38,740
One of the greatest books is Churchill's book
about the history of the Second World War.

305
00:23:39,040 --> 00:23:46,860
He starts in his book describing that in the
old times when a war is over,

306
00:23:49,540 --> 00:23:52,200
the great kings,

307
00:23:52,920 --> 00:24:00,380
they gather together--and most of them are
relatives--and they discuss what should be

308
00:24:00,380 --> 00:24:04,739
done to create peace and they come to an agreement.

309
00:24:04,920 --> 00:24:07,900
And what happens in the First World War?

310
00:24:10,280 --> 00:24:13,020
The general public came in power.

311
00:24:13,460 --> 00:24:17,700
They were so greedy that robbed Germany.

312
00:24:18,080 --> 00:24:24,180
It was clear for everybody that it is not
peace, that peace will only last for 20 years

313
00:24:24,680 --> 00:24:28,600
because they were not professionals.

314
00:24:28,600 --> 00:24:31,660
I see the same in machine logic.

315
00:24:32,000 --> 00:24:40,020
There are mathematicians looking for the problem
from a very deep mathematical point of view

316
00:24:40,020 --> 00:24:46,059
and there are computer scientists that mostly
do not know mathematics.

317
00:24:46,059 --> 00:24:52,440
They just have interpretations of that and
they invented a lot of blah, blah interpretations

318
00:24:52,440 --> 00:24:53,740
like deep learning.

319
00:24:53,740 --> 00:24:55,700
Why did you do deep learning?

320
00:24:55,700 --> 00:24:57,679
Mathematics does not know deep learning.

321
00:24:57,679 --> 00:25:02,619
Mathematics does not know neurons; it is just
functions.

322
00:25:02,620 --> 00:25:06,240
If you like to say piecewise linear function, say that

323
00:25:06,580 --> 00:25:10,540
and do it in a class of piecewise linear function.

324
00:25:10,540 --> 00:25:18,580
But they invented something and then they
tried to prove the advantage of that

325
00:25:18,580 --> 00:25:22,220
through interpretations, which was mostly wrong.

326
00:25:22,220 --> 00:25:27,700
And when it is not enough, they appeal to
the brain and they say they know nothing about that.

327
00:25:27,700 --> 00:25:30,080
Nobody knows what's going in the brain.

328
00:25:30,320 --> 00:25:34,360
So, I think it is more reliable to work on math.

329
00:25:34,360 --> 00:25:39,020
This is a mathematical problem, do your best
to solve this problem.

330
00:25:39,360 --> 00:25:44,220
Try to understand that there is not only one
way of convergence,

331
00:25:44,220 --> 00:25:46,240
which is the strong way of convergence.

332
00:25:46,240 --> 00:25:49,840
There is a big way of convergence
which requires predicates.

333
00:25:49,840 --> 00:25:55,980
And if you will go through all this stuff,
you will see that you don't need deep learning.

334
00:25:56,460 --> 00:26:03,540
Even more, I would say one of the theorems,
which is called Representer theorem,

335
00:26:03,540 --> 00:26:11,100
it says that optimal solution of mathematical problems,

336
00:26:11,100 --> 00:26:19,380
which describe learning, is on a shallow network,

337
00:26:19,380 --> 00:26:20,840
not on deep learning.

338
00:26:20,840 --> 00:26:22,169
Lex: On a shallow network.

339
00:26:22,169 --> 00:26:24,269
Yeah, the problem is there.

340
00:26:24,269 --> 00:26:28,820
Absolutely. So, in the end, what you're saying
is exactly right.

341
00:26:29,180 --> 00:26:37,920
The question is, you have no value for throwing
something on the table, playing with it--not math.

342
00:26:37,920 --> 00:26:43,799
It's like a neural network where you said
throwing something in the bucket or the biological

343
00:26:43,799 --> 00:26:47,889
example in looking at kings and queens or
the cells on the microscope, you don't see

344
00:26:47,889 --> 00:26:55,520
value in imagining the cells or the kings
and queens and using that as inspiration,

345
00:26:55,520 --> 00:26:59,080
an imagination for where the math
will eventually lead you?

346
00:26:59,100 --> 00:27:05,560
Do you think that interpretation basically
deceives you in a way that's not productive?

347
00:27:06,300 --> 00:27:14,180
Vladimir: I think that if you're trying to analyze this
business of learning

348
00:27:14,520 --> 00:27:18,200
and especially, the discussion about deep learning,

349
00:27:18,200 --> 00:27:22,860
it is a discussion about
interpretations and not about things,

350
00:27:22,860 --> 00:27:25,860
about what you can say about things.

351
00:27:26,040 --> 00:27:26,760
Lex: That's right.

352
00:27:26,760 --> 00:27:33,180
But, aren't you surprised by the beauty of it,
not mathematical beauty but the fact

353
00:27:33,180 --> 00:27:35,580
that it works at all?

354
00:27:35,580 --> 00:27:44,360
Or, are you criticizing that very beauty,
our human desire to interpret,

355
00:27:44,360 --> 00:27:48,900
to find our silly interpretations in these constructs?

356
00:27:49,440 --> 00:27:57,879
Like, let me ask you this, are you surprised
or does it inspire you, how do you feel about

357
00:27:57,880 --> 00:28:02,060
the success of a system like AlphaGo at beating
the game of Go

358
00:28:03,100 --> 00:28:05,960
using neural networks to estimate

359
00:28:06,560 --> 00:28:09,400
the quality of a board?

360
00:28:11,200 --> 00:28:14,320
Vladimir: That is your interpretation--quality of the board.

361
00:28:14,320 --> 00:28:15,679
Lex: Yes.

362
00:28:16,940 --> 00:28:19,920
It is not our interpretation.

363
00:28:20,200 --> 00:28:24,460
The fact is a neural network system--it doesn't
matter--a learning system

364
00:28:25,140 --> 00:28:29,760
that we don't, I think, mathematically, understand that well, beats the best human player,

365
00:28:29,760 --> 00:28:31,539
that's something that was thought impossible.

366
00:28:31,540 --> 00:28:34,960
Vladimir: That means it's not a very difficult problem. That's it.

367
00:28:34,960 --> 00:28:40,920
Lex: So we've empirically have discovered that
this is not a very difficult problem.

368
00:28:42,020 --> 00:28:42,740
That's true.

369
00:28:46,200 --> 00:28:47,660
I can't argue.

370
00:28:50,060 --> 00:28:58,080
Vladimir: Even more, I would say, if they used deep
learning, it is not the most effective way

371
00:28:58,080 --> 00:28:59,520
of learning theory.

372
00:28:59,920 --> 00:29:07,700
And usually, when people use deep learning,
they're using zillions of training data,

373
00:29:09,660 --> 00:29:12,620
but you don't need this.

374
00:29:13,300 --> 00:29:20,080
So when I describe a challenge, can we do
some problems that you did well

375
00:29:20,900 --> 00:29:23,900
with deep learning method, with deepnet,

376
00:29:23,900 --> 00:29:27,940
using a hundred times less training data?

377
00:29:27,940 --> 00:29:36,909
Even more, there are some problems that deep
learning cannot solve because it's not necessarily

378
00:29:36,909 --> 00:29:40,820
that they created admissible set of functions.

379
00:29:40,820 --> 00:29:45,859
To create deep architecture means to create
admissible set of functions.

380
00:29:45,860 --> 00:29:49,440
You cannot say that you're creating good admissible
set of functions.

381
00:29:50,340 --> 00:29:52,780
It's your fantasy.

382
00:29:52,780 --> 00:29:54,360
It does not come from us.

383
00:29:54,780 --> 00:30:01,040
But, it is possible to create admissible set
of functions because you have your training data

384
00:30:01,960 --> 00:30:08,580
Actually, for mathematicians, when you
consider a variant,

385
00:30:08,580 --> 00:30:11,880
you need to use the law of large numbers.

386
00:30:11,880 --> 00:30:20,340
When you make a training in existing algorithms,
you need a uniform law of large numbers,

387
00:30:20,340 --> 00:30:22,359
which is much more difficult.

388
00:30:22,360 --> 00:30:24,860
It requires VC dimension and all that stuff.

389
00:30:25,000 --> 00:30:33,500
But nevertheless, if you use both big and
strong way of convergence, you can decrease

390
00:30:33,509 --> 00:30:34,850
a lot of training data.

391
00:30:34,850 --> 00:30:39,660
Lex: Yeah, you could do the three--that swims like
a duck and quacks like a duck.

392
00:30:41,200 --> 00:30:47,400
So let's step back and think about
human intelligence in general.

393
00:30:48,160 --> 00:30:50,160
And clearly, that has evolved

394
00:30:50,840 --> 00:30:52,740
in a non-mathematical way.

395
00:30:55,660 --> 00:31:04,620
Lex: As far as we know, God or whoever didn't come
up with a model and placed in our brain

396
00:31:04,620 --> 00:31:06,700
of admissible functions; it kind of evolved.

397
00:31:06,710 --> 00:31:15,299
I don't know your view on this but Alan Turing
in the 50's in his paper asked and interjected

398
00:31:15,299 --> 00:31:17,019
the question: Can machines think?

399
00:31:17,020 --> 00:31:23,720
It's not a very useful question, but can you
briefly entertain this useless question

400
00:31:24,040 --> 00:31:25,559
"Can machines think?"

401
00:31:25,559 --> 00:31:28,609
So, talk about intelligence and your view of it.

402
00:31:28,609 --> 00:31:29,889
Vladimir: I don't know that.

403
00:31:29,889 --> 00:31:39,240
I know that Turing described imitation--if
a computer can imitate a human being.

404
00:31:39,640 --> 00:31:46,240
Let's call it intelligence and he understands
that it is not a thinking computer.

405
00:31:46,300 --> 00:31:53,200
He completely understands what he was doing,
but he set up a problem of imitation.

406
00:31:53,680 --> 00:31:57,940
So now we understand it as a problem of not
an imitation.

407
00:31:57,940 --> 00:32:03,480
I'm not sure that intelligence is just inside of us.

408
00:32:04,140 --> 00:32:06,220
It may also be outside of us.

409
00:32:06,600 --> 00:32:08,580
I have several observations,

410
00:32:09,200 --> 00:32:14,739
so when I prove some theorems,
it's very difficult theorems.

411
00:32:16,140 --> 00:32:22,460
In a couple of years, in several places, people
will prove the same theorem, say,

412
00:32:22,460 --> 00:32:25,740
saw a dilemma after ours was done,

413
00:32:25,740 --> 00:32:28,799
then another guy proves
the same theorem.

414
00:32:28,800 --> 00:32:31,960
In the history of science, it has happened
all the time.

415
00:32:32,140 --> 00:32:37,100
For example, geometry, it happens simultaneously.

416
00:32:37,100 --> 00:32:43,960
First is Lobachevsky and then Gauss and Bolyai
and then other guys, and approximately,

417
00:32:43,960 --> 00:32:48,280
in a ten-year period of time,

418
00:32:48,620 --> 00:32:51,500
and I saw a lot of examples like that.

419
00:32:51,520 --> 00:32:57,220
And when a mathematician thinks it, when they
develop something, they develop something

420
00:32:57,220 --> 00:33:00,820
in general which affects everybody.

421
00:33:01,380 --> 00:33:07,200
So, maybe our model of intelligence is only
inside of us is incorrect.

422
00:33:07,200 --> 00:33:09,240
Lex: It's our interpretation. Yeah.

423
00:33:09,240 --> 00:33:15,840
Vladimir: It may be that they exist with some
connection with world intelligence.

424
00:33:15,840 --> 00:33:16,840
I don't know that.

425
00:33:16,840 --> 00:33:18,799
Lex: You're almost like plugging in into...

426
00:33:18,940 --> 00:33:20,639
Vladimir: Yeah, exactly.

427
00:33:20,639 --> 00:33:22,519
Lex: ...and contributing to this.

428
00:33:22,519 --> 00:33:24,169
Vladimir: ...into a big network.

429
00:33:24,169 --> 00:33:26,520
Lex: Into a big, maybe a neural network.

430
00:33:28,220 --> 00:33:37,440
On the flip side of that, maybe you can comment
on the big O complexity and how you see classifying

431
00:33:37,440 --> 00:33:41,999
algorithms by worst-case running time
in relation to their input.

432
00:33:41,999 --> 00:33:47,459
So, that way of thinking about functions,
do you think P equals un-P?

433
00:33:47,459 --> 00:33:49,570
Do you think that's an interesting question?

434
00:33:49,570 --> 00:33:51,780
Vladimir: Yeah, it is an interesting question.

435
00:33:51,780 --> 00:33:59,680
But let me talk about complexity and about
worst-case scenario.

436
00:34:01,600 --> 00:34:04,040
There is a mathematical setting.

437
00:34:04,040 --> 00:34:09,460
When I came to the United States in 1991,
people did not know this.

438
00:34:09,460 --> 00:34:12,140
They did not know statistical learning theorem.

439
00:34:13,700 --> 00:34:20,040
In Russia, it was published in our monographs,
but in America, they did not know,

440
00:34:20,320 --> 00:34:21,660
and then, they learned it.

441
00:34:22,900 --> 00:34:27,920
Somebody told me that it was worst-case theory
and they will create real-case theory,

442
00:34:28,640 --> 00:34:30,360
but until now, they haven't.

443
00:34:30,360 --> 00:34:37,800
Because it is a mathematical tool, you can
do only what you can do using mathematics,

444
00:34:38,280 --> 00:34:43,680
which is clear understanding and clear description.

445
00:34:46,940 --> 00:34:50,840
For this reason, we introduced complexity.

446
00:35:01,560 --> 00:35:04,300
In VC dimension you can prove some theorems.

447
00:35:05,060 --> 00:35:12,200
But we also create theory for cases when you
know probability measure

448
00:35:12,560 --> 00:35:15,200
and that is the best case it can happen.

449
00:35:17,840 --> 00:35:22,363
So from a mathematical point of view, you
know the best possible case

450
00:35:22,363 --> 00:35:24,660
is the worst possible case.

451
00:35:25,080 --> 00:35:30,040
You can derive different models in the middle,
but it's not so interesting.

452
00:35:30,220 --> 00:35:33,120
Lex: Do you think the edges are interesting?

453
00:35:33,120 --> 00:35:44,420
Vladimir: The edges are interesting because it is not
so easy to get the exact bounds.

454
00:35:44,420 --> 00:35:51,220
It's not, in many cases where you have the
bounds are not exact, but interesting principles

455
00:35:51,840 --> 00:35:54,440
are discovered the most.

456
00:35:54,700 --> 00:36:00,360
Lex: Do you think it's interesting because it's
challenging and reveals interesting principles

457
00:36:00,369 --> 00:36:04,869
that allow you to get those bounds or do you
think it's interesting because it's actually

458
00:36:04,869 --> 00:36:10,120
very useful for understanding the essence
of a function of an algorithm?

459
00:36:10,400 --> 00:36:17,690
So, it's like me judging your life as a human
being by the worst thing you did and the best

460
00:36:17,690 --> 00:36:20,700
thing you did versus all the stuff in the middle.

461
00:36:21,600 --> 00:36:24,760
It seems not productive.

462
00:36:25,220 --> 00:36:33,920
Vladimir: I don't think so because you cannot describe
situations in the middle or it will not be general.

463
00:36:34,400 --> 00:36:44,119
So you can describe edge cases and it is
clear it has some models, but you cannot describe

464
00:36:44,120 --> 00:36:46,520
a model for every new case.

465
00:36:47,520 --> 00:36:53,560
So, you'll never be accurate when you're using models.

466
00:36:53,560 --> 00:36:58,080
Lex: But, from a statistical point of view, the
way you studied functions

467
00:36:58,080 --> 00:37:01,740
and the nature of learning and the world,

468
00:37:01,740 --> 00:37:06,640
don't you think that the real world
has a very long tail

469
00:37:07,280 --> 00:37:12,480
that the edge cases are very far away from the mean,

470
00:37:14,320 --> 00:37:17,580
the stuff in the middle, or no?

471
00:37:19,440 --> 00:37:21,140
Vladimir: I don't know that.

472
00:37:21,140 --> 00:37:27,700
I think that from my point of view,

473
00:37:30,060 --> 00:37:33,660
if youwill use formal statistics,

474
00:37:34,800 --> 00:37:38,380
you need uniform law of large numbers,

475
00:37:40,040 --> 00:37:47,840
if you will use this invariance business,

476
00:37:48,240 --> 00:37:51,010
you don't need just law of large numbers.

477
00:37:51,800 --> 00:37:56,200
And there's a huge difference between uniform
law of large numbers and large numbers.

478
00:37:56,600 --> 00:38:01,060
Lex: Is it useful to describe that a little more
or shall we just take it at...

479
00:38:01,060 --> 00:38:07,980
Vladimir: No. For example, when I'm talking about ducks,
I get three predicates and that was enough.

480
00:38:09,440 --> 00:38:14,420
But, if you will try to do formally distinguish,

481
00:38:14,420 --> 00:38:17,340
you will need a lot of observations.

482
00:38:19,680 --> 00:38:27,160
So that means that information about "looks
like a duck" contained a lot of bit of information

483
00:38:27,160 --> 00:38:29,020
formal bits of information.

484
00:38:29,600 --> 00:38:39,400
So we don't know how much bit of information
is contained from intelligence

485
00:38:39,400 --> 00:38:42,060
and that is a subject of analysis.

486
00:38:42,700 --> 00:38:43,780
Until now,

487
00:38:45,900 --> 00:38:54,520
on business, I don't have people
consider artificial intelligence.

488
00:38:54,520 --> 00:39:00,620
They consider it as some codes which imitate
activities of human beings.

489
00:39:01,220 --> 00:39:02,359
It is not science.

490
00:39:02,359 --> 00:39:03,890
It is applications.

491
00:39:03,890 --> 00:39:05,559
You would like to imitate Go.

492
00:39:05,559 --> 00:39:09,240
Okay, it's very useful and a good problem,

493
00:39:09,240 --> 00:39:15,060
but you need to learn something more

494
00:39:15,820 --> 00:39:21,400
on how people came to develop, say,

495
00:39:21,400 --> 00:39:27,280
predicates "sleeps like a duck" or "fly like a butterfly"

496
00:39:27,300 --> 00:39:28,720
or something like that.

497
00:39:28,730 --> 00:39:37,279
It's not that the teacher tells you how it
came to his mind, how he chooses the image.

498
00:39:37,280 --> 00:39:39,840
That is a problem of intelligence.

499
00:39:39,840 --> 00:39:41,420
Lex: That is the problem of intelligence.

500
00:39:41,420 --> 00:39:45,200
And you see that connected to the problem
of learning?

501
00:39:45,200 --> 00:39:46,100
Are they?

502
00:39:46,100 --> 00:39:51,120
Vladimir: Absolutely, because you immediately give
this predicate like specific predicates

503
00:39:51,120 --> 00:39:53,920
"swims like a duck" or "quacks like a duck."

504
00:39:54,740 --> 00:39:57,160
It was chosen somehow.

505
00:39:57,480 --> 00:40:04,160
Lex: So what is the line of work, would you say,
if you were to formulate as a set of open problems

506
00:40:06,060 --> 00:40:11,980
that will take us there, to fly like
a butterfly, we'll get a system to be able to?

507
00:40:11,980 --> 00:40:18,220
Vladimir: Let's separate two stories--one mathematical
story that if you have predicates

508
00:40:18,220 --> 00:40:19,480
you can do something,

509
00:40:20,240 --> 00:40:22,720
and another story on how to
get predicates.

510
00:40:23,580 --> 00:40:31,080
It is an intelligence problem and people even
did not start understanding intelligence.

511
00:40:31,920 --> 00:40:37,260
Because to understand intelligence, first of all,
try to understand what they will teach us,

512
00:40:39,120 --> 00:40:43,480
how a teacher teach, why one teacher is
better than another one.

513
00:40:44,060 --> 00:40:50,500
Lex: Yeah. And so, do you think we really even haven't
started on the journey of generating the predicates?

514
00:40:50,720 --> 00:40:52,500
Vladimir: No. We don't understand.

515
00:40:52,500 --> 00:40:55,260
We even don't understand that this problem exists.

516
00:40:57,680 --> 00:40:58,400
Lex:  You do.

517
00:40:59,000 --> 00:41:01,840
Vladimir: No. I just know a name.

518
00:41:02,140 --> 00:41:07,200
I won't understand why one teacher
is better than another

519
00:41:08,140 --> 00:41:12,440
and how the teacher affects the student.

520
00:41:13,240 --> 00:41:17,620
It is not because he is repeating the problem
which is in the textbooks.

521
00:41:18,320 --> 00:41:19,800
He makes some remarks.

522
00:41:20,780 --> 00:41:23,480
He makes some philosophy of reasoning.

523
00:41:23,480 --> 00:41:25,220
Lex: Yeah, that's beautiful.

524
00:41:25,220 --> 00:41:31,160
It is a formulation of a question 
that is the open problem:

525
00:41:31,160 --> 00:41:33,500
Why is one teacher better than another?

526
00:41:33,680 --> 00:41:34,360
Vladimir: Right.

527
00:41:35,120 --> 00:41:36,400
What he does about it.

528
00:41:39,740 --> 00:41:41,880
Lex: "Why" at every level.

529
00:41:42,240 --> 00:41:44,580
How did they get better?

530
00:41:44,940 --> 00:41:46,720
What does it mean to be better?

531
00:41:49,320 --> 00:41:53,000
Vladimir: Yeah. From whatever model I have,

532
00:41:53,860 --> 00:41:56,940
one teacher can give
a very good predicate.

533
00:41:56,940 --> 00:42:02,560
One teacher can say "swims like a duck" and
another can say "jumps like a duck."

534
00:42:03,620 --> 00:42:07,300
And jumps like a duck carries zero information.

535
00:42:09,260 --> 00:42:15,220
Lex: So what is the most exciting problem in statistical
learning you ever worked on or are working on now?

536
00:42:17,520 --> 00:42:21,640
Vladimir: I just finished this invariance story

537
00:42:22,940 --> 00:42:25,660
and I'm happy that I believe that

538
00:42:25,660 --> 00:42:29,920
it is an ultimate learning story.

539
00:42:30,380 --> 00:42:35,080
At least, I can show that there are no other
mechanisms.

540
00:42:35,080 --> 00:42:44,040
There are only two mechanisms but they separate
statistical parts from intelligence parts

541
00:42:44,880 --> 00:42:47,460
and I know nothing about the intelligence part.

542
00:42:48,300 --> 00:42:56,880
And if you will know there's the intelligence
part, it will help us a lot in teaching

543
00:42:57,280 --> 00:42:59,300
and in learning.

544
00:42:59,960 --> 00:43:02,300
Lex: And we'll know it when we see it?

545
00:43:02,700 --> 00:43:06,880
So for example, in my talk, in the last slide
was a challenge.

546
00:43:06,880 --> 00:43:10,820
So you have a NIST digit recognition problem

547
00:43:11,900 --> 00:43:16,860
and deep learning claims that they did it very well

548
00:43:16,860 --> 00:43:21,060
say 99.5% correct answers,

549
00:43:21,840 --> 00:43:24,500
but they used 60,000 observations.

550
00:43:25,020 --> 00:43:31,400
Can you do the same using a hundred times
less but incorporating invariants,

551
00:43:31,400 --> 00:43:34,620
what it means, you know, digit 1, 2, 3?

552
00:43:34,840 --> 00:43:45,080
Just looking on that, explain the vision variant
I should keep, to use a hundred times less

553
00:43:45,080 --> 00:43:46,840
examples, to do the same job.

554
00:43:48,200 --> 00:43:56,440
Lex: Yeah, that last slide, unfortunately, your
talk ended quickly, but that last slide was

555
00:43:56,440 --> 00:44:02,060
a powerful open challenge and a formulation
of the essence there.

556
00:44:02,060 --> 00:44:12,220
Vladimir: That is the exact problem of intelligence
because everybody, when machine learning started

557
00:44:12,220 --> 00:44:17,769
and it was developed by mathematicians, they
immediately recognized that they use much

558
00:44:17,769 --> 00:44:21,600
more training data than humans needed.

559
00:44:22,380 --> 00:44:26,580
But now, again, we came to the same story
of how to decrease.

560
00:44:27,200 --> 00:44:30,100
That is a problem of learning.

561
00:44:30,460 --> 00:44:35,360
It is not like in deep learning, they use
zillions of training data

562
00:44:35,960 --> 00:44:38,300
because maybe zillions are not enough

563
00:44:38,300 --> 00:44:43,780
if you have a good invariance.

564
00:44:44,320 --> 00:44:49,160
Maybe, you'll never collect
some number of observations.

565
00:44:49,160 --> 00:44:54,960
But now, it is a question of intelligence
on how to do that

566
00:44:55,800 --> 00:44:58,000
because the statistical part is ready.

567
00:44:58,010 --> 00:45:03,860
As soon as you supply us this predicate,
we can do a good job

568
00:45:03,860 --> 00:45:06,560
with the small amount of observations

569
00:45:06,560 --> 00:45:12,600
and the very first challenges of a long
digital cognition and you know digits

570
00:45:12,600 --> 00:45:15,560
and 12 invariants.

571
00:45:15,560 --> 00:45:22,120
I'm thinking about that and I can say for
digit 3, I would introduce the concept

572
00:45:22,120 --> 00:45:33,220
of horizontal symmetry, so digit 3 has horizontal
symmetry more than digit 2 or something like that.

573
00:45:34,380 --> 00:45:41,660
But as soon as I get the horizontal symmetry,
I can mathematically invent a lot of measure

574
00:45:41,660 --> 00:45:47,260
of horizontal symmetry or the vertical symmetry
or the diagonal symmetry, whatever,

575
00:45:47,260 --> 00:45:49,140
if I have the ideal symmetry.

576
00:45:49,800 --> 00:45:50,880
What would it tell us?

577
00:45:52,420 --> 00:46:06,960
Looking on digits, I see that it is a meta-predicate
which is not shaped into something like symmetry,

578
00:46:06,960 --> 00:46:11,640
like how dark is the whole picture, something like that,

579
00:46:13,100 --> 00:46:15,740
which can certify as a predicate.

580
00:46:16,120 --> 00:46:27,040
Lex: Do you think such a predicate could rise out
of something that's not general,

581
00:46:27,500 --> 00:46:35,120
meaning, it feels like for me to be able to understand
the difference between the two and the three,

582
00:46:35,120 --> 00:46:46,740
I would need to have had a childhood of 10
to 15 years playing with kids, going to school,

583
00:46:47,700 --> 00:46:49,560
being yelled at by parents,

584
00:46:50,760 --> 00:46:55,660
all of that, walking, jumping, looking at ducks.

585
00:46:56,140 --> 00:47:01,420
And now, then, I would be able to generate
the right predicate for telling the difference

586
00:47:01,420 --> 00:47:06,200
between a two and a three, or do you think
there's a more efficient way?

587
00:47:06,200 --> 00:47:07,200
Vladimir:I don't know.

588
00:47:07,200 --> 00:47:12,240
I know for sure that you must know something
more than digits.

589
00:47:12,540 --> 00:47:15,000
Lex: Yes, and that's a powerful statement.

590
00:47:15,440 --> 00:47:24,040
Vladimir: Yeah, but maybe there are several languages
of description around these elements of digits.

591
00:47:24,300 --> 00:47:29,880
So, I'm talking about symmetry, about some
properties of geometry.

592
00:47:29,880 --> 00:47:32,840
I'm talking about something abstract.

593
00:47:32,840 --> 00:47:38,200
I don't know about that, but it is a problem
of intelligence.

594
00:47:38,740 --> 00:47:47,180
So in one of our articles, it is trivial to
show that every example can carry not more

595
00:47:47,180 --> 00:47:57,340
than one bit of information because when you
show an example and you say, this is a one,

596
00:47:57,340 --> 00:48:02,780
you can remove functions which
doesn't tell you one.

597
00:48:02,780 --> 00:48:08,420
The best strategy if you can do it perfectly
is to remove half of that.

598
00:48:09,980 --> 00:48:15,600
But when you use one predicate which is "looks
like a duck," you can remove

599
00:48:15,600 --> 00:48:17,960
much more functions in half,

600
00:48:18,600 --> 00:48:24,940
and that means it contains a lot of bit
of information from a formal point of view.

601
00:48:25,840 --> 00:48:31,840
But, when you have a general picture,

602
00:48:31,840 --> 00:48:33,720
on whatyou want to recognize

603
00:48:33,720 --> 00:48:35,620
and a general picture of the world,

604
00:48:36,720 --> 00:48:39,200
can you invent this predicate?

605
00:48:40,580 --> 00:48:45,600
And, that predicate carries a lot of information.

606
00:48:47,060 --> 00:48:48,240
Lex: Beautifully put.

607
00:48:48,660 --> 00:48:55,553
Maybe it's just me, but in all the math you
show in your work, which is some of the most

608
00:48:55,560 --> 00:49:00,980
profound mathematical work in the field of
learning AI and just math, in general,

609
00:49:01,380 --> 00:49:04,420
I hear a lot of poetry and philosophy.

610
00:49:04,420 --> 00:49:09,060
You really kind of talk about philosophy of science.

611
00:49:09,060 --> 00:49:13,360
There's a poetry in music to a lot of the
work you're doing and the way you're thinking

612
00:49:13,369 --> 00:49:16,670
about it, so where does that come from?

613
00:49:16,670 --> 00:49:18,940
Do you escape to poetry?

614
00:49:18,940 --> 00:49:20,300
Do you escape to music?

615
00:49:20,300 --> 00:49:23,980
Vladimir: I think that there exists ground truths

616
00:49:26,000 --> 00:49:29,440
and that can be seen everywhere.

617
00:49:30,660 --> 00:49:37,720
The smart guy philosopher, sometimes
I'm surprised how they see deeply.

618
00:49:38,480 --> 00:49:43,860
Sometimes I see that some of them are
completely out of subject.

619
00:49:45,240 --> 00:49:49,300
But the ground truths, I see in music.

620
00:49:50,620 --> 00:49:51,860
Lex: Music are the ground truth?

621
00:49:51,860 --> 00:49:52,660
Vladimir: Yeah.

622
00:49:53,400 --> 00:50:00,300
And in poetry, many poetry, they believe
that they take dictation.

623
00:50:01,780 --> 00:50:12,380
Lex: So what piece of music as a piece of empirical
evidence gave you a sense that they are touching

624
00:50:12,380 --> 00:50:13,540
something in the ground truth?

625
00:50:14,400 --> 00:50:15,420
Vladimir: It is structure.

626
00:50:16,440 --> 00:50:18,900
Lex: The structure, the math of music.

627
00:50:18,900 --> 00:50:24,200
Vladimir: Because when you're listening to Bach,
you see the structure--very clear, very classic,

628
00:50:24,200 --> 00:50:25,420
very simple.

629
00:50:25,420 --> 00:50:32,540
And the same it was when you have axioms in
geometry, you have the same feeling.

630
00:50:33,180 --> 00:50:35,900
And in poetry, sometimes, this is the same.

631
00:50:35,900 --> 00:50:36,540
Lex: Yeah.

632
00:50:38,100 --> 00:50:42,700
And if you look back to your childhood, 
you grew up in Russia.

633
00:50:42,700 --> 00:50:47,980
You maybe were born as a researcher in Russia,
you developed as a researcher in Russia.

634
00:50:47,980 --> 00:50:51,140
You came to the United States and a few places.

635
00:50:51,680 --> 00:50:58,660
If you look back, what were some of your happiest
moments as a research?

636
00:50:59,120 --> 00:51:01,940
Some of the most profound moments,

637
00:51:02,600 --> 00:51:05,960
not in terms of their impact on society,

638
00:51:05,960 --> 00:51:11,520
but in terms of their impact on how damn good

639
00:51:11,520 --> 00:51:14,320
you feel that day and you remember that moment?

640
00:51:15,240 --> 00:51:19,700
Vladimir: You know, every time when you found something,

641
00:51:20,980 --> 00:51:23,580
it is the greatest moments in life,

642
00:51:23,980 --> 00:51:26,340
every simple thing.

643
00:51:26,340 --> 00:51:31,300
But, my general feelings most of the time
was wrong.

644
00:51:32,040 --> 00:51:38,720
You should go again and again and again and
try to be honest in front of yourself,

645
00:51:39,280 --> 00:51:46,020
not to my interpretation, but try to understand
that it is related to ground rules

646
00:51:46,600 --> 00:51:52,160
and it is not my blah, blah, blah
interpretation or something like that.

647
00:51:52,480 --> 00:51:56,920
Lex: But, you're allowed to get excited at the
possibility of discovery.

648
00:51:56,930 --> 00:51:57,970
Vladimir: Oh, yeah.

649
00:51:57,970 --> 00:51:59,760
Lex:  You have to double check it.

650
00:52:00,940 --> 00:52:04,740
Vladimir: No, but how it's relates to the ground rules.

651
00:52:04,740 --> 00:52:10,520
Is it just temporary or is it forever?

652
00:52:10,520 --> 00:52:17,000
You know, you always have a feeling when you
found something.

653
00:52:17,460 --> 00:52:18,740
How big is that?

654
00:52:19,560 --> 00:52:25,520
So 20 years ago, when we discovered statistical
learning theory, nobody believed

655
00:52:25,820 --> 00:52:29,600
except for one guy, Dudley from MIT.

656
00:52:31,980 --> 00:52:39,200
And then, in 20 years, it became in fashion,
and the same with Support Vector Machines.

657
00:52:41,900 --> 00:52:47,640
Lex: So, with support vector machines and learning
theory, when you were working on it,

658
00:52:48,540 --> 00:52:50,120
you had a sense,

659
00:52:51,300 --> 00:53:00,640
a sense of the profundity of it, how this
seems to be right, this seems to be powerful?

660
00:53:00,640 --> 00:53:03,400
Vladimir: Right. Absolutely. Immediately.

661
00:53:03,400 --> 00:53:07,220
I recognized that it will last forever.

662
00:53:08,120 --> 00:53:15,600
And now, when I found this invariant story,

663
00:53:16,900 --> 00:53:20,840
I have a feeling that this is complete learning

664
00:53:21,160 --> 00:53:24,580
because I have proved that there are
no different mechanisms.

665
00:53:24,620 --> 00:53:33,760
You can have some cosmetic improvements that
you can do, but in terms of invariants,

666
00:53:34,840 --> 00:53:40,940
you need more invariants in statistical learning
organization work together.

667
00:53:41,520 --> 00:53:50,020
But, also, I'm happy that you can formulate
what is intelligence from that

668
00:53:51,080 --> 00:53:54,460
and to separate from the technical point.

669
00:53:55,020 --> 00:53:56,880
That is completely different.

670
00:53:57,140 --> 00:53:57,880
Lex: Absolutely.

671
00:53:57,880 --> 00:54:00,430
Well, Vladimir, thank you so much for talking today.

672
00:54:00,430 --> 00:54:01,420
Vladimir: Thank you.

673
00:54:01,420 --> 00:54:01,920
Lex: It's an honor.

